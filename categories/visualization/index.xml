<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Visualization on Dash</title>
    <link>http://purplepalmdash.github.io/categories/visualization/</link>
    <description>Recent content in Visualization on Dash</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Dec 2015 16:29:43 +0000</lastBuildDate>
    <atom:link href="http://purplepalmdash.github.io/categories/visualization/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>用Graphite呈现广州空气质量(4)</title>
      <link>http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-4/</link>
      <pubDate>Wed, 16 Dec 2015 16:29:43 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-4/</guid>
      <description>

&lt;h3 id=&#34;tessera&#34;&gt;Tessera&lt;/h3&gt;

&lt;p&gt;直接导入Graphite中定义好的dashboard即可，值得注意的是，如何创建模板，或者说，如何创建一
个template用于渲染我们导入的各个数据？&lt;/p&gt;

&lt;p&gt;导入的时候出现了如下的问题:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://purplepalmdash.github.io/images/2015_12_16_18_34_44_701x483.jpg&#34; alt=&#34;/images/2015_12_16_18_34_44_701x483.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可见tessera中对数据的定制化是必须的。&lt;/p&gt;

&lt;h3 id=&#34;grafana&#34;&gt;Grafana&lt;/h3&gt;

&lt;p&gt;安装及配置为自动启动:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://grafanarel.s3.amazonaws.com/builds/grafana_2.6.0_amd64.deb
$ sudo dpkg -i grafana_2.6.0_amd64.deb
$ sudo service grafana-server start
$ sudo update-rc.d grafana-server defaults 95 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认用户名/密码为 admin/admin.&lt;/p&gt;

&lt;p&gt;现在添加graphite数据源，例如：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://purplepalmdash.github.io/images/2015_12_16_19_20_33_703x484.jpg&#34; alt=&#34;/images/2015_12_16_19_20_33_703x484.jpg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>用Graphite呈现广州空气质量(3)</title>
      <link>http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-3/</link>
      <pubDate>Wed, 16 Dec 2015 14:18:25 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-3/</guid>
      <description>

&lt;h3 id=&#34;当前节点无数据&#34;&gt;当前节点无数据&lt;/h3&gt;

&lt;p&gt;我们的脚本加入crontab运行后，最开始是可以得到数据的，后面两小时它挂了，查原因，有以下的
报错信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /home/adminubuntu/GuangzhouPM25.py 
Traceback (most recent call last):
  File &amp;quot;/home/adminubuntu/GuangzhouPM25.py&amp;quot;, line 112, in &amp;lt;module&amp;gt;
    airdata = get_air_data(positionsets)
  File &amp;quot;/home/adminubuntu/GuangzhouPM25.py&amp;quot;, line 80, in get_air_data
    PM25 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmtow&#39;}).contents[0]).group())
ValueError: invalid literal for int() with base 10: &#39;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时selenium控制的浏览器停在以下图例:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://purplepalmdash.github.io/images/2015_12_16_14_21_03_497x301.jpg&#34; alt=&#34;/images/2015_12_16_14_21_03_497x301.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，如果当前节点的数据为&lt;code&gt;--&lt;/code&gt;， 则我们的python脚本运行会出现问题。因而我们在代码中
要加入少量修改。&lt;/p&gt;

&lt;h3 id=&#34;错误处理&#34;&gt;错误处理&lt;/h3&gt;

&lt;p&gt;以下的代码更改添加了错误处理，如果该监测点的数值为空，则不提交任何数据:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@@ -66,9 +66,9 @@ def get_air_data(positionsets):
   hourdata = {}
   # Calling selenium, need linux X
   browser = Firefox()
-  # Added 10 seconds for waiting page for loading.
-  time.delay(10)
   browser.get(URL)
+  # Added 10 seconds for waiting page for loading.
+  time.sleep(10)
   # Click button one-by-one
   for position in positionsets:
     # After clicking, should re-get the page_source.
@@ -78,33 +78,37 @@ def get_air_data(positionsets):
     soup = BeautifulSoup(page_source, &#39;html.parser&#39;)
     # pm2.5 value would be something like xx 微克/立方米, so we need an regex for
     # matching, example: print int(pattern.match(input).group())
-    PM25 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmtow&#39;}).contents[0]).group())
-    PM25_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmtow_iaqi&#39;}).contents[0]).group())
-    PM10 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmten&#39;}).contents[0]).group())
-    PM10_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmten_iaqi&#39;}).contents[0]).group())
-    SO2 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;sotwo&#39;}).contents[0]).group())
-    SO2_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;sotwo_iaqi&#39;}).contents[0]).group())
-    NO2 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;notwo&#39;}).contents[0]).group())
-    NO2_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;notwo_iaqi&#39;}).contents[0]).group())
-    # Special notice the CO would be float value
-    CO = float(floatpattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;co&#39;}).contents[0]).group())
-    CO_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;co_iaqi&#39;}).contents[0]).group())
-    O3 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;othree&#39;}).contents[0]).group())
-    O3_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;othree_iaqi&#39;}).contents[0]).group())
-    hourdata_key = pinyin.get(position)
-    hourdata[hourdata_key] = []
-    hourdata[hourdata_key].append(PM25)
-    hourdata[hourdata_key].append(PM25_iaqi)
-    hourdata[hourdata_key].append(PM10)
-    hourdata[hourdata_key].append(PM10_iaqi)
-    hourdata[hourdata_key].append(SO2)
-    hourdata[hourdata_key].append(SO2_iaqi)
-    hourdata[hourdata_key].append(NO2)
-    hourdata[hourdata_key].append(NO2_iaqi)
-    hourdata[hourdata_key].append(CO)
-    hourdata[hourdata_key].append(CO_iaqi)
-    hourdata[hourdata_key].append(O3)
-    hourdata[hourdata_key].append(O3_iaqi)
+    try:
+      PM25 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmtow&#39;}).contents[0]).group())
+      PM25_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmtow_iaqi&#39;}).contents[0]).group())
+      PM10 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmten&#39;}).contents[0]).group())
+      PM10_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmten_iaqi&#39;}).contents[0]).group())
+      SO2 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;sotwo&#39;}).contents[0]).group())
+      SO2_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;sotwo_iaqi&#39;}).contents[0]).group())
+      NO2 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;notwo&#39;}).contents[0]).group())
+      NO2_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;notwo_iaqi&#39;}).contents[0]).group())
+      # Special notice the CO would be float value
+      CO = float(floatpattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;co&#39;}).contents[0]).group())
+      CO_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;co_iaqi&#39;}).contents[0]).group())
+      O3 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;othree&#39;}).contents[0]).group())
+      O3_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;othree_iaqi&#39;}).contents[0]).group())
+      hourdata_key = pinyin.get(position)
+      hourdata[hourdata_key] = []
+      hourdata[hourdata_key].append(PM25)
+      hourdata[hourdata_key].append(PM25_iaqi)
+      hourdata[hourdata_key].append(PM10)
+      hourdata[hourdata_key].append(PM10_iaqi)
+      hourdata[hourdata_key].append(SO2)
+      hourdata[hourdata_key].append(SO2_iaqi)
+      hourdata[hourdata_key].append(NO2)
+      hourdata[hourdata_key].append(NO2_iaqi)
+      hourdata[hourdata_key].append(CO)
+      hourdata[hourdata_key].append(CO_iaqi)
+      hourdata[hourdata_key].append(O3)
+      hourdata[hourdata_key].append(O3_iaqi)
+    except ValueError, Argument:
+      # won&#39;t add the data, simply ignore this position
+      print &amp;quot;The argument does not contain numbers\n&amp;quot;, Argument
   # After clicking all of the button, quit the firefox and return the dictionary
   browser.close()
   return hourdata
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到现在为止，数据可以顺利的写入到Graphite中。&lt;/p&gt;

&lt;h3 id=&#34;graphite-dashboard&#34;&gt;Graphite Dashboard&lt;/h3&gt;

&lt;p&gt;组建Graphite Dashboard可以通过图形界面来进行，举例如下:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://purplepalmdash.github.io/images/2015_12_16_15_22_04_579x386.jpg&#34; alt=&#34;/images/2015_12_16_15_22_04_579x386.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;具体的添加过程就不说了，值得注意的是，设置几个属性，时间范围为过去24小时，
双击某图片后，&lt;code&gt;Render Options&lt;/code&gt;里的&lt;code&gt;Line Mode&lt;/code&gt;选择&lt;code&gt;Connected Line&lt;/code&gt;，
这样可以构建出连接线，比较适合我们所需要展示的数据类型。Y-Axis，即Y轴的起点(Minimal)设置为0.&lt;/p&gt;

&lt;p&gt;点击DashBoard-&amp;gt; Edit Dashboard, 可以看到以下定义:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://purplepalmdash.github.io/images/2015_12_16_15_25_54_789x499.jpg&#34; alt=&#34;/images/2015_12_16_15_25_54_789x499.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个定义文件可以修改，我们将使用这个定义文件来批量制作其他十多个监测点的Dashboard.&lt;/p&gt;

&lt;h3 id=&#34;创建更多的dashboard&#34;&gt;创建更多的Dashboard&lt;/h3&gt;

&lt;p&gt;参考:&lt;br /&gt;
&lt;a href=&#34;http://graphite.readthedocs.org/en/latest/dashboard.html#editing-importing-and-exporting-via-json&#34;&gt;http://graphite.readthedocs.org/en/latest/dashboard.html#editing-importing-and-exporting-via-json&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;将上述的dashboard定义文件存储在某个文本文件中，
用下列命令批量生成新的dashboard定义文件:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat dashboard.txt | sed &#39;s/haizhuhu/aotizhongxin/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/aotizhongxin/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/baiyunshan/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/dafushan/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/gongyuanqian/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/haizhubaogang/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/haizhuchisha/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/haizhuhu/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/haizhushayuan/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/huangpudashadi/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/huangpuwenchong/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/huangshalubianzhan/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/liwanfangcun/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/liwanxicun/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/luhu/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/luogangxiqu/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/tianhelongdong/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/tiyuxi/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/yayuncheng/g&#39;|myclip
$ cat dashboard.txt | sed &#39;s/haizhuhu/yangjilubianzhan/g&#39;|myclip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;myclip&lt;/code&gt;是一个自定义的命令，可以将管道输出直接到系统剪贴板，
而后将内容新添加到dashboard定义文件中，点击update后，另存为新的dashboard即可.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>用Graphite呈现广州空气质量(2)</title>
      <link>http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-2/</link>
      <pubDate>Wed, 16 Dec 2015 12:11:21 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-2/</guid>
      <description>

&lt;h3 id=&#34;更改后的脚本&#34;&gt;更改后的脚本&lt;/h3&gt;

&lt;p&gt;以下脚本可以用于取回网页上的数据，并将其写入到Graphite远程服务器。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
#-*-coding:utf-8 -*-

##################################################################################
# For fetching back the Air Quality Data and write it into Graphite on local server
# Graphite Data Definition, this is the general definition among every city
# air.city.citypoint.so2
# air.city.citypoint.no2
# air.city.citypoint.pm10
# air.city.citypoint.co
# air.city.citypoint.o38h
# air.city.citypoint.pm25
# air.city.citypoint.aqi
# air.city.citypoint.firstp
# air.city.citypoint.overp
# When running this script in crontab, be sure to give it a display
# Example, execute this script every hour at xx:05
# 5 */1 * * * export DISPLAY=:0;/home/adminubuntu/GuangzhouPM25.py
##################################################################################

# BeautifulSoup
from bs4 import BeautifulSoup

# Selenium
from contextlib import closing
from selenium.webdriver import Firefox
from selenium.webdriver.support.ui import WebDriverWait

# For writing into Graphite
import platform
import socket
import time

# Regex
import re

# pinyin
import pinyin

# Parameters comes here 
CARBON_SERVER = &#39;0.0.0.0&#39;
CARBON_PORT = 2003
DELAY = 5  # secs
URL = &#39;http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html&#39;
CITY = &#39;guangzhou&#39;

# All Points In Guangzhou City
positionsets = [&amp;quot;天河龙洞&amp;quot;, &amp;quot;白云山&amp;quot;, &amp;quot;麓湖&amp;quot;, &amp;quot;公园前&amp;quot;, &amp;quot;荔湾西村&amp;quot;, &amp;quot;黄沙路边站&amp;quot;, &amp;quot;杨箕
路边站&amp;quot;, &amp;quot;荔湾芳村&amp;quot;, &amp;quot;海珠宝岗&amp;quot;, &amp;quot;海珠沙园&amp;quot;, &amp;quot;海珠湖&amp;quot;, &amp;quot;大夫山&amp;quot;, &amp;quot;奥体中心&amp;quot;, &amp;quot;萝岗西区
&amp;quot;, &amp;quot;黄埔文冲&amp;quot;, &amp;quot;黄埔大沙地&amp;quot;, &amp;quot;亚运城&amp;quot;, &amp;quot;体育西&amp;quot;, &amp;quot;海珠赤沙&amp;quot;]

# regex for matching the digits.
pattern = re.compile(r&#39;\d*&#39;)
floatpattern=re.compile(r&#39;[\d|\.]*&#39;)

# Sending message to graphite server. 
def send_msg(message):
  print &#39;sending message:\n%s&#39; % message
  sock = socket.socket()
  sock.connect((CARBON_SERVER, CARBON_PORT))
  sock.sendall(message)
  sock.close()

# Fetching data, runs each hour. In one-time access should fetch all of the data. 
def get_air_data(positionsets):
  # Dictionary hourdata is for holding data, DataStructure like: 
  # {&#39;baiyunshan&#39;: [44, 5], &#39;haizhubaogang&#39;: [55, 6]}
  hourdata = {}
  # Calling selenium, need linux X
  browser = Firefox()
  browser.get(URL)
  # Click button one-by-one
  for position in positionsets:
    # After clicking, should re-get the page_source.
    browser.find_element_by_id(position).click()
    page_source = browser.page_source
    # Cooking Soup
    soup = BeautifulSoup(page_source, &#39;html.parser&#39;)
    # pm2.5 value would be something like xx 微克/立方米, so we need an regex for
    # matching, example: print int(pattern.match(input).group())
    PM25 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmtow&#39;}).contents[0]).group())
    PM25_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;:
&#39;pmtow_iaqi&#39;}).contents[0]).group())
    PM10 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;pmten&#39;}).contents[0]).group())
    PM10_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;:
&#39;pmten_iaqi&#39;}).contents[0]).group())
    SO2 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;sotwo&#39;}).contents[0]).group())
    SO2_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;:
&#39;sotwo_iaqi&#39;}).contents[0]).group())
    NO2 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;notwo&#39;}).contents[0]).group())
    NO2_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;:
&#39;notwo_iaqi&#39;}).contents[0]).group())
    # Special notice the CO would be float value
    CO = float(floatpattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;co&#39;}).contents[0]).group())
    CO_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;co_iaqi&#39;}).contents[0]).group())
    O3 = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;: &#39;othree&#39;}).contents[0]).group())
    O3_iaqi = int(pattern.match(soup.find(&#39;td&#39;,{&#39;id&#39;:
&#39;othree_iaqi&#39;}).contents[0]).group())
    hourdata_key = pinyin.get(position)
    hourdata[hourdata_key] = []
    hourdata[hourdata_key].append(PM25)
    hourdata[hourdata_key].append(PM25_iaqi)
    hourdata[hourdata_key].append(PM10)
    hourdata[hourdata_key].append(PM10_iaqi)
    hourdata[hourdata_key].append(SO2)
    hourdata[hourdata_key].append(SO2_iaqi)
    hourdata[hourdata_key].append(NO2)
    hourdata[hourdata_key].append(NO2_iaqi)
    hourdata[hourdata_key].append(CO)
    hourdata[hourdata_key].append(CO_iaqi)
    hourdata[hourdata_key].append(O3)
    hourdata[hourdata_key].append(O3_iaqi)
  # After clicking all of the button, quit the firefox and return the dictionary
  browser.close()
  return hourdata

if __name__ == &#39;__main__&#39;:
  airdata = get_air_data(positionsets)
  timestamp = int(time.time())
  for i in airdata.keys():
    # each key should contains the corresponding hourdata
    lines = [
      &#39;air.guangzhou.%s.pm25 %s %d&#39; % (i, airdata[i][0], timestamp),
      &#39;air.guangzhou.%s.pm25_iaqi %s %d&#39; % (i, airdata[i][1], timestamp),
      &#39;air.guangzhou.%s.pm10 %s %d&#39; % (i, airdata[i][2], timestamp),
      &#39;air.guangzhou.%s.pm10_iaqi %s %d&#39; % (i, airdata[i][3], timestamp),
      &#39;air.guangzhou.%s.so2 %s %d&#39; % (i, airdata[i][4], timestamp),
      &#39;air.guangzhou.%s.so2_iaqi %s %d&#39; % (i, airdata[i][5], timestamp),
      &#39;air.guangzhou.%s.no2 %s %d&#39; % (i, airdata[i][6], timestamp),
      &#39;air.guangzhou.%s.no2_iaqi %s %d&#39; % (i, airdata[i][7], timestamp),
      &#39;air.guangzhou.%s.co %s %d&#39; % (i, airdata[i][8], timestamp),
      &#39;air.guangzhou.%s.co_iaqi %s %d&#39; % (i, airdata[i][9], timestamp),
      &#39;air.guangzhou.%s.o3 %s %d&#39; % (i, airdata[i][10], timestamp),
      &#39;air.guangzhou.%s.o3_iaqi %s %d&#39; % (i, airdata[i][11], timestamp)
    ]
    message = &#39;\n&#39;.join(lines) + &#39;\n&#39;
    send_msg(message)
    # delay for graphite server will use a DELAY time for inserting data
    time.sleep(DELAY)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;使用方法&#34;&gt;使用方法&lt;/h3&gt;

&lt;p&gt;将上面的文件保存为可执行文件，然后使用crontab添加一个定时任务，譬如以下的crontab条目会
在每个小时的xx:05分时自动运行该脚本文件，将取回的数据写入到Graphite远端。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ crontab -l
# hourly execute pm25 updating task, xx:05 will be the execute time
5 */1 * * * export DISPLAY=:0;/home/adminubuntu/GuangzhouPM25.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写入graphite后的效果如下:&lt;br /&gt;
&lt;img src=&#34;http://purplepalmdash.github.io/images/2015_12_16_12_20_04_284x453.jpg&#34; alt=&#34;/images/2015_12_16_12_20_04_284x453.jpg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>用Graphite呈现广州空气质量</title>
      <link>http://purplepalmdash.github.io/2015/12/15/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang/</link>
      <pubDate>Tue, 15 Dec 2015 10:05:39 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/15/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang/</guid>
      <description>

&lt;h3 id=&#34;数据源准备&#34;&gt;数据源准备&lt;/h3&gt;

&lt;p&gt;数据源地址在:&lt;br /&gt;
&lt;a href=&#34;http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html&#34;&gt;http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;但是这个地址取回数据比较困难。而在&lt;a href=&#34;http://www.gzepb.gov.cn/&#34;&gt;http://www.gzepb.gov.cn/&lt;/a&gt;
右侧的栏里可以通过点击，打开某个监测点当前的空气质量指数,例如海珠湖的数据位于:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96&#34;&gt;http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;beautiful-soup&#34;&gt;Beautiful Soup&lt;/h3&gt;

&lt;p&gt;Beautiful Soup可以被理解为网页爬虫，用于爬取某个页面并取回所需信息。在Ubuntu/Debian系统
中，安装命令如下。同时为了使用对XML解析速度更快的lxml解析器，我们安装python-lxml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install -y python-bs4
$ sudo apt-get install -y python-lxml 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在我们打开某个终端，开始用命令行交互的方式，取回海珠湖监测点的数据:&lt;/p&gt;

&lt;p&gt;首先，引入所需的库：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
&amp;gt;&amp;gt;&amp;gt; from bs4 import BeautifulSoup
&amp;gt;&amp;gt;&amp;gt; import urllib2
&amp;gt;&amp;gt;&amp;gt; response = urllib2.urlopen(&#39;http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96&#39;)
&amp;gt;&amp;gt;&amp;gt; print response.info()
Content-Length: 10216
Content-Type: text/html
Last-Modified: Wed, 13 May 2015 08:12:28 GMT
Accept-Ranges: bytes
ETag: &amp;quot;b680828d548dd01:da2&amp;quot;
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
Date: Tue, 15 Dec 2015 02:25:17 GMT
Connection: close

&amp;gt;&amp;gt;&amp;gt; html = response.read()
&amp;gt;&amp;gt;&amp;gt; print &amp;quot;Get the length :&amp;quot;, len(html)
Get the length : 10216
&amp;gt;&amp;gt;&amp;gt; response.close()  # best practice to close the file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上述的操作里调用urllib2取回了页面， html变量里包含了该网页的内容。接下来我们使用
BeautifulSoup来美化并从中取回我们想要的元素。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; soup = BeautifulSoup(html, &#39;html.parser&#39;)    
&amp;gt;&amp;gt;&amp;gt; print soup.prettify()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;仔细检查后发现，用urllib2取回的网页中，html变量里未包含当前的数据值。通过阅读代码得知，
当前页面的值是浏览器在载入网页时执行javascript函数得到的。因而我们使用一个真实的浏览器
来实现页面的抓取。&lt;/p&gt;

&lt;p&gt;Selenium是一套用于进行浏览器自动化测试的开源工具集，可进行Web应用的端到端测试
。Selenium主要包括两个工具：一是Selenium IDE，二是Selenium WebDriver（简称
WebDriver）. 安装命令如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip install selenium
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用selenium抓取该网页的代码如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from contextlib import closing
&amp;gt;&amp;gt;&amp;gt; from selenium.webdriver import Firefox
&amp;gt;&amp;gt;&amp;gt; from selenium.webdriver.support.ui import WebDriverWait
&amp;gt;&amp;gt;&amp;gt; url=&#39;http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96&#39;
&amp;gt;&amp;gt;&amp;gt; with closing(Firefox()) as browser:
...   browser.get(url)
...   page_source = browser.page_source
... 
&amp;gt;&amp;gt;&amp;gt; print page_source
&amp;gt;&amp;gt;&amp;gt; soup = BeautifulSoup(page_source, &#39;html.parser&#39;)
&amp;gt;&amp;gt;&amp;gt; print soup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在我们可以看到，取回的&lt;code&gt;page_source&lt;/code&gt;变量中已经包含有该时段的数据。接下来就是如何把数据
从其中提取出来的过程。&lt;/p&gt;

&lt;p&gt;定位到含有数据的表格, 根据其层叠结构，获得tr的值:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; table = soup.find(&#39;table&#39;, {&#39;class&#39;: &#39;headTable&#39;})
&amp;gt;&amp;gt;&amp;gt; for td in table.tbody.tr:
...     print td
... 
&amp;lt;td class=&amp;quot;SO2_24H&amp;quot;&amp;gt;7&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;NO2_24H&amp;quot;&amp;gt;50&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;PM10_24H&amp;quot;&amp;gt;29&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;CO_24H&amp;quot;&amp;gt;31&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;O3_8H_24H&amp;quot;&amp;gt;18&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;PM25_24H&amp;quot;&amp;gt;25&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;AQI&amp;quot;&amp;gt;50&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;Pollutants&amp;quot;&amp;gt;—&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;jibie2&amp;quot;&amp;gt;--&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;jibie2&amp;quot;&amp;gt;一级&amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;leibie&amp;quot;&amp;gt;优                  &amp;lt;/td&amp;gt;
&amp;lt;td class=&amp;quot;yanse&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;&amp;quot; src=&amp;quot;Images/you.jpg&amp;quot;/&amp;gt;&amp;lt;/td&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更进一步得到值:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; for td in table.tbody.tr:
...     print td.contents[0]
... 
7
50
29
31
18
25
50
—
--
一级
优                  
&amp;lt;img alt=&amp;quot;&amp;quot; src=&amp;quot;Images/you.jpg&amp;quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对应的图片如下:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://purplepalmdash.github.io/images/2015_12_15_12_06_23_943x201.jpg&#34; alt=&#34;/images/2015_12_15_12_06_23_943x201.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;提取出来了数据，就可以做后续处理了。&lt;/p&gt;

&lt;h3 id=&#34;graphite&#34;&gt;Graphite&lt;/h3&gt;

&lt;p&gt;Graphite的搭建过程不提及。基于我们前面提取出的数据，只需要将其写入Graphite，就可以看
到数据的显示了。&lt;/p&gt;

&lt;p&gt;具体的写入代码参考(需翻墙):&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html&#34;&gt;http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;按照博客中提供的例子，写入到Graphite后的数据在Graphite看起来是这样的:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://purplepalmdash.github.io/images/2015_12_15_14_48_48_318x115.jpg&#34; alt=&#34;/images/2015_12_15_14_48_48_318x115.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;而对应的数据格式则如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; sending message:
     system.monitorserver.loadavg_1min 0.18 1450161396
     system.monitorserver.loadavg_5min 0.25 1450161396
     system.monitorserver.loadavg_15min 0.23 1450161396
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以仿照这样的数据来组织自己的空气质量数据。&lt;/p&gt;

&lt;h3 id=&#34;数据来源再加工&#34;&gt;数据来源再加工&lt;/h3&gt;

&lt;p&gt;前面取回地址失败， 因为它只是返回空气日报的地址，我们需要的是实时情况，所以还是回到&lt;br /&gt;
&lt;a href=&#34;http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html&#34;&gt;http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里需要在selenium里模拟出鼠标快速点击所有链接的效果。&lt;/p&gt;

&lt;p&gt;下面是一次完整的点击白云山按钮并获得PM2.5页面的过程:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@monitorserver:~/Code# python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
&amp;gt;&amp;gt;&amp;gt; from contextlib import closing
&amp;gt;&amp;gt;&amp;gt; from selenium.webdriver import Firefox
&amp;gt;&amp;gt;&amp;gt; from selenium.webdriver.support.ui import WebDriverWait
&amp;gt;&amp;gt;&amp;gt; driver = Firefox()                                                 
&amp;gt;&amp;gt;&amp;gt; driver.get(&#39;http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html&#39;)
&amp;gt;&amp;gt;&amp;gt; driver.refresh()
&amp;gt;&amp;gt;&amp;gt; baiyunmountain=driver.find_element_by_id(&amp;quot;白云山&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; baiyunmountain.click()
&amp;gt;&amp;gt;&amp;gt; PM25=driver.find_element_by_id(&amp;quot;PM25&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; type(PM25)
&amp;lt;class &#39;selenium.webdriver.remote.webelement.WebElement&#39;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; PM25.click()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>