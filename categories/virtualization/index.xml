<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Virtualization on Dash</title>
    <link>http://purplepalmdash.github.io/categories/virtualization/</link>
    <description>Recent content in Virtualization on Dash</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 May 2017 15:28:51 +0800</lastBuildDate>
    
	<atom:link href="http://purplepalmdash.github.io/categories/virtualization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>DevEnvsForLAB</title>
      <link>http://purplepalmdash.github.io/blog/2017/05/24/devenvsforlab/</link>
      <pubDate>Wed, 24 May 2017 15:28:51 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2017/05/24/devenvsforlab/</guid>
      <description>目的 LAB验证环境的搭建。
前提条件： 最小化CentOS7系统迁移（见前一篇文章）.
安装 CentOS 7.3(1611), 最小化安装。
# yum update -y # yum install -y vim qemu libvirt libvirt-devel ruby-devel gcc qemu-kvm net-tools virt-manager wget lm_sensors iotop # wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo # yum install -y nethogs byobu ansible rubygem-ruby-libvirt.x86_64 # wget https://releases.hashicorp.com/vagrant/1.9.1/vagrant_1.9.1_x86_64.rpm # yum install vagrant_1.9.1_x86_64.rpm # vagrant plugin install --plugin-version 0.0.37 vagrant-libvirt # cd ~/.vagrant.d/gems/2.2.5/gems # ln -s ../extensions ./ # vi /etc/modprobe.d/kvm-nested.conf options kvm_intel nested=1  Disable the selinux, firewalld:</description>
    </item>
    
    <item>
      <title>MigrationForCentOS7</title>
      <link>http://purplepalmdash.github.io/blog/2017/05/24/migrationforcentos7/</link>
      <pubDate>Wed, 24 May 2017 14:32:50 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2017/05/24/migrationforcentos7/</guid>
      <description>目的 使用磁盘克隆的方式快速安装、部署系统。
验证环境 Virt-manager, CentOS 7 ISO安装盘
安装注意事项 分区时选择xfs(CentOS默认)， 选择LVM分区。
The disk partition should be Automatically create them.
安装时选择minimum installation. 安装完毕后，关闭虚拟机
安装后调整 复制安装好的硬盘:
$ ls CentOS5G.qcow2 $ cp CentOS5G.qcow2 Duplicated.qcow2 $ ls CentOS5G.qcow2 Duplicated.qcow2  创建一个中转盘，一个大小为50G的目标盘, 中转盘用于存放克隆文件，而目标盘则是我们将克隆文件拷贝过去的盘。
$ qemu-img create -f qcow2 Middle.qcow2 16G Formatting &#39;Middle.qcow2&#39;, fmt=qcow2 size=17179869184 encryption=off cluster_size=65536 lazy_refcounts=off refcount_bits=16 $ qemu-img create -f qcow2 Dest.qcow2 50G Formatting &#39;Dest.qcow2&#39;, fmt=qcow2 size=53687091200 encryption=off cluster_size=65536 lazy_refcounts=off refcount_bits=16  在virt-manager中，依次添加剩余的三块硬盘.</description>
    </item>
    
    <item>
      <title>vagrant-libvirt issue on ArchLinux</title>
      <link>http://purplepalmdash.github.io/blog/2017/05/23/vagrant-libvirt-issue-on-archlinux/</link>
      <pubDate>Tue, 23 May 2017 16:54:24 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2017/05/23/vagrant-libvirt-issue-on-archlinux/</guid>
      <description>Previously install vagrant-libvirt is a tough task on ArchLinux, thus I do following steps to let my vagrant-libvirt running again on archlinux, steps are listed as following:
Remove the installed vagrant and backup the configuration files.
$ sudo pacman -Rsn vagrant $ sudo mv ~/.vagrant.d ~/vagrant.d.back $ sudo rm -rf /opt/vagrant  Install the vagrant-libvirt in AUR repository:
$ yaourt vagrant-libvirt $ tsocks vagrant plugin install vagrant-libvirt $ vagrant plugin list vagrant-libvirt (0.</description>
    </item>
    
    <item>
      <title>AddSecondDiskForVagrant</title>
      <link>http://purplepalmdash.github.io/blog/2017/05/22/addseconddiskforvagrant/</link>
      <pubDate>Mon, 22 May 2017 17:00:19 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2017/05/22/addseconddiskforvagrant/</guid>
      <description>Purpose For adding second disk to vagrant machine, while vagrant machine is running under virtualbox hypervisor. Following are the steps for doing this.
Steps Vagrantfile definition:
# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.provider &amp;quot;virtualbox&amp;quot; do |vb| # Display the VirtualBox GUI when booting the machine #vb.gui = true unless File.exist?(&#39;./secondDisk.vdi&#39;) vb.customize [&#39;createhd&#39;, &#39;--filename&#39;, &#39;./secondDisk.vdi&#39;, &#39;--variant&#39;, &#39;Fixed&#39;, &#39;--size&#39;, 10 * 1024] end # Customize the amount of memory on the VM: vb.</description>
    </item>
    
    <item>
      <title>快速搭建MiniKube开发环境</title>
      <link>http://purplepalmdash.github.io/blog/2017/04/07/%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAminikube%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Fri, 07 Apr 2017 18:08:19 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2017/04/07/%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAminikube%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</guid>
      <description>版本 minikube的版本是v0.17.1, 运行于ArchLinux.
镜像准备 感谢万能的防火墙，我们需要手动下载docker镜像到本地:
sudo docker pull gcr.io/google-containers/kube-addon-manager:v6.3 sudo docker pull gcr.io/google_containers/kubedns-amd64:1.9 sudo docker pull gcr.io/google_containers/kube-dnsmasq-amd64:1.4 sudo docker pull gcr.io/google_containers/exechealthz-amd64:1.2 sudo docker pull gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.1 sudo docker pull gcr.io/google_containers/heapster:v1.2.0 sudo docker pull kubernetes/heapster_influxdb:v0.6 sudo docker pull gcr.io/google_containers/heapster_grafana:v2.6.0-2 sudo docker pull gcr.io/google_containers/pause-amd64:3.0  存储docker镜像并打包的命令如下, 这样一个wget就可取下来所有的镜像:
sudo docker save gcr.io/google-containers/kube-addon-manager:v6.3 | bzip2&amp;gt;~/serve/addonmanagerv63.tar.bz2 sudo docker save gcr.io/google_containers/kubedns-amd64:1.9|bzip2&amp;gt;~/serve/dns19.tar.bz2 sudo docker save gcr.io/google_containers/kube-dnsmasq-amd64:1.4 |bzip2&amp;gt;~/serve/dnsmasq14.tar.bz2 sudo docker save gcr.io/google_containers/exechealthz-amd64:1.2|bzip2&amp;gt;~/serve/exechealthz12.tar.bz2 sudo docker save gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.1|bzip2&amp;gt;~/serve/dashboard151.tar.bz2 sudo docker save gcr.</description>
    </item>
    
    <item>
      <title>CoreOS Cluster On Libvirt</title>
      <link>http://purplepalmdash.github.io/blog/2017/04/06/coreos-cluster-on-libvirt/</link>
      <pubDate>Thu, 06 Apr 2017 13:27:15 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2017/04/06/coreos-cluster-on-libvirt/</guid>
      <description>Environment ArchLinux, libvirt.
Libvirt network(NAT, NOT dhcp), 172.17.8.1&amp;frasl;24.
etcd server On ArchLinux, create a etcd server via following commands:
http://purplepalmdash.github.io/blog/2016/12/21/trycoreos2/
ArchLinux etcd2 Example.</description>
    </item>
    
    <item>
      <title>DockerCloudReadingDigests</title>
      <link>http://purplepalmdash.github.io/blog/2017/01/06/dockercloudreadingdigests/</link>
      <pubDate>Fri, 06 Jan 2017 15:42:25 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2017/01/06/dockercloudreadingdigests/</guid>
      <description>第一章 Docker镜像准备:
$ sudo docker pull redis $ sudo docker pull django $ sudo docker pull haproxy $ sudo docker pull ubuntu  应用栈节点架构:
启动redis-master容器节点, 两个redis-slave容器节点启动时连接到redis-master上面, 两个app容器节点启动时连接到redis-master上面, haproxy容器结点启动时连接到两个app结点上面。
容器的启动顺序为：redis-master -&amp;gt; redis-slave -&amp;gt; APP -&amp;gt; HAProxy.
Redis Master:
$ sudo docker run -it --name redis-master redis /bin/bash root@4e4e597ffcb6:/data#  Redis Slave1/Slave2:
$ sudo docker run -it --name redis-slave1 --link redis-master:master redis /bin/bash $ sudo docker run -it --name redis-slave2 --link redis-master:master redis /bin/bash  App1, App2:</description>
    </item>
    
    <item>
      <title>运行K8S例程</title>
      <link>http://purplepalmdash.github.io/blog/2017/01/03/%E8%BF%90%E8%A1%8Ck8s%E4%BE%8B%E7%A8%8B/</link>
      <pubDate>Tue, 03 Jan 2017 19:10:32 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2017/01/03/%E8%BF%90%E8%A1%8Ck8s%E4%BE%8B%E7%A8%8B/</guid>
      <description>GuestBook 注意修改imagePullPolicy为IfNotPresent, 创建服务的步骤分别为:
$ kubectl create -f redis-master-deployment.yaml $ kubectl create -f redis-master-service.yaml $ kubectl create -f frontend-deployment.yaml $ kubectl create -f frontend-service.yaml  现在得到其运行状态:
$ kubectl get pod NAME READY STATUS RESTARTS AGE frontend-88237173-02dvl 1/1 Running 0 2h frontend-88237173-r7g3v 1/1 Running 0 2h frontend-88237173-vjbv5 1/1 Running 0 2h redis-master-4154998525-f186t 1/1 Running 0 2h redis-slave-132015689-3qh7b 1/1 Running 0 2h redis-slave-132015689-hpw88 1/1 Running 0 2h  可以用proxy-forward直接访问某个pod中暴露出来的frontend服务:
$ kubectl port-forward frontend-88237173-02dvl 9081:80  上述命令的意思是，将pod frontend-88237173-02dvl80端口的流量转发到 本地的9081端口，则可以通过访问http://127.</description>
    </item>
    
    <item>
      <title>VisualizerForKubernetes</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/27/visualizerforkubernetes/</link>
      <pubDate>Tue, 27 Dec 2016 19:06:23 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/27/visualizerforkubernetes/</guid>
      <description>Download the source code from github:
$ git clone https://github.com/saturnism/gcp-live-k8s-visualizer $ kubectl proxy ---www=./  Create the service/pods like the ones in examples, then you get the beautiful view for your pods and services:
Change the rcs to 10:
$ kubectl scale rc nginx --replicas=10  Now the image changes to:</description>
    </item>
    
    <item>
      <title>离线安装CoreOS上的Kubernetes</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/26/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85coreos%E4%B8%8A%E7%9A%84kubernetes/</link>
      <pubDate>Mon, 26 Dec 2016 10:09:41 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/26/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85coreos%E4%B8%8A%E7%9A%84kubernetes/</guid>
      <description>先决条件 CoreOS安装iso: coreos_production_iso_image.iso.
https://coreos.com/os/docs/latest/booting-with-iso.html
VirtualBox.
https://www.virtualbox.org/wiki/Downloads
硬盘安装介质, 放置于某web服务器根目录下(这里的根目录是/var/download):
$ pwd /var/download/1185.5.0 $ ls coreos_production_image.bin.bz2 coreos_production_image.bin.bz2.sig  准备硬盘安装介质，需要通过coreos-baremetal项目，从./examples/assets下拷贝相应文件到web服务器根目录下:
 $ git clone https://github.com/coreos/coreos-baremetal # Make a copy of example files $ cp -R coreos-baremetal/examples . # Download the CoreOS image assets referenced in the target profile. $ ./coreos-baremetal/scripts/get-coreos stable 1185.5.0 ./examples/assets  网络配置 三个CoreOS节点IP配置
coreos1: 172.17.8.221 coreos2: 172.17.8.222 coreos3: 172.17.8.223  etcd discovery Server IP: 172.17.8.1.
Virtualbox网络配置如下:
第一块网卡接入到NAT网络，第二块网卡接入到Host-only网络，这也就是在下面的Cloudinit文件中 需要定义的Name=enp0s8字段。
Discovery Server配置 实际上这个Server是运行etcd2的一个物理机，接入172.</description>
    </item>
    
    <item>
      <title>LoadBalancingInCoreOS</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/23/loadbalancingincoreos/</link>
      <pubDate>Fri, 23 Dec 2016 19:01:18 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/23/loadbalancingincoreos/</guid>
      <description>这几天一直在把玩CoreOS， 主要参考的是DigitalOcean上的tutorial以及《CoreOS实践之路》这本书， 奈何文章离如今年代已经久远，一直搭建不成功. 几经周折后终于在一篇guideline 的指导下把负载均衡的服务跑通，这里是搭建该服务的步骤和心得。
参考网址:
http://blog.dixo.net/2015/02/load-balancing-with-coreos/
架构图 该网址中列举的的架构图如下:
刚开始看到这个图是有点发蒙的，这里简单说一下操作步骤，与图中一一对应.
 CoreOS Machine B和Core Machine C是两个CoreOS系统节点，位于其上分别 运行了两个apache容器，B上的容器监听8001端口，C上的容器监听8002端口。
 两个apache容器将自身的IP和端口发布到etcd服务(etcd.service).
 CoreOS Machine A上运行了三个单元，分别是nginx服务、confdata服务、confd服务. 其中nginx服务在配置好的负载均衡后端上分发http请求。confdata服务主要用于 为nginx配置文件共享数据卷。confd服务查看etcd中元数据的变化，根据这些变化 在共享数据卷中写入新的配置文件.
 详细说明一下confd的作用，A. 发现coreos集群中可用的apache容器. B. 实时生成 nginx.conf文件，并将此文件写入到共享存储. C. 写入完成后，通知docker给nginx发送 一个HUP信号. D. Docker发送HUP信号给nginx容器后，容器将重新加载其配置文件。
  以上就是我对架构图的解读。接下来将一步步来实现这个负载均衡。
先决条件 一个3节点的CoreOS集群
有效的/etc/environment文件(有时候需要手动生成) 相关的容器(制作流程见后)
容器镜像制作 Apache容器 在某台安装好Docker的机器上，或者直接在CoreOS节点机上，运行:
$ docker run -i -t ubuntu:14.04 /bin/bash # apt-get update # apt-get install apache2 # sudo bash # echo &amp;quot;&amp;lt;h1&amp;gt;Running from Docker on CoreOS&amp;lt;/h1&amp;gt;&amp;quot; &amp;gt; /var/www/html/index.</description>
    </item>
    
    <item>
      <title>CoreOSTips</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/22/coreostips/</link>
      <pubDate>Thu, 22 Dec 2016 18:02:47 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/22/coreostips/</guid>
      <description>Add additional ssh keys Adding new keys into the deployed system:
# echo &#39;ssh-rsa AAAAB3Nza....... key@host&#39; | update-ssh-keys -a core  Write files Take /etc/environment file for example:
core@coreos1 ~ $ cat /usr/share/oem/cloud-config.yml #cloud-config write_files: - path: /etc/environment permissions: 0644 content: | COREOS_PUBLIC_IPV4=172.17.8.201 COREOS_PRIVATE_IPV4=172.17.8.201  Add User Also add in the file /usr/share/oem/cloud-config.yml, like following:
users: - name: &amp;quot;dash&amp;quot; passwd: &amp;quot;xxxxxxxxxxxxxxxxxx&amp;quot; groups: - &amp;quot;sudo&amp;quot; - &amp;quot;docker&amp;quot; ssh-authorized-keys: - &amp;quot;ssh-rsa ADD ME&amp;quot;  Password could be generated via openssl -1 &amp;quot;YourPasswd&amp;quot;</description>
    </item>
    
    <item>
      <title>TryCoreOS(3)</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/21/trycoreos3/</link>
      <pubDate>Wed, 21 Dec 2016 18:02:15 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/21/trycoreos3/</guid>
      <description>fleetctl Configuration Cluster Status fleetctl list-machines will display all of the nodes in cluster:
core@coreos1 ~ $ fleetctl list-machines MACHINE	IP	METADATA bea5741d...	172.17.8.203	- dd464e69...	172.17.8.202	- f22aee5d...	172.17.8.201	-  fleetctl list-units will list all of the services in cluster:
core@coreos1 ~ $ fleetctl list-units UNIT	MACHINE	ACTIVE	SUB  Nodes Jumping Use ssh-keygen for generating the id_rsa.pub, and add them into other nodes&amp;rsquo;s /home/core/.ssh/authorized_keys.
Start the ssh-agent via:</description>
    </item>
    
    <item>
      <title>TryCoreOS(2)</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/21/trycoreos2/</link>
      <pubDate>Wed, 21 Dec 2016 14:29:28 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/21/trycoreos2/</guid>
      <description>Local Discovery Service Take a look at the yaml configuration file:
 etcd2: # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3 # specify the initial size of your cluster with ?size=X discovery: https://discovery.etcd.io/4add2186302763c8876afd1684ca06fe  This means all of you coreos nodes should reach the internet, what if we deploy a coreos cluster offline? We need to deploy a local discovery service.
ArchLinux etcd2 Example Download the etcd v2.</description>
    </item>
    
    <item>
      <title>TryCoreOS</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/20/trycoreos/</link>
      <pubDate>Tue, 20 Dec 2016 15:35:29 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/20/trycoreos/</guid>
      <description>Preparation Create image file via:
$ sudo mkdir corecluster $ cd corecluster $ qemu-img create -f qcow2 coreos1.qcow2 30G $ qemu-img create -f qcow2 coreos2.qcow2 30G $ qemu-img create -f qcow2 coreos3.qcow2 30G  Create a network named 172.17.8.1/24, dhcp disabled.
Since the vnet interface is occupied via virtualbox, switches to virtualbox installation.
Two Ethernet cards:
Start from CD:
Installation File Preparation For saving time, we use local installation repository, download the installation images via:</description>
    </item>
    
    <item>
      <title>UseMesosForCI</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/15/usemesosforci/</link>
      <pubDate>Thu, 15 Dec 2016 21:19:18 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/15/usemesosforci/</guid>
      <description>参考了http://container-solutions.com/continuous-delivery-with-docker-on-mesos-in-less-than-a-minute/
Nodejs程序 app.js程序如下:
// Load the http module to create an http server. var http = require(&#39;http&#39;); // Configure our HTTP server to respond with Hello World to all requests. var server = http.createServer(function (request, response) { response.writeHead(200, {&amp;quot;Content-Type&amp;quot;: &amp;quot;text/plain&amp;quot;}); response.end(&amp;quot;Hello World&amp;quot;); }); // Listen on port 8000, IP defaults to &amp;quot;0.0.0.0&amp;quot; server.listen(8000); // Put a friendly message on the terminal console.log(&amp;quot;Server running at http://127.0.0.1:8000/&amp;quot;);  配置文件package.json如下:
{ &amp;quot;name&amp;quot;: &amp;quot;hello-world&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;hello world&amp;quot;, &amp;quot;version&amp;quot;: &amp;quot;0.</description>
    </item>
    
    <item>
      <title>VirtualBoxAndVagrantOnXenial</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/09/virtualboxandvagrantonxenial/</link>
      <pubDate>Fri, 09 Dec 2016 10:35:28 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/09/virtualboxandvagrantonxenial/</guid>
      <description>Since the vagrant version and virtualbox version are too old in official repository, we need to upgrade them for using the newest feature, following are the tips.
Virtualbox Manually Install Download URL:
https://www.virtualbox.org/wiki/Downloads
Before installing the newest version, make sure you have uninstalled the old version via `sudo apt-get purge virtualbox*&amp;ldquo;.
Use dpkg for installing the virtualbox then using sudo apt-get -f install for resolving the dependency issue.
After installed the virtualbox, use sudo /sbin/vboxconfig for building the kernel module.</description>
    </item>
    
    <item>
      <title>TipsOnCassandraOnKubernetes</title>
      <link>http://purplepalmdash.github.io/blog/2016/12/06/tipsoncassandraonkubernetes/</link>
      <pubDate>Tue, 06 Dec 2016 19:27:19 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/12/06/tipsoncassandraonkubernetes/</guid>
      <description>First download the image from gcr.io:
$ sudo docker pull gcr.io/google-samples/cassandra:v11  Create a replicas using following yaml file:
apiVersion: v1 kind: ReplicationController metadata: name: cassandra # The labels will be applied automatically # from the labels in the pod template, if not set # labels: # app: cassandra spec: replicas: 1 # The selector will be applied automatically # from the labels in the pod template, if not set. # selector: # app: cassandra template: metadata: labels: app: cassandra spec: containers: - command: - /run.</description>
    </item>
    
    <item>
      <title>BuildCollectdForXenServer</title>
      <link>http://purplepalmdash.github.io/blog/2016/11/30/buildcollectdforxenserver/</link>
      <pubDate>Wed, 30 Nov 2016 15:00:57 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/11/30/buildcollectdforxenserver/</guid>
      <description> 先决条件 启用aliyun CentOS源，安装工具:
$ yum install rpm-build --skip-broken  因为编译可能会占用大量硬盘空间，预先加载某个NFS卷:
# mount -t nfs 192.168.0.221:/xxxx /mnt  下载源码包:
错误！！！ rpm-build安装失败。
不建议在xenserver上手动编译，上网搜索，找到collectd正确的源:
# vim /etc/yum.repos.d/collectd-ci.repo [collectd-ci] name=collectd CI baseurl=http://pkg.ci.collectd.org/rpm/collectd-5.5/epel-5-$basearch enabled=1 gpgkey=http://pkg.ci.collectd.org/pubkey.asc gpgcheck=0 repo_gpgcheck=0 # yum remove collectd &amp;amp;&amp;amp; yum install -y collectd  Trouble-Shooting 如果激活有其他源，则可能会因为优先级顺序，优先安装例如epel里的collectd-i386之类的包， 解决方案是将这些源全盘屏蔽。
[root@xenserver-WolfHunter yum.repos.d]# ls back Citrix.repo collectd-ci.repo  </description>
    </item>
    
    <item>
      <title>MinikubeMyBlog</title>
      <link>http://purplepalmdash.github.io/blog/2016/11/30/minikubemyblog/</link>
      <pubDate>Wed, 30 Nov 2016 10:44:10 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/11/30/minikubemyblog/</guid>
      <description>Generate blog Generate the static blog via:
# hugo --theme=hyde-a  Persist Volume Define a pv:
$ vim blog.yaml kind: PersistentVolume apiVersion: v1 metadata: name: pvblog labels: type: local spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce hostPath: path: &amp;quot;/data/hugoblog&amp;quot;  Create this pv:
$ kubectl create -f blog.yaml persistentvolume &amp;quot;pvblog&amp;quot; created  Create a pv claim:
$ vim blogclaim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: blogclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi  Create this pv claim:</description>
    </item>
    
    <item>
      <title>RunWordPressOnMinikube</title>
      <link>http://purplepalmdash.github.io/blog/2016/11/28/runwordpressonminikube/</link>
      <pubDate>Mon, 28 Nov 2016 11:53:25 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/11/28/runwordpressonminikube/</guid>
      <description>Installation On Ubuntu16.04, first download the deb package from
https://github.com/kubernetes/minikube/releases
Install virtualbox:
$ sudo apt-get install -y virtualbox $ sudo dpkg -i minikube_0.12-2.deb $ which minikube-linux-amd64 /usr/bin/minikube-linux-amd64  Start Cluster First install kubectl:
$ curl -Lo kubectl \ https://storage.googleapis.com/kubernetes-release/release/v1.3.0/bin/linux/amd64/kubectl \ &amp;amp;&amp;amp; chmod +x kubectl &amp;amp;&amp;amp; sudo mv kubectl /usr/local/bin/  Start kubernetes cluster via:
$ minikube-linux-amd64 start Starting local Kubernetes cluster... Downloading Minikube ISO 36.00 MB / 36.00 MB [==============================================] 100.</description>
    </item>
    
    <item>
      <title>WorkingTipsOnKubernetes</title>
      <link>http://purplepalmdash.github.io/blog/2016/11/26/workingtipsonkubernetes/</link>
      <pubDate>Sat, 26 Nov 2016 20:43:20 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/11/26/workingtipsonkubernetes/</guid>
      <description>先决条件 CentOS 7.2 1511, Vagrant for kvm.
关闭selinux, 关闭firewalld, 使用以下命令安装docker最新版:
$ curl -sSL \ http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet | \ sh -  IP地址配置:
master	192.168.0.223 node1	192.168.0.224  配置无密码登录，master到master, master到node1.
# ssh-copy-id root@192.168.0.223 # ssh-copy-id root@192.168.0.224  安装kubernetes 修改配置文件如下:
$ cat kubernetes/cluster/centos/config-default.sh # Master配置 export MASTER=${MASTER:-&amp;quot;root@192.168.0.223&amp;quot;} export MASTER_IP=${MASTER#*@} # Minion节点配置 export NODES=${NODES:-&amp;quot;root@192.168.0.223 root@192.168.0.224&amp;quot;} # Cluster中含有的节点数 export NUM_NODES=${NUM_NODES:-2} # service cluster配置的IP地址范围 export SERVICE_CLUSTER_IP_RANGE=${SERVICE_CLUSTER_IP_RANGE:-&amp;quot;192.168.22.0/24&amp;quot;} # flannel的overlay网络IP地址范围, 不能和上面定义的SERVICE_CLUSTER_IP_RANGE地址范围冲突 export FLANNEL_NET=${FLANNEL_NET:-&amp;quot;172.20.0.0/16&amp;quot;} # Docker参数，这里我们开启daocloud加速模式 export DOCKER_OPTS=${DOCKER_OPTS:-&amp;quot;--cluster-store=etcd://$MASTER_IP:2379, --registry-mirror=http://1a653205.</description>
    </item>
    
    <item>
      <title>XenServerAndDocker</title>
      <link>http://purplepalmdash.github.io/blog/2016/11/24/xenserveranddocker/</link>
      <pubDate>Thu, 24 Nov 2016 09:44:04 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/11/24/xenserveranddocker/</guid>
      <description>XenServer配置 XenServer 6.5 下载xscontainer iso并安装之:
# wget http://downloadns.citrix.com.edgesuite.net/10343/XenServer-6.5.0-SP1-xscontainer.iso # xe-install-supplemental-pack XenServer-6.5.0-SP1-xscontainer.iso  XenServer 7.0 下载并安装:
# wget http://downloadns.citrix.com.edgesuite.net/11621/XenServer-7.0.0-xscontainer.iso # xe-install-supplemental-pack XenServer-7.0.0-xscontainer.iso  guest虚拟机准备 安装docker:
$ curl -sSL http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/experimental/internet | sh $ sudo usermod -aG docker  安装ncat/openssh-server(nmap中包含ncat):
$ sudo apt-get install -y openssh-server nmap  添加guest虚拟机 添加docker monitor:
# xscontainer-prepare-vm -v 05cd5c8f-eb32-86c6-b687-7a296180e3d3 -u dash  添加后的效果如下:</description>
    </item>
    
    <item>
      <title>PlayXenOnUbuntu1604</title>
      <link>http://purplepalmdash.github.io/blog/2016/11/23/playxenonubuntu1604/</link>
      <pubDate>Wed, 23 Nov 2016 17:29:07 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/11/23/playxenonubuntu1604/</guid>
      <description>硬件环境 AMD E-350， 8G内存，320G 硬盘，绝对垃圾配置。
软件环境 Ubuntu16.04, LVM磁盘配置:
$ sudo lvdisplay ..... ..... ......  安装完毕后，安装xen hypervisor:
$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade -y &amp;amp;&amp;amp; sudo apt-get dist-upgrade -y $ sudo apt-get install -y xen-hypervisor-amd64 $ sudo apt-get install -y virtinst virt-manager  更改grub配置，默认使用xen hypervisor内核, 并指定dom0最大可用内存:
$ sudo vim /etc/default/grub GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;Xen 4.1-amd64&amp;quot; GRUB_CMDLINE_XEN=&amp;quot;dom0_mem=1024M,max:1024M dom0_max_vcpus=2&amp;quot; $ sudo update-grub $ sudo reboot  检查:
$ sudo xl list Name ID Mem VCPUs	State Time(s) Domain-0 0 1024 2 r----- 28.</description>
    </item>
    
    <item>
      <title>TurnQcow2IntoVagrantBox</title>
      <link>http://purplepalmdash.github.io/blog/2016/11/07/turnqcow2intovagrantbox/</link>
      <pubDate>Mon, 07 Nov 2016 14:59:09 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/11/07/turnqcow2intovagrantbox/</guid>
      <description>img preparation First you got a qcow2 file, which could let you startup a virtualmachine, so you startup a machine like following:
# cp YourQcowFile.qcow2 ./box.img # qemu-system-x86_64 -net nic -net user,hostfwd=tcp::2222-:22 -hda ./box.img -m 512 --enable-kvm  Login to the machine and run following scripts:
useradd -m vagrant mkdir /home/vagrant/.ssh chmod 700 /home/vagrant/.ssh echo &amp;quot;ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key&amp;quot; &amp;gt; /home/vagrant/.ssh/authorized_keys chmod 600 /home/vagrant/.ssh/authorized_keys chown -R vagrant.vagrant /home/vagrant/.</description>
    </item>
    
    <item>
      <title>WorkTipsOnOpenStackDeployment</title>
      <link>http://purplepalmdash.github.io/blog/2016/08/31/worktipsonopenstackdeployment/</link>
      <pubDate>Wed, 31 Aug 2016 17:29:36 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/08/31/worktipsonopenstackdeployment/</guid>
      <description>Repository Sync The current version of OpenStack could be found in:
https://releases.openstack.org/
Mitaka is the recent version.
Mitaka	Current stable release, security-supported	2016-04-07	2017-04-10  Ubuntu repository could be found at:
http://ubuntu-cloud.archive.canonical.com/ubuntu/dists/
Use apt-mirror for syncing them. Current the ubuntu1404 is well supported.</description>
    </item>
    
    <item>
      <title>OnXenServerBridgeWorkingTips2</title>
      <link>http://purplepalmdash.github.io/blog/2016/08/19/onxenserverbridgeworkingtips2/</link>
      <pubDate>Fri, 19 Aug 2016 14:37:59 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/08/19/onxenserverbridgeworkingtips2/</guid>
      <description>VirtualBox Setup Define a virtual machine:
Use 7G of 8G memory for this VM:
Create a new disk(200G), choose VDI, Dynamically allocated, And specify the location for storing it.
Now create the virtual machine, and configure its networking like following:
CPUs, we allocated 4:
And also the acceleration configuration:
Storage Configuration:
For saving the resources, disable USB/Audio.
Now insert the XenServer Installation CDROM, and install it.
XenServer Configuration IP Address, for bridged networking:</description>
    </item>
    
    <item>
      <title>OnXenServerBridgeWorkingTips</title>
      <link>http://purplepalmdash.github.io/blog/2016/08/18/onxenserverbridgeworkingtips/</link>
      <pubDate>Thu, 18 Aug 2016 19:29:45 +0800</pubDate>
      
      <guid>http://purplepalmdash.github.io/blog/2016/08/18/onxenserverbridgeworkingtips/</guid>
      <description>Background I will use XenServer for testing, while I made a vagrant box of XenServer 6.5, it could work properly in seperated networking, so following I will try to setup a bridged &amp;ldquo;XenServer&amp;rdquo; which will acts like a real physical machine.
Vagrantfile The configuration part is listed as:
 # csagentxen65 node. # Add one networking, modify hostname, define memory, CPU cores. config.vm.define :csagentxen65 do |csagentxen65| csagentxen65.vm.box = &amp;quot;Xen65Box&amp;quot; csagentxen65.</description>
    </item>
    
    <item>
      <title>XenServer Statistics</title>
      <link>http://purplepalmdash.github.io/2016/07/01/xenserver-statistics/</link>
      <pubDate>Fri, 01 Jul 2016 17:45:50 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/07/01/xenserver-statistics/</guid>
      <description>Direct write rrd into graphite, refers to:
$ git clone https://github.com/jgilmour/XenGraphiteIT.git  Then you get the storage pool information fro xsconsole via:
$ xe vdi-list  Notice it will contain the hard disk and iso repositories, use harddisk.
Now edit the .config file:
[XENAPI] URL = http://192.168.10.187 USERNAME = root PASSWORD = xxxxxxx SR-UUID = 51977c4b-8dc2-bcff-b7ad-de7cc5c7e717 [GRAPHITE] CARBON_HOST = 192.168.1.79 CARBON_PORT = 2003 CARBON_NAME = collectd.com.IT.servers.xen.  Run python2 xengraphite.py you could get your XenServer statistic data into your graphite database, enjoy it.</description>
    </item>
    
    <item>
      <title>site-to-site VPN</title>
      <link>http://purplepalmdash.github.io/2016/06/29/site-to-site-vpn/</link>
      <pubDate>Wed, 29 Jun 2016 18:51:40 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/06/29/site-to-site-vpn/</guid>
      <description>Reference Refers to:
https://clauseriksen.net/2011/02/02/ipsec-on-debianubuntu/
And http://xmodulo.com/create-site-to-site-ipsec-vpn-tunnel-openswan-linux.html
Network Topology The topology is listed as following:
Host1 &amp;ndash; LAN1 &amp;ndash; Router1 &amp;ndash;[BIG, BAD INTERNET]&amp;ndash; Router2 &amp;ndash; LAN2 &amp;ndash; Host2
Router1 and Router2 are Ubuntu14.04 machine, which runs in virt-manager,thus you have to create 2 new networks, each in one physical machine.
Physical Machine 1: 192.168.1.79
Router1:
eth0: bridge to physical machine&amp;rsquo;s networking. 192.168.10.100
eth1: 10.47.70.2.
DHCP on eth1.
Physical Machine 2: 192.</description>
    </item>
    
    <item>
      <title>RackHD Worktips</title>
      <link>http://purplepalmdash.github.io/2016/06/07/rackhd-worktips/</link>
      <pubDate>Tue, 07 Jun 2016 17:05:19 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/06/07/rackhd-worktips/</guid>
      <description>Vagrant Preparation rackhd/rackhd vagrant box could be downloaded from following link:
https://atlas.hashicorp.com/rackhd/boxes/rackhd
Clone the repository from the github:
$ pwd /home/dash/Code/Jun13 $ git clone https://github.com/RackHD/RackHD $ cd RackHD  Change into the directory example, create config and run the setup command:
$ cd example $ cp config/monorail_rack.cfg.example config/monorail_rack.cfg  Edits can be made to this new file to adjust the number of pxe clients created.
$ bin/monorail_rack  The monorail_rack script will auto-start all of the services by default, but you can also run them manually if you prefer.</description>
    </item>
    
    <item>
      <title>Use RackHD For Deploying Systems</title>
      <link>http://purplepalmdash.github.io/2016/06/01/use-rakehd-for-deploying-systems/</link>
      <pubDate>Wed, 01 Jun 2016 09:14:18 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/06/01/use-rakehd-for-deploying-systems/</guid>
      <description>Following are the steps for using the RackHD for deploying systems. Mainly refers to http://dojoblog.emc.com/rackhd-cpi/setting-up-rackhd/
But the tutorial from emc includes lots of mistakes, so I listed all of the steps in following chapters.
Vagrant Env Preparation Initialize the vagrant env via(ubuntu1404 is my box name):
$ vagrant init ubuntu1404 A `Vagrantfile` has been placed in this directory. $ vim Vagrantfile config.vm.provider &amp;quot;virtualbox&amp;quot; do |vb| # # Display the VirtualBox GUI when booting the machine # vb.</description>
    </item>
    
    <item>
      <title>Setup LXD On Ubuntu1604</title>
      <link>http://purplepalmdash.github.io/2016/05/11/setup-lxd-on-ubuntu1604/</link>
      <pubDate>Wed, 11 May 2016 15:38:23 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/05/11/setup-lxd-on-ubuntu1604/</guid>
      <description>Preparation By default the lxd is installed in ubuntu1604.
Image The image file are downloaded before we actually install it, install the image via:
$ lxc image import ubuntu-16.04-server-cloudimg-amd64-lxd.tar.xz ubuntu-16.04-server-cloudimg-amd64-root.tar.xz --alias ubuntu1604 $ lxc image list +--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+ | ALIAS | FINGERPRINT | PUBLIC | DESCRIPTION | ARCH | SIZE | UPLOAD DATE | +--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+ | ubuntu1604 | f4c4c60a6b75 | no | Ubuntu 16.04 LTS server (20160420.3) | x86_64 | 137.</description>
    </item>
    
    <item>
      <title>Working Tips On Mesos/Ansible</title>
      <link>http://purplepalmdash.github.io/2016/05/09/working-tips-on-mesos-slash-ansible/</link>
      <pubDate>Mon, 09 May 2016 12:20:34 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/05/09/working-tips-on-mesos-slash-ansible/</guid>
      <description>Package Prepare We have the default vagrant box generated by bento, listed it via:
➜ mesos vagrant box list | grep -i centos | grep -i virtualbox centos72 (virtualbox, 0)  Now we want to generate a new box from it, and added our own configuration:
$ vagrant init centos72 $ vagrant up $ vagrant ssh  Edit for keeping the cache:
$ cat /etc/yum.conf | more [main] cachedir=/home/vagrant/rpms/$basearch/$releasever #keepcache=0 keepcache=1  Now poweroff the machine and export it to the new box:</description>
    </item>
    
    <item>
      <title>Working Tips on Ansible-cobbler(2)</title>
      <link>http://purplepalmdash.github.io/2016/05/06/working-tips-on-ansible-cobbler-2/</link>
      <pubDate>Fri, 06 May 2016 15:03:59 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/05/06/working-tips-on-ansible-cobbler-2/</guid>
      <description>AIM Change the vagrant box to libvirt, and let this libvirt machine working properly.
Image Transformation Transform the image via following command:
$ VBoxManage clonehd /home/dash/VirtualBox\ VMs/ansible-cobbler_cobbler-ubuntu_1462410925173_15793/packer-virtualbox-iso-1454031074-disk1.vmdk /home/dash/output.img --format raw &amp;amp;&amp;amp; qemu-img convert -f raw /home/dash/output.img -O qcow2 /home/dash/ansible-cobbler.qcow2 0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100% Clone medium created in format &#39;raw&#39;. UUID: 6fbb99be-8004-43b5-831b-ec794a001c10  Qemu Virtual Machine Create a new machine, then configure the networking, edit the cobbler setting/dhcp setting, then cobbler sync, now the cobbler is adjusting to new environment.</description>
    </item>
    
    <item>
      <title>Working Tips On ansible-cobbler</title>
      <link>http://purplepalmdash.github.io/2016/05/04/working-tips-on-ansible-cobbler/</link>
      <pubDate>Wed, 04 May 2016 15:36:55 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/05/04/working-tips-on-ansible-cobbler/</guid>
      <description>Source The source are downloaded from:
https://github.com/signed8bit/ansible-cobbler
Git clone it via:
$ git clone https://github.com/signed8bit/ansible-cobbler.git  Test The test will be done via vagrant up, while we met the problem: the cobbler version in ansible playbooks are too old, thus the command cobbler get-loaders won&amp;rsquo;t acts well. we have to changing to the newest cobbler version which is available in
http://cobbler.github.io/
Manually Steps(Ubuntu) Install the newest cobbler via:
$ wget -qO - http://download.</description>
    </item>
    
    <item>
      <title>TipsOnOSExperiment</title>
      <link>http://purplepalmdash.github.io/2016/04/24/tipsonosexperiment/</link>
      <pubDate>Sun, 24 Apr 2016 17:32:12 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/04/24/tipsonosexperiment/</guid>
      <description>为了在家里验证一下DevStack的网络配置，组建了一个网络，涉及到的点比较多，以下是 具体记录。
交换机配置 前段时间从美国亚马逊买回来一个TP-LINK的千兆交换机一直没用起来，型号是TL-SG108E ，8口可网管交换机。但前面有写过文章可以用来参考:
http://purplepalmdash.github.io/blog/2015/12/12/ba-wan-tl-sg108e/
但这篇文章里讲的主要还是ovs后虚拟机的vlan，和最近要做的DevStack的FloatIP配置稍 微有点差异。
网络规划 家里已有网络192.168.177.0/24, 这个网络是可以访问Internet的。所以DevStack机器上 的eth0将连接到这个网段，并在其上分配floating IP.
另外我们需要创建一个VLAN隔离的private network，用于给DevStack里的虚拟机默认启 动后分配IP地址。将DevStack机器上的eth1连接到此网络。
配置交换机 TP-LINK的DEB-100网卡是比较皮实，奈何win10驱动需要找，随便找了个淘宝9.9包邮的 USB转10兆网卡连上SurfacePro，开始配置交换机。
步骤:
打开桌面的Easy Smart Configuration Utility，开始自动发现局域网内的交换机，如 下图:
需要重新配置下USB有线网卡的IP地址才能连接上交换机:
双击发现的交换机，用admin/admin登录后的界面如下:
以前我曾经把这个交换机配置成802.1Q VLAN, 这次基于端口来隔离，所以要配置成Port Based VLAN, 配置完毕后的画面如下:
这种基于端口VLAN的验证方法很简单，将上网机和宽带路由器分别插在1～4和5～8口，即 可测试出VLAN被端口隔离。但这好像不是DevStack中需要设置的。
还是继续配置802.1Q VLAN. 值得注意的是，如果之前配置端口VLAN时将SurfacePro的连 接和交换机网段隔离了，那可能会连接不上，换回同一VLAN即可连接上。
配置vlan100如以下图所示:
测试VLAN 将两台PC连接在5~8口上，VLAN100。
PC1, ArchLinux, Systemd配置VLAN:
$ cat enp0s25.100.network [Match] Name=enp0s25.100 [Network] DNS=192.168.2.1 Address=192.168.2.1/24 $ cat enp0s25.100.netdev [NetDev] Name=enp0s25.100 Kind=vlan [VLAN] Id=100 [Network] DNS=192.168.2.1 Address=192.168.2.1/24 Gateway=192.168.2.1 $ pwd /etc/systemd/network  重新启动PC1后，192.</description>
    </item>
    
    <item>
      <title>在DevStack中使用Packer</title>
      <link>http://purplepalmdash.github.io/2016/04/24/zai-devstackzhong-shi-yong-packer/</link>
      <pubDate>Sun, 24 Apr 2016 10:16:39 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/04/24/zai-devstackzhong-shi-yong-packer/</guid>
      <description> 导入源镜像 源镜像可以从ubuntu.com下载到，并使用以下命令导入:
$ wget https://cloud-images.ubuntu.com/trusty/current/trusty-server-cloudimg-amd64-disk1.img $ glance image-create --name ubuntu-trusty --disk-format qcow2 \ --container-format bare --file trusty-server-cloudimg-amd64-disk1.img  openstack.json文件 github上有现成的，克隆到本地:
$ git clone https://github.com/Thingee/packer-devstack.git  </description>
    </item>
    
    <item>
      <title>rsync the vault.centos.com</title>
      <link>http://purplepalmdash.github.io/2016/04/22/rsync-the-vault-dot-centos-dot-com/</link>
      <pubDate>Fri, 22 Apr 2016 19:39:58 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/04/22/rsync-the-vault-dot-centos-dot-com/</guid>
      <description>For I want to do some configuration workings on old distribution of CentOS, I have to use lots of materials which from vault.centos.com, following are the steps for syncing them.
First, rsync in vault.centos.com is closed, thus we have to choose http://archive.kernel.org/centos/.
Rsync Scripts Refers to:
https://www.totalnetsolutions.net/2013/10/02/setting-up-a-corporate-yum-repository-mirror-for-bandwidth-and-staged-update-management/
Make Repository https://wiki.centos.org/HowTos/CreateLocalMirror</description>
    </item>
    
    <item>
      <title>DevStack Enable Neutron</title>
      <link>http://purplepalmdash.github.io/2016/04/20/devstack-enable-neutron/</link>
      <pubDate>Wed, 20 Apr 2016 11:11:54 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/04/20/devstack-enable-neutron/</guid>
      <description>Steps Install steps are listed as following:
First as root user, create the initial stack user via:
# git clone https://git.openstack.org/openstack-dev/devstack # tools/create-stack-user.sh # passwd stack  Now login with user stack, clone the repository and begin devstack installation:
$ git clone https://git.openstack.org/openstack-dev/devstack $ cd devstack $ git checkout stable/liberty $ cp samples/local.conf ./ $ vim local.conf  The local.conf file should added following items:
 # use TryStack git mirror GIT_BASE=http://git.</description>
    </item>
    
    <item>
      <title>Change IP In DevStack</title>
      <link>http://purplepalmdash.github.io/2016/04/18/change-ip-in-devstack/</link>
      <pubDate>Mon, 18 Apr 2016 12:15:35 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/04/18/change-ip-in-devstack/</guid>
      <description>Tips After changing the IP Address of the DevStack machine, do following for re-installing the envs:
$ ssh stack@Your_IP stack@packer-PerforceTest:~$ pwd /opt/stack stack@packer-PerforceTest:~$ cd devstack/ stack@packer-PerforceTest:~/devstack$ ./unstack.sh &amp;amp;&amp;amp; ./stack.sh  Now visiting your http://Your_New_IP/dashboard you will got the openstack dashborad.</description>
    </item>
    
    <item>
      <title>Extend Vagrant&#39;s Disk</title>
      <link>http://purplepalmdash.github.io/2016/04/05/extend-vagrants-disk/</link>
      <pubDate>Tue, 05 Apr 2016 15:30:35 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/04/05/extend-vagrants-disk/</guid>
      <description>In Vagrantfile, edit the following definition:
 config.vm.provider &amp;quot;virtualbox&amp;quot; do |vb| # # Display the VirtualBox GUI when booting the machine # vb.gui = true # # # Customize the amount of memory on the VM: vb.memory = &amp;quot;1024&amp;quot; file_to_disk = File.realpath( &amp;quot;.&amp;quot; ).to_s + &amp;quot;/disk.vdi&amp;quot; if ARGV[0] == &amp;quot;up&amp;quot; &amp;amp;&amp;amp; ! File.exist?(file_to_disk) puts &amp;quot;Creating 5GB disk #{file_to_disk}.&amp;quot; vb.customize [ &#39;createhd&#39;, &#39;--filename&#39;, file_to_disk, &#39;--format&#39;, &#39;VDI&#39;, &#39;--size&#39;, 5000 * 1024 # 5 GB ] vb.</description>
    </item>
    
    <item>
      <title>Monitoring XenServer</title>
      <link>http://purplepalmdash.github.io/2016/03/29/monitoring-xenserver/</link>
      <pubDate>Tue, 29 Mar 2016 12:23:50 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/03/29/monitoring-xenserver/</guid>
      <description>Prerequisition Refers tohttps://github.com/crashdump/collectd-xenserver
 Collected 4.9 or later Python2.4 Or Later sudo pip install XenAPI sudo pip install collectd  Configuration Make a directory under /etc/collectd folder, and copy the collectd-xenserver.py into this folder:
$ sudo mkdir -p /var/collectd/plugins $ sudo cp YourDictory/collectd-xenserver.py /var/collectd/plugins/collectd_xenserver.py  Now edit the configuration file of /etc/collectd/collectd.conf:
&amp;lt;LoadPlugin python&amp;gt; Globals true &amp;lt;/LoadPlugin&amp;gt; &amp;lt;Plugin python&amp;gt; ModulePath &amp;quot;/etc/collectd/plugins/&amp;quot; #LogTraces true #Interactive true Import &amp;quot;collectd_xenserver&amp;quot; &amp;lt;Module &amp;quot;collectd_xenserver&amp;quot;&amp;gt; &amp;lt;Host &amp;quot;192.</description>
    </item>
    
    <item>
      <title>XenServer6.2切换管理端口</title>
      <link>http://purplepalmdash.github.io/2016/03/24/cloudstackqie-huan-guan-li-duan-kou/</link>
      <pubDate>Thu, 24 Mar 2016 17:28:55 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/03/24/cloudstackqie-huan-guan-li-duan-kou/</guid>
      <description>默认安装完的XenServer，不能找到eth1, 用以下命令找到eth1:
[root@csagentv1 ~]# xe pif-list uuid ( RO) : cd4409d4-b2b8-543c-ea9c-35170673e924 device ( RO): eth0 currently-attached ( RO): true VLAN ( RO): -1 network-uuid ( RO): 597114f0-e71a-34fe-d6a2-230cc75e085a [root@csagentv1 ~]# xe host-list uuid ( RO) : 367ebe92-0634-41a8-825a-cd23184824ea name-label ( RW): csagentv1 name-description ( RW): Default install of XenServer [root@csagentv1 ~]# xe pif-scan host-uuid=367ebe92-0634-41a8-825a-cd23184824ea [root@csagentv1 ~]# xe pif-list uuid ( RO) : 3f6e551b-993e-0a3d-96b6-0f0d172f867f device ( RO): eth1 currently-attached ( RO): false VLAN ( RO): -1 network-uuid ( RO): 1ff03ece-8b93-b231-ac2d-679d035422da uuid ( RO) : cd4409d4-b2b8-543c-ea9c-35170673e924 device ( RO): eth0 currently-attached ( RO): true VLAN ( RO): -1 network-uuid ( RO): 597114f0-e71a-34fe-d6a2-230cc75e085a  在Console上可以看到管理端口:</description>
    </item>
    
    <item>
      <title>Setup Vagrant-libvirt Env On Ubuntu15.04</title>
      <link>http://purplepalmdash.github.io/2016/03/23/setup-vagrant-libvirt-env-on-ubuntu15-dot-04/</link>
      <pubDate>Wed, 23 Mar 2016 20:49:52 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/03/23/setup-vagrant-libvirt-env-on-ubuntu15-dot-04/</guid>
      <description>For continue working at home, I have to install vagrant-libvirt on Ubuntu15.04, following are steps:
Vagrant Installation The vagrant version in repository is too old, examine it via:
$ apt-cache policy vagrant vagrant: Installed: (none) Candidate: 1.6.5+dfsg1-2 Version table: 1.6.5+dfsg1-2 0 500 http://mirrors.aliyun.com/ubuntu/ vivid/universe amd64 Packages  Download the installation file in:
https://releases.hashicorp.com/vagrant/1.8.1/vagrant_1.8.1_x86_64.deb
Install it via:
$ sudo dpkg -i vagrant_1.8.1_x86_64.deb $ which vagrant /usr/bin/vagrant  Vagrant-libvirt For building vagrant-libvirt, we have to install following packages:</description>
    </item>
    
    <item>
      <title>Use Vagrant To Manage XenServer</title>
      <link>http://purplepalmdash.github.io/2016/03/21/use-vagrant-to-manage-xenserver/</link>
      <pubDate>Mon, 21 Mar 2016 14:20:09 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/03/21/use-vagrant-to-manage-xenserver/</guid>
      <description>Building Templates Build XenServer 6.2 Template is pretty easy, simply do following:
$ git clone https://github.com/imduffy15/packer-xenserver.git $ cd packer-xenserver $ packer build template.iso  After building, check the following box file available under the directory:
$ ls -l -h XenServer.box -rw-rw-r-- 1 dash dash 708M 3月 21 14:41 XenServer.box  Import box File Import the generated box file via:
$ vagrant box add XenServer.box --name &amp;quot;XenServer62&amp;quot; $ vagrant box list | grep XenServer62 XenServer62 (virtualbox, 0)  Start the Virtualbox XenServer $ mkdir XenServer62 $ cd XenServer62 $ vim Vagrantfile Vagrant.</description>
    </item>
    
    <item>
      <title>Vagrant-libvirt Playing</title>
      <link>http://purplepalmdash.github.io/2016/03/16/vagrant-libvirt-playing/</link>
      <pubDate>Wed, 16 Mar 2016 10:31:53 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/03/16/vagrant-libvirt-playing/</guid>
      <description>最终目的是用vagrant实现CloudStack+Xenserver的自动化部署。
CentOS6.7 box Creating 用packer生成CentOS6.7 amd64的镜像，这个镜像默认是virtualbox兼容的，用vagrant-mutate插件 将其转换为libvirt可用的box镜像:
# vagrant mutate centos-6.7.virtualbox.box libvirt # cd /root/.vagrant.d/boxes # ls centos-6.7.virtualbox trusty64 # mv centos-6.7.virtualbox/ centos6764 # vagrant box list centos6764 (libvirt, 0) trusty64 (libvirt, 0)  创建Vagrantfile文件启动一个实验性质的虚拟机：
# pwd /media/opensusue/dash/Code/Vagrant/CentOS2New # ls Vagrantfile Vagrantfile~ # cat Vagrantfile # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(2) do |config| # The most common configuration options are documented and commented below. # For a complete reference, please see the online documentation at # https://docs.</description>
    </item>
    
    <item>
      <title>用Vagrant管理libvirt</title>
      <link>http://purplepalmdash.github.io/2016/03/13/yong-vagrantguan-li-libvirt/</link>
      <pubDate>Sun, 13 Mar 2016 16:07:59 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/03/13/yong-vagrantguan-li-libvirt/</guid>
      <description>先决条件 Vagrant为0.8.1.
参考:
http://linuxsimba.com/vagrant.html
http://linuxsimba.com/vagrant-libvirt-install/
Ubuntu设置 考虑到天朝防火墙的存在,需要经过以下命令才能安装vagrant-libvirt插件:
$ sudo apt-get install -y libvirt-dev ruby-dev $ gem source -r https://rubygems.org/ $ gem source -a http://mirrors.aliyun.com/rubygems/ $ gem install ruby-libvirt -v &#39;0.6.0&#39; $ gem install vagrant-libvirt -v &#39;0.0.32&#39; $ vagrant plugin install vagrant-libvirt $ vagrant plugin list vagrant-libvirt (0.0.32) $ axel http://linuxsimba.com/vagrantbox/ubuntu-trusty.box $ vagrant box add ./ubuntu-trusty.box --name &amp;quot;trusty64&amp;quot;  ArchLinux设置 按照ArchLinux wiki的方法,安装vagrant-libvirt插件:
 # in case it&#39;s already installled vagrant plugin uninstall vagrant-libvirt # vagrant&#39;s copy of curl prevents the proper installation of ruby-libvirt sudo mv /opt/vagrant/embedded/lib/libcurl.</description>
    </item>
    
    <item>
      <title>virtio-gpu working tips</title>
      <link>http://purplepalmdash.github.io/2016/01/31/virtio-gpu-working-tips/</link>
      <pubDate>Sun, 31 Jan 2016 11:49:13 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/01/31/virtio-gpu-working-tips/</guid>
      <description>System Install Fedora 23, select fedora server and install the system, then using the lastest development kernel via:
$ curl -s https://repos.fedorapeople.org/repos/thl/kernel-vanilla.repo | sudo tee /etc/yum.repos.d/kernel-vanilla.repo $ sudo dnf --enablerepo=kernel-vanilla-mainline update  Checking the running kernel via:
$ uname -r 4.5.0-0.rc1.git0.1.vanilla.knurd.1.fc23.x86_64  Running the system which have kernel version newer than 4.4 is the basis for enable the virt-io.
Install packages $ sudo dnf install -y gcc zlib-devel glib2-devel pixman-devel libfdt-devel \ lzo-devel snappy-devel bzip2-devel libseccomp-devel gtk2-devel gtk3-devel \ gnutls-devel vte-devel SDL-devel librdmacm-devel libuuid-devel \ libcap-ng-devel libcurl-devel ceph-devel libssh2-devel libaio-devel \ glusterfs-devel glusterfs-api-devel numactl-devel gperftools-devel \ texinfo libiscsi-devel spice-server-devel libusb-devel usbredir-devel \ libnfs-devel libcap-devel libattr-devel  virglrenderer coprs have the repository for this:</description>
    </item>
    
    <item>
      <title>Read Digest On KVM</title>
      <link>http://purplepalmdash.github.io/2016/01/26/read-digest-on-kvm/</link>
      <pubDate>Tue, 26 Jan 2016 09:12:26 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2016/01/26/read-digest-on-kvm/</guid>
      <description>Some Words VMM: (Virtual Machine Monitor)
VMX: (Virtual Machine eXtensions): instructions on processors with x86 virtualization.
Virtualization software: is most often used to emulate a complete computer system in order to allow a guest operating system to be run, for example allowing Linux to run as a guest on top of a PC that is natively running a Microsoft Windows operating system (or the inverse, running Windows as a guest on Linux).</description>
    </item>
    
    <item>
      <title>Integrate FreeNAS</title>
      <link>http://purplepalmdash.github.io/2015/12/30/integrate-freenas/</link>
      <pubDate>Wed, 30 Dec 2015 09:56:50 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/30/integrate-freenas/</guid>
      <description>Use Virtualbox for integrating FreeNAS.
Add Disks Add a new SCSI controller and two disks:
Then in FreeNAS, import this new disks via:
Volume Manager Add volume of added 2 disks:
Continue:
After added:
ISCSI Sharing Create new sharing:
Add portal:
Add Initiator:
Add Target:
Add Extent:
Extend options:
Associate:
Enable the iscsi service:
Integration Integaration with cloudstack would be looked like following:</description>
    </item>
    
    <item>
      <title>Re-Scan XenServer NICs</title>
      <link>http://purplepalmdash.github.io/2015/12/28/re-scan-xenserver-nics/</link>
      <pubDate>Mon, 28 Dec 2015 15:25:45 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/28/re-scan-xenserver-nics/</guid>
      <description>Since OpenXenManager could not scan the NICs, we need to scan the newly added NICs under terminal.
# xe pif-list # xe host-list # xe pif-scan host-uuid=a7991728-a86f-4a9c-a163-9b819e444488  Via xe host-list we could get the host-uuid, then pif-scan it via this host-uuid.
Now you could see the newly added NICs in openxenmanager:</description>
    </item>
    
    <item>
      <title>CloudMonkey Issue</title>
      <link>http://purplepalmdash.github.io/2015/12/22/cloudmonkey-issue/</link>
      <pubDate>Tue, 22 Dec 2015 21:18:21 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/22/cloudmonkey-issue/</guid>
      <description>Could not start cloudmonkey After installing cloudstack, cloudmonkey couldnot be used, the reason is listed as following:
% cloudmonkey Traceback (most recent call last): File &amp;quot;/usr/local/bin/cloudmonkey&amp;quot;, line 5, in &amp;lt;module&amp;gt; from pkg_resources import load_entry_point File &amp;quot;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py&amp;quot;, line 2603, in &amp;lt;module&amp;gt; working_set.require(__requires__) File &amp;quot;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py&amp;quot;, line 666, in require needed = self.resolve(parse_requirements(requirements)) File &amp;quot;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py&amp;quot;, line 565, in resolve raise DistributionNotFound(req) # XXX put more info here pkg_resources.DistributionNotFound: requests  The solution is via:</description>
    </item>
    
    <item>
      <title>NetScaler VPX初始化配置</title>
      <link>http://purplepalmdash.github.io/2015/12/17/netscaler-vpxchu-shi-hua-pei-zhi/</link>
      <pubDate>Thu, 17 Dec 2015 14:08:35 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/17/netscaler-vpxchu-shi-hua-pei-zhi/</guid>
      <description>初始化配置 启动虚拟机以后，通过nsroot/nsroot登录入VPX.
清除所有配置:
如下，做IP配置:
初始化配置完毕以后，即可在web后台进行配置。
License 申请license的时候注意，选择的MAC地址不能有任何的:符号， 例如52:54:00:这种就不能通过成 功。 在Netscaler上可以通过以下命令查看host id:
root@ns# lmutil lmhostid lmutil - Copyright (c) 1989-2013 Flexera Software LLC. All Rights Reserved. The FlexNet host ID of this machine is &amp;quot;xxxxxxx&amp;quot;  查看激活后的license情形:
&amp;gt; sh license License status: Web Logging: YES Surge Protection: YES Load Balancing: YES Content Switching: YES ....  参考:
http://sam.yeung.blog.163.com/blog/static/222663482013811102013782/</description>
    </item>
    
    <item>
      <title>OpenVswitch On Ubuntu14.04</title>
      <link>http://purplepalmdash.github.io/2015/12/10/openvswitch-on-ubuntu14-dot-04/</link>
      <pubDate>Thu, 10 Dec 2015 15:33:24 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/10/openvswitch-on-ubuntu14-dot-04/</guid>
      <description>Installation Install openvswitch via:
# apt-get update # apt-get install -y openvswitch-common openvswitch-switch  List the installed module via:
# lsmod | grep open openvswitch 66901 0 gre 13808 1 openvswitch vxlan 37619 1 openvswitch libcrc32c 12644 1 openvswitch # ovs-vsctl --version ovs-vsctl (Open vSwitch) 2.0.2 Compiled May 13 2015 18:49:53  Configuration Edit the configuration of the networking:
$ sudo vim /etc/network/interfaces ########################################### ## By using openVswitch, we enabled the following ########################################### auto ovsbr0 iface ovsbr0 inet static address 192.</description>
    </item>
    
    <item>
      <title>tessera how-to</title>
      <link>http://purplepalmdash.github.io/2015/12/08/tessera-how-to/</link>
      <pubDate>Tue, 08 Dec 2015 10:26:48 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/12/08/tessera-how-to/</guid>
      <description>Installation Installation steps are listed as following:
# apt-get install -y python-virtualenv # git clone git@github.com:urbanairship/tessera.git # cd tessera # virtualenv . # . bin/activate # cd tessera-server/ # pip install -r requirements.txt # pip install -r dev-requirements.txt # cd ../tessera-frontend # apt-get install -y npm # npm install -g grunt-cli # npm install # ln -s /usr/bin/nodejs /usr/bin/node # grunt # which inv /root/Code/second/tessera/bin/inv  Start Start the service via:</description>
    </item>
    
    <item>
      <title>Spice Client</title>
      <link>http://purplepalmdash.github.io/2015/11/16/spice-client/</link>
      <pubDate>Mon, 16 Nov 2015 21:20:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/11/16/spice-client/</guid>
      <description>Image Conversion Convert the vdi files into qcow2 file:
$ qemu-img convert -f vdi -O qcow2 Windows81.vdi Windows81.qcow2  Then continue to create the virtual machine via importing the img.
Spice Client Using virtviewer for view the remote machine.
$ sudo pacman -S virtviewer $ remote-viewer spice://localhost:5900  Or you could view the desktop via spicec.
The listening port could be view via netstat -anp | grep 5900.</description>
    </item>
    
    <item>
      <title>Thinking In VR</title>
      <link>http://purplepalmdash.github.io/2015/10/29/thinking-in-vr/</link>
      <pubDate>Thu, 29 Oct 2015 16:00:39 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/10/29/thinking-in-vr/</guid>
      <description>Restriction of the connections.
http://www.cnblogs.com/cmt/archive/2013/03/13/2957583.html http://bbs.m0n0china.org/viewthread.php?tid=16459 http://my.oschina.net/u/1169079/blog/397705  </description>
    </item>
    
    <item>
      <title>OpenStack Liberty安装(1)</title>
      <link>http://purplepalmdash.github.io/2015/10/25/openstack-libertyan-zhuang-1/</link>
      <pubDate>Sun, 25 Oct 2015 12:05:58 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/10/25/openstack-libertyan-zhuang-1/</guid>
      <description>初始化准备 用Packer.io制作Ubuntu14.04的qcow2文件镜像, 设定磁盘大小为100G, lvm分区.
网络: 在virt-manager中制作一个网段为10.0.0.0/24的网段. 所有创建虚拟机的eth0均 加入到此网络中.
To be Continued.</description>
    </item>
    
    <item>
      <title>Tips On Ceph On Docker</title>
      <link>http://purplepalmdash.github.io/2015/10/23/tips-on-ceph-on-docker/</link>
      <pubDate>Fri, 23 Oct 2015 22:32:17 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/10/23/tips-on-ceph-on-docker/</guid>
      <description>Installation Pull the docker image via:
$ sudo docker pull ceph/demo  Run Ceph Run the container via:
# sudo docker run -d --net=host -e MON_IP=192.168.10.190 -e CEPH_NETWORK=192.168.10.0/24 ceph/demo  View the docker instance via:
# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cbe567594adb ceph/demo &amp;quot;/entrypoint.sh&amp;quot; About an hour ago Up About an hour furious_hopper # docker exec -it cbe567594adb  Ceph Operation View the ceph processes:</description>
    </item>
    
    <item>
      <title>On VM Performance Test</title>
      <link>http://purplepalmdash.github.io/2015/10/19/on-vm-performance-test/</link>
      <pubDate>Mon, 19 Oct 2015 09:54:45 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/10/19/on-vm-performance-test/</guid>
      <description>AIM To build the testing Framework.
Reference Material Paper_16-Performance_Evaluation_of_Private_Clouds.pdf
http://www.junauza.com/2012/05/best-system-benchmarking-tools-for.html
Software Install following software:
 # apt-get install -y libx11-dev libgl1-mesa-dev libxext-dev perl perl-modules make gcc nfs-common postgresql-9.1 postgresql-contrib-9.1 mbw iperf  CPU The following software are introduced for testing CPU Performance: * Linpack * Lookbusy
Linpack Note: Only works on INTEL CPU.
Linux
Windows
Mac
Linux Steps:
$ tar xvf l_lpk_p_10.3.4.007.tgz $ cd linpack_10.3.4/benchmarks/linpack $ ./runme_xeon64  You can see the testing result via tail -f lin_xeon64.</description>
    </item>
    
    <item>
      <title>Tips on Cloud-Init and CloudStack(2)</title>
      <link>http://purplepalmdash.github.io/2015/10/14/tips-on-cloud-init-and-cloudstack/</link>
      <pubDate>Wed, 14 Oct 2015 11:36:51 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/10/14/tips-on-cloud-init-and-cloudstack/</guid>
      <description> Cloudmonkey Resize First deploy the VM in stopped status:
(local) mycloudmonkey&amp;gt; deploy virtualmachine startvm=false serviceofferingid=683f31f8-a939-468e-b4de-4512a8ccff8e templateid=13fb2961-533e-4a7d-80f9-21d860269aad zoneid=78509dc3-c828-429c-8154-9fffbc09384c networkids=7c6e7e6b-6aa2-4f95-a835-8d18bf930061 name=testuserdata50G userdata=&#39;...`  Now resize the root volume into 50G size:
 deploy virtualmachine serviceofferingid=ff775183-282b-48d7-b08e-eff51fef7683 templateid=67ca66ea-b021-4f91-ac8c-ff95f2576c9d zoneid=1a258852-831c-4612-94f4-2551a98667bb name=testuserdata userdata=&#39;Q29udGVudC1UeXBlOiBtdWx0aXBhcnQvbWl4ZWQ7IGJvdW5kYXJ5PSI9PT09PT09PT09PT09PT0xOTk5MDU5OTcyMjA5ODg1MjY2PT0iCk1JTUUtVmVyc2lvbjogMS4wCgotLT09PT09PT09PT09PT09PTE5OTkwNTk5NzIyMDk4ODUyNjY9PQpDb250ZW50LVR5cGU6IHRleHQveC1zaGVsbHNjcmlwdDsgY2hhcnNldD0idXMtYXNjaWkiCk1JTUUtVmVyc2lvbjogMS4wCkNvbnRlbnQtVHJhbnNmZXItRW5jb2Rpbmc6IDdiaXQKQ29udGVudC1EaXNwb3NpdGlvbjogYXR0YWNobWVudDsgZmlsZW5hbWU9ImhlbGxvX3dvcmxkLnNoIgoKIyEvYmluL2Jhc2gKZWNobyAiaGVsbG8gd29ybGQhIiA+PiAvcm9vdC90ZXN0CgotLT09PT09PT09PT09PT09PTE5OTkwNTk5NzIyMDk4ODUyNjY9PQpDb250ZW50LVR5cGU6IHRleHQvY2xvdWQtY29uZmlnOyBjaGFyc2V0PSJ1cy1hc2NpaSIKTUlNRS1WZXJzaW9uOiAxLjAKQ29udGVudC1UcmFuc2Zlci1FbmNvZGluZzogN2JpdApDb250ZW50LURpc3Bvc2l0aW9uOiBhdHRhY2htZW50OyBmaWxlbmFtZT0ibXktdXNlci1kYXRhIgoKI2Nsb3VkLWNvbmZpZwpncm93cGFydDoKICBtb2RlOiBhdXRvCmNocGFzc3dkOiB7IGV4cGlyZTogRmFsc2UgfQpzc2hfcHdhdXRoOiBUcnVlCgpzc2hfYXV0aG9yaXplZF9rZXlzOgogLSBzc2gtcnNhIEFBQUFCM056YUMxeWMyRUFBQUFEQVFBQkFBQUJBUUNzMFA4aFNCM05qN2tmd2lRT01PQ0Z2RXVqd3JLZjVuUFdmdzdzbmplVzd3TnhCYi9pTHhqbGxIK0tJdjdpS0dRaGI5WGtpZ3dXelhjdktSRk9OQTF0UU5CUHBsUE9RQXhHYUpoYzcxYlhZTVRabWsxcmZ5L0U4bUZIQmJ3U0trdm04Z3oxaFVqQWFITHdiZ21iaUE3eUNDUkVXbVR1SWpudm1FZnJXYU92WERRZFFPb2RkSzFhZThKM3BnRUNtQ21mRldrQmR3Y1JaN05jTUxBSkVkYTNpYWtJbWdaR2NqTWNCc1hjUUNOcjN1RGlKbERvc1V6Mjg4L3grTnZteTlzcHZnc2x4RXVUV0VQWFRGY1l5eVBrUHdkTnlpQm5TaWFoZTExcUdUZkk0Z2IyWllEb3JDZU5Ca1QxdkVaY0psL1JqT3NKRUFXT04rbno3Nm16MmdhZCByb290QHItOS1WTSAKCnRpbWV6b25lOiBBc2lhL0Nob25ncWluZwoKLS09PT09PT09PT09PT09PT0xOTk5MDU5OTcyMjA5ODg1MjY2PT0tLQo=&#39;  </description>
    </item>
    
    <item>
      <title>Cloud-Init Grow Partition</title>
      <link>http://purplepalmdash.github.io/2015/10/13/cloud-init-grow-partition/</link>
      <pubDate>Tue, 13 Oct 2015 14:37:57 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/10/13/cloud-init-grow-partition/</guid>
      <description>Resize QCOW2 Resize qcow2 file via:
$ qemu-img info my_vm.img image: my_vm.img file format: qcow2 virtual size: 2.2G (2361393152 bytes) disk size: 904M cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false $ qemu-img resize my_vm.img +100G Image resized. $ qemu-img info my_vm.img image: my_vm.img file format: qcow2 virtual size: 102G (109735575552 bytes) disk size: 904M cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false  Enlarge Partition Modify the meta-data, and enable the partition grow options in user-data:</description>
    </item>
    
    <item>
      <title>Tips On OZ</title>
      <link>http://purplepalmdash.github.io/2015/10/12/tips-on-oz/</link>
      <pubDate>Mon, 12 Oct 2015 12:27:04 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/10/12/tips-on-oz/</guid>
      <description>Reference The reference URL is located at:
http://www.chenshake.com/oz-making-centos-mirror/
Installation On CentOS 7, install oz via:
# yum install -y oz # rpm -qa | grep oz- oz-0.14.0-1.el7.noarch  Configuration The configuration file for oz should be configured as:
# vim /etc/oz/oz.cfg [paths] output_dir = /var/lib/libvirt/images data_dir = /var/lib/oz screenshot_dir = /var/lib/oz/screenshots # sshprivkey = /etc/oz/id_rsa-icicle-gen [libvirt] uri = qemu:///system #image_type = raw image_type = qcow2 # type = kvm bridge_name = virbr0 cpus = 1 memory = 1024  The configuration file for the oz should have one tdl file and one kickstart file:</description>
    </item>
    
    <item>
      <title>Tips On Cloud-Init</title>
      <link>http://purplepalmdash.github.io/2015/10/12/tips-on-cloud-init/</link>
      <pubDate>Mon, 12 Oct 2015 10:50:53 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/10/12/tips-on-cloud-init/</guid>
      <description>参考 主要参考了
http://huang-wei.github.io/programming/2013/12/23/run-cloud-init-in-local-kvm.html
这里主要记录的是操作步骤。
介绍 红帽介绍:
Cloud-Init 是一个用来自动配置虚拟机的初始设置（如主机名，网卡和密钥）的工具。它可以在 使用模板部署虚拟机时使用，从而达到避免网络冲突的目的。
在使用这个工具前，cloud-init 软件包必须在虚拟机上被安装。安装后，Cloud-Init 服务会在系 统启动时搜索如何配置系统的信息。您可以使用只运行一次窗口来提供只需要配置一次的设置信息 ；或在 新建虚拟机、编辑虚拟机和编辑模板窗口中输入虚拟机每次启动都需要的配置信息。
cloud-init安装 Ubuntu 14.04上可以通过以下命令来安装cloud-init:
$ apt-cache search cloud-utils cloud-utils - metapackage for installation of upstream cloud-utils source $ sudo apt-get install cloud-utils  镜像准备 在http://cloud-images.ubuntu.com/releases/ 可以找到Ubuntu制作的ubuntu cloud image, image分版本, 这里使用14.04的image。
$ wget http://cloud-images.ubuntu.com/releases/14.04.3/ release-20151008/ubuntu-14.04-server-cloudimg-amd64-disk1.img  取回来后的镜像可以直接使用，但解压开后读取速度会更快:
$ qemu-img convert -O qcow2 ubuntu-14.04-server-cloudimg-amd64-disk1.img my_vm.img  对比两个镜像大小可以看到:
$ qemu-img info ubuntu-14.04-server-cloudimg-amd64-disk1.img image: ubuntu-14.04-server-cloudimg-amd64-disk1.img file format: qcow2 virtual size: 2.</description>
    </item>
    
    <item>
      <title>Kickstarting Ubuntu In SpaceWalk</title>
      <link>http://purplepalmdash.github.io/2015/08/20/kickstarting-ubuntu-in-spacewalk/</link>
      <pubDate>Thu, 20 Aug 2015 11:58:14 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/20/kickstarting-ubuntu-in-spacewalk/</guid>
      <description>Preparation You have to use apt-mirror for getting the packages to local repository, so that you could directly install the system via http method. The configuration file for apt-mirror is listed as following:
$ cat /etc/apt/mirror.list set base_path /mnt/myrepo set nthreads 20 set _tilde 0 #################Trusty Repository Starts ######################## deb-amd64 http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse main/debian-installer restricted/debian-installer multiverse/debian-installer universe/debian-installer deb-amd64 http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse main/debian-installer restricted/debian-installer universe/debian-installer deb-amd64 http://mirrors.</description>
    </item>
    
    <item>
      <title>Automatically Create Virtual Machine</title>
      <link>http://purplepalmdash.github.io/2015/08/13/automatically-create-virtual-machine/</link>
      <pubDate>Thu, 13 Aug 2015 16:21:23 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/13/automatically-create-virtual-machine/</guid>
      <description>Just record the whole scripts for create/define/start the vm machine.
#!/bin/bash # $1: The name of the virtual machine. ### 1. Check Input Parameters. if [ $# != 1 ] then echo &amp;quot;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&amp;quot; echo &amp;quot;!! Parameters error !!&amp;quot; echo &amp;quot;!! Example: ./createvm.sh name !!&amp;quot; echo &amp;quot;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&amp;quot; exit 1 fi ### 2. Create the qcow2 file in local directory. echo $1 qemu-img create -f qcow2 -b /home/juju/img/WolfHunter/SpaceWalk/Base/packer-ubuntu-1204-server-i386 $1.qcow2 ### 3. Generate Random MAC Address.</description>
    </item>
    
    <item>
      <title>Trouble Shooting On SpaceWalk OSAD On Ubuntu Clients</title>
      <link>http://purplepalmdash.github.io/2015/08/12/trouble-shooting-on-spacewalk-osad-on-ubuntu-clients/</link>
      <pubDate>Wed, 12 Aug 2015 11:25:34 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/12/trouble-shooting-on-spacewalk-osad-on-ubuntu-clients/</guid>
      <description>Problem On Clients(Ubuntu nodes), you will see lots of the following message in /var/log/osad:
$ tail /var/log/osad 2015-08-11 19:31:14 jabber_lib.main: Unable to connect to jabber servers, sleeping 78 seconds 2015-08-11 19:32:32 jabber_lib.main: Unable to connect to jabber servers, sleeping 117 seconds  When restart the osda service you will see following error message:
# service osad restart OSAD SpaceWalk Deamon osad Traceback (most recent call last): File &amp;quot;/usr/share/rhn/osad/jabber_lib.py&amp;quot;, line 252, in setup_connection c = self.</description>
    </item>
    
    <item>
      <title>Added Precise Repository In SpaceWalk</title>
      <link>http://purplepalmdash.github.io/2015/08/11/added-precise-repository-in-spacewalk/</link>
      <pubDate>Tue, 11 Aug 2015 16:26:32 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/11/added-precise-repository-in-spacewalk/</guid>
      <description>SpaceWalk Backend Configruation First Create the Channel:
Then Create the Repository like following:
Associate the channel together with repository:
Install packages Do following for the prerequisition for syncing the repository.
# yum update python-debian # vim /usr/lib/python2.6/site-packages/debian/debfile.py PART_EXTS = [&#39;gz&#39;, &#39;xz&#39;, &#39;lzma&#39;] # wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm # rpm -ivh epel-release-6-8.noarch.rpm # cat epel-testing.repo | more [epel-testing] name=Extra Packages for Enterprise Linux 6 - Testing - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/testing/6/$basearch #mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=testing-epel6&amp;amp;arch=$basearch baseurl=http://mirrors.aliyun.com/epel/testing/6/x86_64/ failovermethod=priority  Sync With Local Repository An example of precise-backports is listed like following, take this example for example, create other 3 shell scripts:</description>
    </item>
    
    <item>
      <title>Add Ubuntu Agent into SpaceWalk</title>
      <link>http://purplepalmdash.github.io/2015/08/10/add-ubuntu-agent-into-spacewalk/</link>
      <pubDate>Mon, 10 Aug 2015 14:53:28 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/10/add-ubuntu-agent-into-spacewalk/</guid>
      <description>Install via:
$ sudo apt-get install debhelper build-essential gcc devscripts git intltool quilt \ automake python-all-dev libnl-route-3-dev asciidoc pkg-config libxml2-utils \ docbook-xml xsltproc sgml-data docbook-xs $ sudo apt-get install apt-transport-spacewalk rhnsd  Fix the bug of XMLRPCLib:
--- /usr/lib/python2.7/xmlrpclib.py 2013-05-28 20:44:38.000000000 +0200 +++ new/xmlrpclib.py 2013-05-28 20:44:24.000000000 +0200 @@ -654,8 +654,8 @@ f(self, value, write) def dump_nil (self, value, write): - if not self.allow_none: - raise TypeError, &amp;quot;cannot marshal None unless allow_none is enabled&amp;quot; +# if not self.</description>
    </item>
    
    <item>
      <title>Build Kickstartable ISO</title>
      <link>http://purplepalmdash.github.io/2015/08/06/build-kickstartable-iso/</link>
      <pubDate>Thu, 06 Aug 2015 22:11:44 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/06/build-kickstartable-iso/</guid>
      <description>Installation # mount -t iso9660 -o loop ./ubuntu1404.iso /mnt # cp -rT /mnt/ iso/  Make a kickstart file use system-config-kickstart, and copy it to:
$ cp ks.cfg ./ $ vim isolinux/langlinux en $ vim isolinux/txt.cfg default install label install menu label ^Install Ubuntu Server kernel /install/vmlinuz append file=/cdrom/preseed/ubuntu-server.seed initrd=/install/initrd.gz ks=cdrom:/ks.cfg --  Make ISO Make the iso.
$ chmod a+w ./iso/isolinux/isolinux.bin $ mkisofs -J -l -b isolinux/isolinux.bin -no-emul-boot -boot-load-size 4 -boot-info-table -z -iso-level 4 -c isolinux/isolinux.</description>
    </item>
    
    <item>
      <title>Enlarge RootFS In CentOS7</title>
      <link>http://purplepalmdash.github.io/2015/08/06/enlarge-rootfs-in-centos7/</link>
      <pubDate>Thu, 06 Aug 2015 14:48:50 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/06/enlarge-rootfs-in-centos7/</guid>
      <description>Disk Preparation Create a new disk via following command in host machine:
# qemu-img create -f qcow2 Added.qcow2 200G  Boot the machine and you will have the newly added disk as /dev/vdb, format it via fdisk.
[root@spacewalker ~]# fdisk /dev/vdb Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table Building a new DOS disklabel with disk identifier 0x694e59ec.</description>
    </item>
    
    <item>
      <title>Setup SpaceWalker Trusty Channel</title>
      <link>http://purplepalmdash.github.io/2015/08/06/setup-spacewalker-trusty-channel/</link>
      <pubDate>Thu, 06 Aug 2015 11:47:31 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/06/setup-spacewalker-trusty-channel/</guid>
      <description>Channel Definition The definition for this channel should includes Architecture to &amp;lsquo;amd64-debian&amp;rsquo;, and Yum Repository to SHA256:
Yum Repository Checksum Type sha256 Architecture AMD64 Debian
Channel Definition The definition for this channel should includes Architecture to &amp;lsquo;amd64-debian&amp;rsquo;, and Yum Repository to SHA256:
Yum Repository Checksum Type sha256 Architecture AMD64 Debian  Then create more channels with its parent channel to your named channel.
Also create trusty-security and trusty-backports, after all configuration, your channel would be looks like following:</description>
    </item>
    
    <item>
      <title>Enable DHCP/DNS Server For SpaceWalker Server</title>
      <link>http://purplepalmdash.github.io/2015/08/05/enable-dhcp-slash-dns-server-for-spacewalker-server/</link>
      <pubDate>Wed, 05 Aug 2015 11:28:21 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/05/enable-dhcp-slash-dns-server-for-spacewalker-server/</guid>
      <description>DHCP Server Install the dhcp server via:
# yum install -y dhcp  Then edit the configuration of /etc/dhcp/dhcpd.conf, like following:
# # DHCP Server Configuration file. # see /usr/share/doc/dhcp*/dhcpd.conf.example # see dhcpd.conf(5) man page # # specify name server&#39;s hostname or IP address option domain-name-servers 10.9.10.13; # default lease time default-lease-time 600; # max lease time max-lease-time 7200; # this DHCP server to be declared valid authoritative; # specify network address and subnet mask subnet 10.</description>
    </item>
    
    <item>
      <title>Tips On Using SpaceWalker For Deploying CentOS7</title>
      <link>http://purplepalmdash.github.io/2015/08/04/tips-on-using-spacewalker-for-deploying-centos7/</link>
      <pubDate>Tue, 04 Aug 2015 15:57:47 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/08/04/tips-on-using-spacewalker-for-deploying-centos7/</guid>
      <description>Configuration After SpaceWalker has been setup, the configuration we need to done is listed as following:
In /etc/rhn/rhn.conf change the value of the parameter cobbler.host to the ip address of the spacewalk server. In /etc/cobbler/settings change the value of the parameters server and redhat_management_server to the ip-address of the spacewalk server.  Install cobbler bootloaders via:
# yum install -y cobbler-loaders  Build Customized ISO via cobbler buildiso, and in the same folder you will get generated.</description>
    </item>
    
    <item>
      <title>Ubuntu1504&#39;s Cobbler Server</title>
      <link>http://purplepalmdash.github.io/2015/07/29/ubuntu1504-s-cobbler-server/</link>
      <pubDate>Wed, 29 Jul 2015 21:23:44 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/29/ubuntu1504-s-cobbler-server/</guid>
      <description> Installation The newest version is 2.6.9.
# wget -qO - http://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/xUbuntu_15.04/Release.key | sudo apt-key add - # sudo add-apt-repository &amp;quot;deb http://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/xUbuntu_15.04/ ./&amp;quot; # sudo apt-get install cobbler  </description>
    </item>
    
    <item>
      <title>Trying SpaceWalk</title>
      <link>http://purplepalmdash.github.io/2015/07/29/trying-spacewalk/</link>
      <pubDate>Wed, 29 Jul 2015 16:43:23 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/29/trying-spacewalk/</guid>
      <description>Server Installation &amp;amp;&amp;amp; Configration Server have 2-core and 3072MB, running CentOS6.6, IP address is 10.9.10.2.
Installation:
# rpm -Uvh http://yum.spacewalkproject.org/2.3/RHEL/6/x86_64/spacewalk-repo-2.3-4.el6.noarch.rpm # wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo # yum update # yum install -y spacewalk-postgresql  Now add a new repository:
# cat /etc/yum.repos.d/jpackage-generic.repo [jpackage-generic] name=JPackage generic #baseurl=http://mirrors.dotsrc.org/pub/jpackage/5.0/generic/free/ mirrorlist=http://www.jpackage.org/mirrorlist.php?dist=generic&amp;amp;type=free&amp;amp;release=5.0 enabled=1 gpgcheck=1 gpgkey=http://www.jpackage.org/jpackage.asc  Met some problems on CentOS6.6, mainly the dependencies problem, switches to CentOS7(10.9.10.100) :
Install it via:
# rpm -Uvh http://yum.</description>
    </item>
    
    <item>
      <title>WorkTips On LandScape</title>
      <link>http://purplepalmdash.github.io/2015/07/28/worktips-on-landscape/</link>
      <pubDate>Tue, 28 Jul 2015 16:10:49 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/28/worktips-on-landscape/</guid>
      <description>Installation Landscape is now in PPA repository, add it via:
$ sudo apt-get install software-properties-common $ sudo add-apt-repository ppa:landscape/15.01 $ sudo apt-get update $ sudo apt-get install landscape-server-quickstart  During the installation will ask you the configuration of postfix, specify local.
Configuration First time you login into the LandScape Root machine, you have to setup your email address and your password.
Then you could visit https://YourIPAddress for the configuration page.</description>
    </item>
    
    <item>
      <title>Tips On Packer</title>
      <link>http://purplepalmdash.github.io/2015/07/27/tips-on-packer/</link>
      <pubDate>Mon, 27 Jul 2015 12:18:46 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/27/tips-on-packer/</guid>
      <description>Installation Install Packer via:
$ wget https://dl.bintray.com/mitchellh/packer/packer_0.8.2_linux_amd64.zip $ unzip packer_0.8.2_linux_amd64.zip $ mv packer* ~/bin $ export PATH=~/bin:$PATH  KVM Based Image Build Fetch the kickstart configuration file.
$ mkdir ~/Code/packer $ wget https://gist.githubusercontent.com/mitchellh/7328271/raw/9035b8e26d001f14a2a960d3cec65eceb0e716ea/centos6-ks.cfg # vim centos6-ks.cfg ### Replace your own repository URL  Create the json definition file for the packer build:
$ packer validate first.json Template validated successfully. $ cat first.json { &amp;quot;builders&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;qemu&amp;quot;, &amp;quot;iso_url&amp;quot;: &amp;quot;/media/opensuse/dash/iso/CentOS-6.</description>
    </item>
    
    <item>
      <title>Libvirtd Trouble-Shooting On CentOS7.1</title>
      <link>http://purplepalmdash.github.io/2015/07/22/libvirtd-trouble-shooting-on-centos7-dot-1/</link>
      <pubDate>Wed, 22 Jul 2015 09:54:59 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/22/libvirtd-trouble-shooting-on-centos7-dot-1/</guid>
      <description> Problem Description When upgraded from CentOS6.6 to CentOS7, the libvirtd will encounter following error:
Simply remove:
# virsh edit nodename - &amp;lt;feature policy=&#39;require&#39; name=&#39;invtsc&#39;/&amp;gt;  </description>
    </item>
    
    <item>
      <title>On Migration of KVM</title>
      <link>http://purplepalmdash.github.io/2015/07/19/on-migration-of-kvm/</link>
      <pubDate>Sun, 19 Jul 2015 13:31:03 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/19/on-migration-of-kvm/</guid>
      <description>Migration First create the qemu based vm:
$ pwd /media/arch/home/juju/img/migration $ qemu-img create -f qcow2 ubuntu1504.qcow2 100G $ sudo qemu-system-x86_64 -enable-kvm -m 512 -smp 4 -name ubuntu1504 -monitor stdio -boot c -drive file=/media/arch/home/juju/img/migration/ubuntu1504.qcow2,if=none,id=drive-virtio-disk0,boot=on -device virtio-blk-pci,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0 -drive file=/media/arch/home/dash/iso/ubuntu-15.04-server-amd64.iso,if=none,media=cdrom,id=drive-ide0-1-0 -device ide-drive,bus=ide.1,unit=0,drive=drive-ide0-1-0,id=ide0-1-0 -device virtio-net-pci,vlan=0,id=net0,mac=52:54:00:13:08:96 -net tap -vnc 127.0.0.1:3  After Installation, startup the vm via(didn&amp;rsquo;t attach the file):
$ sudo qemu-system-x86_64 -enable-kvm -m 512 -smp 4 -name ubuntu1504 -monitor stdio -boot c -drive file=/media/arch/home/juju/img/migration/ubuntu1504.</description>
    </item>
    
    <item>
      <title>ISCSI Installed Debian Jessie</title>
      <link>http://purplepalmdash.github.io/2015/07/17/iscsi-installed-debian-jessie/</link>
      <pubDate>Fri, 17 Jul 2015 14:24:20 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/17/iscsi-installed-debian-jessie/</guid>
      <description>FreeNAS Installation And Configuration Install Procedure ignored, because it&amp;rsquo;s simple.
Following steps are used for adding iscsi partition.
Manually setup the volumn and now you could add your volumn into the FreeNAS System.
Configure iscsi:
Add the name of iqn.onecloud.iscsi, next we add portal:
Add Authorized Access Now:
Add Initator:
Create target:
Add extent:
LUM RPMs could also be spcified:
Associate Targets:
Enable the iscsi service:
Better you change the IP Address into static IP address.</description>
    </item>
    
    <item>
      <title>WH Worktips(11)</title>
      <link>http://purplepalmdash.github.io/2015/07/13/wh-worktips-11/</link>
      <pubDate>Mon, 13 Jul 2015 21:12:14 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/13/wh-worktips-11/</guid>
      <description>CentOS7 Based Following modification need to be done:
On Deployed node:
$ sudo vim /etc/hosts 10.47.58.157	node157  There will be iptables issue. for you have to install following packages:
# yum install -y install iptables-utils iptables-services # reboot  Now begin to install again, and it will be ok.
Add the client, and make sure the &amp;ldquo;COPY CPU configuration&amp;rdquo; is selected in the menu.
Or, your system vm won&amp;rsquo;t startup.</description>
    </item>
    
    <item>
      <title>WH Worktips(9)</title>
      <link>http://purplepalmdash.github.io/2015/07/10/wh-worktips-9/</link>
      <pubDate>Fri, 10 Jul 2015 20:21:13 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/10/wh-worktips-9/</guid>
      <description>Cobbler In Non-DHCP Server Networking Sometimes you want to deploy system with cobbler server, in some restricted network which dhcp service is not allowed(considering broadcasting storm, security, etc.) Following are the steps of how-to:
Change Cobbler server setting:
$ sudo vim /etc/cobbler/settings manage_dhcp: 0 $ sudo cobbler sync $ sudo service dhcpd stop  Add the node definition in cobbler(useless):
# cobbler system add --name=node2 --profile=CentOS6.5-x86_64 --mac=52:54:00:71:59:64 --interface=eth0 --static=1 --ip-address=10.</description>
    </item>
    
    <item>
      <title>WH Worktips(7)</title>
      <link>http://purplepalmdash.github.io/2015/07/07/wh-worktips-7/</link>
      <pubDate>Tue, 07 Jul 2015 15:22:23 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/07/wh-worktips-7/</guid>
      <description>Cloudstack Agent Repository Setup the CloudStack Agent Repository via:
# yum install yum-plugin-downloadonly # vim /etc/yum.repos.d/cloudstack.repo [cloudstack] name=cloudstack baseurl=http://cloudstack.apt-get.eu/rhel/4.3/ enabled=1 gpgcheck=0 # mkdir Code # yum install --downloadonly --downloaddir=/root/Code/ cloud-agent  Now all of the installation rpm packages has been downloaded to directory, simply upload them to a server, use createrepo . to generate the repository, and link them to nginx&amp;rsquo;s root directory.
Mine is under:
http://192.168.0.79&amp;frasl;4.4.3CloudStackAgent/
Agent Installation Steps In a new deployed machine:</description>
    </item>
    
    <item>
      <title>Add Private NAT Networking In XenServer</title>
      <link>http://purplepalmdash.github.io/2015/07/06/add-private-nat-networking-in-xenserver/</link>
      <pubDate>Mon, 06 Jul 2015 11:42:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/06/add-private-nat-networking-in-xenserver/</guid>
      <description>Create Networking In XenCenter Create the networking under the XenCenter UI&amp;rsquo;s tab &amp;ldquo;Networking&amp;rdquo;.
Networking Setting Enable the ip forward:
# vim /etc/sysctl.conf net.ipv4.ip_forward = 1 # sysctl -p # cat /proc/sys/net/ipv4/ip_forward 1  Use iptables for forwarding the network flow:
# iptables -A FORWARD --in-interface xapi0 -j ACCEPT # iptables --table nat -A POSTROUTING --out-interface eth0 -j MASQUERADE  But this didn&amp;rsquo;t bring up the internal networking, after discussing with college, edit the file:</description>
    </item>
    
    <item>
      <title>Preseed File For Ubuntu1404 In CobblerServer</title>
      <link>http://purplepalmdash.github.io/2015/07/04/preseed-file-for-ubuntu1404-in-cobblerserver/</link>
      <pubDate>Sat, 04 Jul 2015 14:57:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/07/04/preseed-file-for-ubuntu1404-in-cobblerserver/</guid>
      <description>Proseed File d-i time/zone string Asia/Shanghai # Setup the installation source d-i mirror/country string manual d-i mirror/http/hostname string $http_server #d-i mirror/http/directory string $install_source_directory d-i mirror/http/directory string /cobbler/ks_mirror/Ubuntu-14.04-x86_64/ubuntu d-i mirror/http/proxy string d-i apt-setup/security_host string $http_server d-i apt-setup/security_path string /cobbler/ks_mirror/Ubuntu-14.04-x86_64/ubuntu  Local Repository In one installed machine, do following for getting the repository of all of the installed packages:
$ sudo apt-get install dselect $ dpkg --get-selections | grep -v deinstall&amp;gt;InstalledPackage.txt $ awk {&#39;print $1&#39;} InstalledPackage.</description>
    </item>
    
    <item>
      <title>Insert public key into Cobbler Deployed System</title>
      <link>http://purplepalmdash.github.io/2015/06/29/insert-public-key-into-cobbler-deployed-system/</link>
      <pubDate>Mon, 29 Jun 2015 14:13:08 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/29/insert-public-key-into-cobbler-deployed-system/</guid>
      <description>First edit your kickstart file, add following line before the end of your kickstart:
[root@z_WHServer kickstarts]# pwd /var/lib/cobbler/kickstarts [root@z_WHServer kickstarts]# cat sample_end.ks # Start final steps + $SNIPPET(&#39;publickey_root&#39;) $SNIPPET(&#39;kickstart_done&#39;) # End final steps %end  And the publickey_root should be edited as following:
[root@z_WHServer snippets]# pwd /var/lib/cobbler/snippets [root@z_WHServer snippets]# cat publickey_root # Install CobblerServer&#39;s(10.47.58.2) public key for root user cd /root mkdir --mode=700 .ssh cat &amp;gt;&amp;gt; .ssh/authorized_keys &amp;lt;&amp;lt; &amp;quot;PUBLIC_KEY&amp;quot; ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA3B3GtGuKY0l2Ak9+WSkorY7R+Cx5/u3RMua/7GrvP05IPywQdkR+mqwdRydNjyhB96nHlYZtr8Fbfn5iwqn0j8dz8wmTZicBNeRqIdbe/YUje5NjXxDXjYda63VfDhpgzJ53KICTx6pBhGaeOKS/U5HqCpDbF7ODP8siU7bRhk1LkIQ6VwZYUg7b0oR+Sw6XJ31Z7gs4CWF6zfjfQQoF7EoMA+dnqvt2K4PQPXNSBJQx3qb9jyXIXvo333PcfIX6mD1TW1wDAIXLm4qz4mi7C8Ax9h+T/D98r08WX360vC5Tzr8feXMs6H4il4s4Ftq7RVoqCNKmG3AB1LTp4AQAzw== root@z_WHServer PUBLIC_KEY chmod 600 .</description>
    </item>
    
    <item>
      <title>Install Newest Version Of Cobbler</title>
      <link>http://purplepalmdash.github.io/2015/06/24/install-newest-version-of-cobbler/</link>
      <pubDate>Wed, 24 Jun 2015 09:38:50 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/24/install-newest-version-of-cobbler/</guid>
      <description>Background Cobbler has changed its homepage from www.cobblerd.org to http://cobbler.github.io/, thus some configuration will be be failed. In order to solve these problems, we have to upgrade to the newest version, current newest version is 2.6.9.
Download And Install Go to http://cobbler.github.io/downloads/2.6.x.html for selecting your distribution, and download them to your folder. Mine is configured via:
$ wget http://download.opensuse.org/repositories/home:/libertas-ict:/cobbler26/CentOS_CentOS-6/home:libertas-ict:cobbler26.repo $ cp home\:libertas-ict\:cobbler26.repo /etc/yum.repos.d/cobbler26.repo $ yum install -y cobbler cobbler-web  Configuration After installation, configure your newest cobbler via:</description>
    </item>
    
    <item>
      <title>Import More ISOs in Cobbler</title>
      <link>http://purplepalmdash.github.io/2015/06/23/import-more-isos-in-cobbler/</link>
      <pubDate>Tue, 23 Jun 2015 20:52:09 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/23/import-more-isos-in-cobbler/</guid>
      <description>Ubuntu12.04 Since you have the iso files, import it via:
[root@CobblerServer iso]# mount -t iso9660 -o loop /mnt/iso/ubuntu-12.04.3-server-amd64.iso /mnt1 [root@CobblerServer iso]# cobbler import --name=Ubuntu-12.04 --arch=x86_64 --path=/mnt1/ [root@CobblerServer iso]# cobbler profile list CentOS-7-x86_64 Ubuntu-12.04-x86_64 Ubuntu14.04-x86_64  To make it quickly deployment, visit:
http://purplepalmdash.github.io/blog/2015/05/18/my-configuration-on-cobbler-for-deploying-ubuntu12-dot-04/
Edit its profile like following:
[root@CobblerServer kickstarts]# cobbler profile edit --name=Ubuntu-12.04-x86_64 --kickstart=/var/lib/cobbler/kickstarts/ubuntu1204.seed  Create a new virtual machine for testing.
[root@CobblerServer kickstarts]# cobbler system add --name=test1204 --profile=Ubuntu-12.04-x86_64 --mac=52:54:00:e4:2c:df --interface=eth0 --ip-address=10.</description>
    </item>
    
    <item>
      <title>WH Worktips(5)</title>
      <link>http://purplepalmdash.github.io/2015/06/19/wh-worktips-5/</link>
      <pubDate>Fri, 19 Jun 2015 15:29:56 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/19/wh-worktips-5/</guid>
      <description>This worktips will enable the visibility for ansible deployed nodes.
[root@z_WHServer Code]# cd ansible-tower-setup-2.1.5/ [root@z_WHServer ansible-tower-setup-2.1.5]# ls ansible.cfg configure group_vars host_vars README.md roles setup.sh site.yml  Later I will finish this. But remember the tutorial pdf works well.</description>
    </item>
    
    <item>
      <title>Use VirtualEnv For Downloading PIP Packages</title>
      <link>http://purplepalmdash.github.io/2015/06/18/use-virtualenv-for-downloading-pip-packages/</link>
      <pubDate>Thu, 18 Jun 2015 20:56:30 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/18/use-virtualenv-for-downloading-pip-packages/</guid>
      <description>In order to use offline installation of pip, I tried following steps for retrieving the packages.
Fetch Packages First install virtualenv packages on CentOS7.
$ sudo yum install -y python-virtualenv $ virtualenv venv New python executable in venv/bin/python Installing Setuptools..............................................................................................................................................................................................................................done. Installing Pip.....................................................................................................................................................................................................................................................................................................................................done. $ ls venv bin include lib lib64 $ source venv/bin/activate (venv)$ (venv)$ pip install --download-cache=/home/dash/pipcache cloudmonkey (venv)$ ls pipcache/ https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fa%2Fargcomplete%2Fargcomplete-0.8.9.tar.gz https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fa%2Fargcomplete%2Fargcomplete-0.8.9.tar.gz.content-type https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fc%2Fcloudmonkey%2Fcloudmonkey-5.3.1-0.tar.gz https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fc%2Fcloudmonkey%2Fcloudmonkey-5.3.1-0.tar.gz.content-type https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2FP%2FPrettyTable%2Fprettytable-0.7.2.tar.bz2 ...... (venv)$ mkdir ~/pipcache2 (venv)$ pip install cloudmonkey --download=/home/dash/pipcache2 (venv)$ ls pipcache2 argcomplete-0.</description>
    </item>
    
    <item>
      <title>WH Worktips(2)</title>
      <link>http://purplepalmdash.github.io/2015/06/18/wh-worktips-2/</link>
      <pubDate>Thu, 18 Jun 2015 10:20:48 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/18/wh-worktips-2/</guid>
      <description>Cobbler Web Visit the following website:
http://10.47.58.2/cobbler_web
You will see:
Added More Profiles The default kickstart configuration file could found under:
/var/lib/cobbler/kickstarts/sample_end.ks, copy it to your own.
$ cp /var/lib/cobbler/kickstarts/sample_end.ks CentOS65Desktop.cfg $ vim CentOS65Desktop.cfg # Allow anaconda to partition the system as needed # autopart # 1G Swap and remains others to be ext4 part swap --fstype=&amp;quot;swap&amp;quot; --size=1024 part / --asprimary --fstype=&amp;quot;ext4&amp;quot; --grow --size=1 ....... %packages # Added from here @additional-devel @basic-desktop @chinese-support @desktop-platform @development @fonts @general-desktop @input-methods @x11 git -ibus-table-cangjie -ibus-table-erbi -ibus-table-wubi # End of added $SNIPPET(&#39;func_install_if_enabled&#39;) %end  More configurations could be customized.</description>
    </item>
    
    <item>
      <title>WH Worktips(1)</title>
      <link>http://purplepalmdash.github.io/2015/06/17/wh-worktips-1/</link>
      <pubDate>Wed, 17 Jun 2015 14:55:39 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/17/wh-worktips-1/</guid>
      <description>Preparation Hardware: 2G Memory, 1-Core, the Cobbler Server, which runs CentOS6.6.
Network: Use a 10.47.58.0/24(Its name is WHNetwork), no dhcp server in this network.
Cobbler Server Preparation First Change its IP address to 10.47.58.2, gateway to 10.47.58.1.
[root@z_WHServer ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 TYPE=Ethernet UUID=a6e5b56f-661f-4128-ab8c-c575a9623245 ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=none IPADDR=10.47.58.2 GATEWAY=10.47.58.1 ...... [root@z_WHServer ~]# cat /etc/sysconfig/network NETWORKING=yes HOSTNAME=z_WHServer # vim /etc/selinux/config #SELINUX=enforcing SELINUX=disabled # reboot  Install and configure Cobbler Server via:</description>
    </item>
    
    <item>
      <title>XenServer Tips</title>
      <link>http://purplepalmdash.github.io/2015/06/16/xenserver-tips/</link>
      <pubDate>Tue, 16 Jun 2015 14:52:20 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/16/xenserver-tips/</guid>
      <description>Recently I want to research desktop virtualization on Xen, so this blog records all of the tips for Xen Hypervisor related info.
Nested Virtualization I place 4 core(Copy Host Configuration on CPU parameter), but the XenServer refuse to start, by using a none-hosted-configuration CPU configuration, it will fail on starting the machine, So I choose to install xen hypervisor on Ubuntu14.04.
Ubuntu and Xen Install via:
$ sudo apt-get install xen-hypervisor-amd64 $ sudo reboot  The Ubuntu will automatically choose xen for startup, so verify it via:</description>
    </item>
    
    <item>
      <title>OpenVSwitch and VXLAN How-to</title>
      <link>http://purplepalmdash.github.io/2015/06/08/openvswitch-and-vxlan-how-to/</link>
      <pubDate>Mon, 08 Jun 2015 09:48:20 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/08/openvswitch-and-vxlan-how-to/</guid>
      <description>Following records the steps for my setup for OpenVSwitch environment and configure VXLAN on it.
Preparation I use two VMs for this experiment, created a new virtual network, it&amp;rsquo;s 10.94.94.0/24, every vm machines adds into this network.
VM1, VM2, both have 1G Memory. 1 Core.
VM1: 10.94.94.11, VM2: 10.94.94.12.
$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get -y upgrade $ sudo apt-get install build-essential$ $ sudo reboot $ uname -a $ uname -a Linux OpenVSwitchVM1 3.</description>
    </item>
    
    <item>
      <title>Tips on using vagrant and chefdk</title>
      <link>http://purplepalmdash.github.io/2015/06/03/tips-on-using-vagrant-and-chefdk/</link>
      <pubDate>Wed, 03 Jun 2015 21:00:20 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/03/tips-on-using-vagrant-and-chefdk/</guid>
      <description>You should install all of the gem of berkshelf via:
  $ gem install berkshelf $ /opt/chef/embedded/bin/gem install berkshelf $ /opt/vagrant/embedded/bin/gem install berkshelf   Besure to add following into your PATH:
  $ echo $PATH /opt/chefdk/bin:/home/kkk/.rvm/gems/ruby-2.2.1/bin:/home/kkk/.rvm/gems/ruby-2.2.1@global/bin:/home/kkk/.rvm/rubies/ruby-2.2.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/kkk/.rvm/bin:/home/kkk/.rvm/bin:/home/kkk/.rvm/bin  So now you could continue with vagrant up or other steps.</description>
    </item>
    
    <item>
      <title>Chef Trouble-Shooting</title>
      <link>http://purplepalmdash.github.io/2015/06/02/chef-trouble-shooting/</link>
      <pubDate>Tue, 02 Jun 2015 16:16:42 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/06/02/chef-trouble-shooting/</guid>
      <description>Error Could not Add new nodes.
Reason This is because the chefDK remains the old version of chef-client,
[dash@~/chef-repo]$ chef --version Chef Development Kit Version: 0.6.0 chef-client version: ERROR berks version: ERROR kitchen version: 1.4.0  Solution In node, manually get verified via following command:
$ knife ssl fetch --config /etc/chef/client.rb $ chef-client -l debug -S https://ChefServer/organizations/xxxxx -K /xxx/xxx/xxxxx.pem  Now bootstrap again, and you will see the node could be added into the Chef-Server&amp;rsquo;s system.</description>
    </item>
    
    <item>
      <title>Tips on deleteing neutron subnet and router</title>
      <link>http://purplepalmdash.github.io/2015/05/25/tips-on-deleteing-neutron-subnet-and-router/</link>
      <pubDate>Mon, 25 May 2015 21:44:54 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/25/tips-on-deleteing-neutron-subnet-and-router/</guid>
      <description>Get the existing subnet:
root@Controller:~# neutron subnet-list +--------------------------------------+-------------+----------------+--------------------------------------------------+ | id | name | cidr | allocation_pools | +--------------------------------------+-------------+----------------+--------------------------------------------------+ | 98725e3a-7ee2-4e3f-83e3-eaca0236918f | demo-subnet | 192.168.1.0/24 | {&amp;quot;start&amp;quot;: &amp;quot;192.168.1.2&amp;quot;, &amp;quot;end&amp;quot;: &amp;quot;192.168.1.254&amp;quot;} | +--------------------------------------+-------------+----------------+--------------------------------------------------+  Delete it via:
root@Controller:~# neutron subnet-delete --name demo-subnet Unable to complete operation on subnet 98725e3a-7ee2-4e3f-83e3-eaca0236918f. One or more ports have an IP allocation from this subnet. (HTTP 409) (Request-ID: req-7d729bcc-ec50-4de6-83d9-5d2b98332127)  Because we have the router, so we list the router via:</description>
    </item>
    
    <item>
      <title>三节点搭建OpenStack Juno(4)</title>
      <link>http://purplepalmdash.github.io/2015/05/25/san-jie-dian-da-jian-openstack-juno-4/</link>
      <pubDate>Mon, 25 May 2015 20:11:17 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/25/san-jie-dian-da-jian-openstack-juno-4/</guid>
      <description>Neutron和nova-network的区别在于，nova-network可以让你在每个instance上部署一种网络类型，适合基本的网络功能。而Neutron则使得你可以在一个instance上部署多种网络类型，并且以插件的方式支持多种虚拟化网络。
详细的介绍，以后慢慢加，理解吃透了再加上来，这里单单提操作步骤。
准备 数据库准备如下:
root@Controller:~# mysql -u root -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \g. Your MariaDB connection id is 39 Server version: 5.5.43-MariaDB-1ubuntu0.14.04.2 (Ubuntu) Copyright (c) 2000, 2015, Oracle, MariaDB Corporation Ab and others. Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement. MariaDB [(none)]&amp;gt; CREATE DATABASE neutron; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON neutron.</description>
    </item>
    
    <item>
      <title>三节点搭建OpenStack Juno(3)</title>
      <link>http://purplepalmdash.github.io/2015/05/25/san-jie-dian-da-jian-openstack-juno-3/</link>
      <pubDate>Mon, 25 May 2015 16:51:01 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/25/san-jie-dian-da-jian-openstack-juno-3/</guid>
      <description>Nova Nova数据库 创建nova数据库:
# mysql -u root -p CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova.* TO &#39;nova&#39;@&#39;localhost&#39; \ IDENTIFIED BY &#39;NOVA_DBPASS&#39;; GRANT ALL PRIVILEGES ON nova.* TO &#39;nova&#39;@&#39;%&#39; \ IDENTIFIED BY &#39;NOVA_DBPASS&#39;; quit;  创建nova用户:
# source /home/dash/admin-openrc.sh root@Controller:~# keystone user-create --name nova --pass xxxxxx +----------+----------------------------------+ | Property | Value | +----------+----------------------------------+ | email | | | enabled | True | | id | 4a3768e3f4754cd0b9d47c6fadb22c7e | | name | nova | | username | nova | +----------+----------------------------------+  为admin角色添加nova用户:</description>
    </item>
    
    <item>
      <title>LXCize the KVM machine</title>
      <link>http://purplepalmdash.github.io/2015/05/25/lxcize-the-kvm-machine/</link>
      <pubDate>Mon, 25 May 2015 11:46:28 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/25/lxcize-the-kvm-machine/</guid>
      <description>In order to make exsiting kvm based machine to be lxc container, following is the steps.
Refers to:
https://www.stgraber.org/2012/03/04/booting-an-ubuntu-12-04-virtual-machine-in-an-lxc-container/
Convert Disk Formats First we want to convert the qcow2 format image to raw format, by following command:
$ qemu-img convert u12-debug-ui.qcow2 Contrail.raw  This will take a very long time, because qcow2 file will expand to a whole images, like mine, the Contrail.raw in fact expands to 100G size.</description>
    </item>
    
    <item>
      <title>三节点搭建OpenStack Juno(2)</title>
      <link>http://purplepalmdash.github.io/2015/05/24/san-jie-dian-da-jian-openstack-juno-2/</link>
      <pubDate>Sun, 24 May 2015 22:37:28 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/24/san-jie-dian-da-jian-openstack-juno-2/</guid>
      <description>MySQL数据库 绝大多数的OpenStack服务使用SQL数据库来存储信息，一般情况下数据库运行在控制节点上，这里我们使用MariaDB或者MySQL来作为SQL数据库。
安装, 注意安装过程中需要输入密码:
# apt-get install mariadb-server python-mysqldb  配置, 主要是更改了bind的地址，添加了一些有用选项，并支持UTF-8编码:
$ sudo vim /etc/mysql/my.cnf [mysqld] ... bind-address = 10.55.55.2 ... default-storage-engine = innodb innodb_file_per_table collation-server = utf8_general_ci init-connect = &#39;SET NAMES utf8&#39; character-set-server = utf8  完成安装，包括重启服务及加密数据库服务:
# service mysql restart # mysql_secure_installation  消息服务器 OpenStack使用message broker用来在各种服务器之间调度操作和协调状态信息，通常情况下消息服务器也运行在控制节点上，OpenStack支持RabbitMQ, Qpid和ZeroMQ, 这里使用RabbitMQ.
安装:
# apt-get install rabbitmq-server  配置，首先我们需要设定rabbitMQ使用的密码:
# rabbitmqctl change_password guest RABBIT_PASS Changing password for user &amp;quot;guest&amp;quot; ... .</description>
    </item>
    
    <item>
      <title>三节点搭建OpenStack Juno(1)</title>
      <link>http://purplepalmdash.github.io/2015/05/24/san-jie-dian-da-jian-openstack-juno/</link>
      <pubDate>Sun, 24 May 2015 14:36:34 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/24/san-jie-dian-da-jian-openstack-juno/</guid>
      <description>目的 最近在研究解耦OpenStack，以及OpenStack的各种网络模型，下面是一个最简单的用于搭建OpenStack Juno的过程。
硬件及网络准备 物理服务器 物理服务器: i5-4460/32G 内存，128G SSD+3T IDE，事实上这个教程跑完你也用不到这么强悍的配置，理论上在8G的物理机器上就可以运行完本文。
物理服务器操作系统: Ubuntu14.04
虚拟机： 虚拟机1, Controller: 1 processor, 2 GB memory, and 5 GB storage.
虚拟机2, Network: 1 processor, 512 MB memory, and 5 GB storage.
虚拟机3, Compute: 1 processor, 2 GB memory, and 10 GB storage.
网络规划 Management: 10.55.55.0/24, 只用于管理的网络，公网无法访问。简单来说，这个网络用于OpenStack各个组件之间的相互通信。
Tunnel: 10.66.66.0/24, 用于计算节点和网络节点之间的通信。这个隧道使得虚拟机的实例可以和相互通信。
External: 192.168.1.0/24, 用于虚拟机实例的Internet访问。
当然我们可以添加额外的存储网络，这里为了简单起见我们不使用cinder服务，使用单纯的虚拟机镜像即可。
节点网络名规划 Controller节点: controller.openstack.local, 10.55.55.2(管理网络), N/A, N/A.
Network节点: Network.openstack.local, 10.55.55.3(管理网络), 10.66.66.3(隧道网络), 192.168.1.3(Internet公网). Compute节点: Compute.</description>
    </item>
    
    <item>
      <title>Change Cobbler Profile For Using Local Repository</title>
      <link>http://purplepalmdash.github.io/2015/05/22/change-cobbler-profile-for-using-local-repository/</link>
      <pubDate>Fri, 22 May 2015 14:12:58 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/22/change-cobbler-profile-for-using-local-repository/</guid>
      <description>Cobbler Profiles For getting the profiles and get the detailed information of the profile.
# cobbler profile list ubuntu1404-x86_64 # cobbler profile help usage ===== cobbler profile add cobbler profile copy cobbler profile dumpvars cobbler profile edit cobbler profile find cobbler profile getks cobbler profile list cobbler profile remove cobbler profile rename cobbler profile report # cobbler profile report ubuntu1404-x86_64 ...... Kickstart : /var/lib/cobbler/kickstarts/sample.seed ......  Use Local Repository For adding the repository via following command, you could use your local repository:</description>
    </item>
    
    <item>
      <title>My Configuration On Cobbler For Deploying Ubuntu12.04</title>
      <link>http://purplepalmdash.github.io/2015/05/18/my-configuration-on-cobbler-for-deploying-ubuntu12-dot-04/</link>
      <pubDate>Mon, 18 May 2015 18:15:47 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/18/my-configuration-on-cobbler-for-deploying-ubuntu12-dot-04/</guid>
      <description>Configuration file for preseed, put it under: /var/lib/cobbler/kickstarts/autoinstall.seed:
# BASIC d-i debian-installer/locale string en_US.UTF-8 d-i debian-installer/splash boolean false d-i console-setup/ask_detect boolean false d-i console-setup/layoutcode string us d-i console-setup/variantcode string d-i clock-setup/utc boolean true d-i clock-setup/ntp boolean true # DISKPART d-i partman-auto/method string regular d-i partman-lvm/device_remove_lvm boolean true d-i partman-lvm/confirm boolean true d-i partman/confirm_write_new_label boolean true d-i partman/choose_partition select Finish partitioning and write changes to disk d-i partman/confirm boolean true d-i partman/confirm_nooverwrite boolean true d-i partman/default_filesystem string ext3 # SOFTWARE # /var/www/cobbler/ks_mirror/Ubuntu12.</description>
    </item>
    
    <item>
      <title>使用Fuel部署OpenContrail(6)</title>
      <link>http://purplepalmdash.github.io/2015/05/06/shi-yong-fuelbu-shu-opencontrail-6/</link>
      <pubDate>Wed, 06 May 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/05/06/shi-yong-fuelbu-shu-opencontrail-6/</guid>
      <description>前面在HA类型的Fuel OpenStack基础上集成了OpenContrail，然而在实际的开发和测试中，用HA类型比较浪费硬件资源，因此这次我把部署节点从7个压缩到3个，做多节点上非HA类型的OpenStack集成OpenContrail.
先决条件 这次只用三台机器来做部署，分别为:
2-Core, 3G内存, 100G硬盘, 用于安装OpenStack Controller.
2-Core, 2G内存, 100G硬盘, 用于安装OpenStack Compute. 注意这台机器需要Copy Host CPU configuration, 以激活KVM。
2-Core, 3G内存, 100G硬盘, 用于安装Contrail.
创建出来的两个用于部署的OpenStack环境如下:
值得注意的是，在OpenStack的配置中，我们激活了Ceilometer，用于统计，所以需要额外增加一台2G内存大小的虚拟机。
安装 安装过程和HA的过程大同小异，配置好网络以后，现在I3OpenStack中部署好OpenStack，而后用provision的方式将I3Contrail中的Contrail部署节点机器安装为Ubuntu的格式。
这里的具体配置过程可以参考《使用Fuel部署OpenContrail(1)》到《使用Fuel部署OpenContrail(3)》.
一切就绪后，我们进入到配置过程.
配置 详细配置如下:
(Contrail) 配置Contrail部署节点 删除不用的网络端口, 并配置ifccfg-eth4后重启:
# cd /etc/network/interfaces.d/ # rm -f ifcfg-eth1 # rm -f ifcfg-eth2 # rm -f ifcfg-eth3 # vim ifcfg-eth4 auto eth4 iface eth4 inet static address 10.77.77.100 netmask 255.255.255.0 gateway 10.77.77.1 post-up ethtool -K eth4 gso off gro off || true # reboot  确保在Contrail部署节点上，可以ping通OpenStack Controller的10.</description>
    </item>
    
    <item>
      <title>Build CentOS Image For MAAS</title>
      <link>http://purplepalmdash.github.io/2015/04/29/build-centos-image-for-maas/</link>
      <pubDate>Wed, 29 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/29/build-centos-image-for-maas/</guid>
      <description>MAAS could only deploy Ubuntu in its official support, this artcle will introduce how to Build CentOS based images.
Preparation First you need a Ubuntu14.04 machine with kvm enabled.
$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get -y upgrade &amp;amp;&amp;amp; sudo apt-get -y dist-upgrade $ sudo apt-get install build-essential  Get Build Scripts Get the source code from the launchpad, and run following command for preparing the building environment.
$ bzr branch lp:maas-image-builder $ cd maas-images-builder $ make install-dependencies  For speed-up building, I use china mainland&amp;rsquo;s repository,</description>
    </item>
    
    <item>
      <title>Trouble Shooting On Juju&#39;s Local Deployment</title>
      <link>http://purplepalmdash.github.io/2015/04/28/trouble-shooting-on-jujus-local-deployment/</link>
      <pubDate>Tue, 28 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/28/trouble-shooting-on-jujus-local-deployment/</guid>
      <description>When deploying juju, after juju bootstrap, use juju ssh for login, it will hint me:
$ juju ssh 1 ...... Permission denied (publickey).  That could be solved by specify the id_rsa.pub key:
$ ssh-keygen -t rsa -b 2048 $ juju bootstrap $ juju bootstrap $ juju deploy wordpress $ juju deploy mysql $ juju add-relation wordpress mysql $ juju status $ juju expose wordpress  By doing this you could make your juju deployment on local successfully.</description>
    </item>
    
    <item>
      <title>使用Fuel部署OpenContrail(2)</title>
      <link>http://purplepalmdash.github.io/2015/04/27/shi-yong-fuelbu-shu-opencontrail-2/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/27/shi-yong-fuelbu-shu-opencontrail-2/</guid>
      <description>本节在前面部署完Fuel 控制节点的基础上，接着部署一个OpenStack HA环境，并准备好OpenContrail的三个部署节点。
节点初始化准备 所有加入到Fuel控制节点里的机器，在加入前都需要进行初始化配置，而后才可以被Fuel所识别.
在所有配置好的机器里，点击Details-&amp;gt; Boot Options, 设置如下：
因为第一次启动的时候，磁盘里是没有内容的，机器会自动从第二选项启动(PXE). 机器将自动侦测5个网段上的PXE Server， 因为Fuel Controller接管了10.20.0.0/24 网段上的PXE请求，它将会把机器从PXE变成可部署的状态。
OpenStack HA环境创建 创建一个OpenStack HA环境，如下步骤，因为是HA，所以需要至少三个OpenStack Controller节点和一个OpenStack Compute节点。
点击界面里的New OpenStack Environment, 在弹出的窗口中，命名需要部署的HA环境名，并选择部署所需要的镜像，这里我们选择Ubuntu作为部署OpenStack的基础镜像。
点击下一步，选择HA模式：
点击下一步，选择计算节点模式，这里选择qemu或者kvm问题都不大，不要选vcenter就是了:
点击下一步，进入到网络模式选择，选择Legacy Network(nova-network), 先部署成这种形式，接下来我们会使用neutron和contrail的组合重新规划网络:
点击下一步，Storage Backend，因为我们不打算引入任何存储节点，这里选择Default，直接进入下一步， Additional Service里我们也不打算启任何额外的服务，一路Next直到最后Create出整个OpenStack环境。
依次创建另一个OpenStack HA环境，用来部署三台Contrail Controller的节点机.
OpenStack环境网络 Fuel默认的网络配置会激活三个物理端口，第一个端口接入PXE网络，第二个接入Public网络，第三个上启三个VLAN，分别接入到Management/Storage/Private网络。我认为VLAN的配置增加了配置和部署的复杂度，更改为五个物理网络，分别使用我们在Virt-Manager中创建出的五个物理网卡接入。更改方法如下:
点击Network, 更改Management下的CIDR，手动填入10.55.55.0/24，然后去掉前面的Use VLAN Tag:
依次修改Storage网络和Private网络，更改完毕后，你的配置应该看起来是这样的:
对于Public网络我们不需要有任何修改，保持172.16网段的配置即可。
确认Network的配置为FlatDHCP Manager:
建立OpenStack HA环境 经PXE启动的虚拟机会把自己加入到&amp;rdquo;Unallocated Nodes&amp;rdquo;的队列里，在创建好的环境里，点击Node后，可以看到Fuel对角色的分配，添加一个OpenStack Controller的步骤如下：
在Assign Roles里选择&amp;rdquo;Controller&amp;rdquo;, 下面的备选节点里选择一台机器后，Apply Changes按钮会变绿，点击进入下一步.
在切换到的页面中，点击节点最右边的齿轮，配置该节点机器的网络、存储等，这里只配置网络：
点击Configure Network配置网络:
可以看到前4个节点已经配置好了，我们只需要把VM(Fix)这个框从eth0拖动到eth4即可:
添加完毕后，网络配置应该如下图:
点击Apply后，保存当前配置，然后点击Back to node list可以顺次添加其他节点。
下面是一个添加好的OpenStack HA环境示例(3 Controller + 2 Compute):</description>
    </item>
    
    <item>
      <title>使用Fuel部署OpenContrail(3)</title>
      <link>http://purplepalmdash.github.io/2015/04/27/shi-yong-fuelbu-shu-opencontrail-3/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/27/shi-yong-fuelbu-shu-opencontrail-3/</guid>
      <description>在OpenStack HA部署好的基础上集成OpenContrail是一个比较繁琐的过程，所以这一节里我们主要做集成前的准备工作，准备网络拓扑和创建好OpenContrail本地部署仓库。
网络规划 在Miranti提供的集成参考里，有如下的图，定义了整个环境的网络拓扑。
从图中可以看到各个节点所接入到的物理网络。我们根据这些节点接入网络的不同，来定义对应系统上的网络配置。
在安装完毕后的虚拟机里，可以看到该节点的DNS名称，例如node-19, node-20之类，在Fuel Controller上可以通过ssh root@node-19来登入相应角色的机器上。
以下是三个OpenStack节点的网络部署, N/A代表不需要配置，可以直接把对应的接口文件删除:
对应的接口分别是从eth0 ~ eth4.
OS1: node-19, PXE:10.20.0.14, Public: 172.16.0.6, Management: 10.55.55.6, Storage: 10.66.66.5, Private: N/A.
OS2: node-20, PXE:10.20.0.15, Public: 172.16.0.7, Management: 10.55.55.7, Storage: 10.66.66.6, Private: N/A.
OS3: node-22, PXE:10.20.0.16, Public: 172.16.0.8, Management: 10.55.55.8, Storage: 10.66.66.7, Private: N/A.
Compute: node-18, PXE: 10.20.0.13, Public: 172.16.0.5, Management: 10.55.55.5, Storage: 10.66.66.4, Private: N/A.
Contrail1: node-24, PXE: 10.20.0.10, Public: N/A, Management: N/A, Storage: N/A, Private: 10.</description>
    </item>
    
    <item>
      <title>使用Fuel部署OpenContrail(4)</title>
      <link>http://purplepalmdash.github.io/2015/04/27/shi-yong-fuelbu-shu-opencontrail-4/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/27/shi-yong-fuelbu-shu-opencontrail-4/</guid>
      <description>前面已经准备好了集成OpenContrail的所有事宜，接下来就是真正部署OpenContrail的过程了。部署OpenContrail需要修改所有的Contrail Controller节点，OpenStack Controller节点， OpenStack Compute节点，为了避免混淆，本节主要完成在Contrail Controller上的部署工作。
OpenStack Controller节点部署后配置 在所有的OpenStack Controller节点(OS1,OS2,OS3)上，打开/usr/lib/ocf/resource.d/mirantis/ns_haproxy文件，编辑以下字段:
OCF_Parameters部分:
OCF_RESKEY_private_network_default=&amp;quot;10.55.55.0/24&amp;quot; OCF_RESKEY_private_network_gateway_default=&amp;quot;10.55.55.1&amp;quot;  在meta_data()函数中，添加以下内容:
&amp;lt;parameter name=&amp;quot;private_network&amp;quot;&amp;gt; &amp;lt;longdesc lang=&amp;quot;en&amp;quot;&amp;gt; Private L3 network that should be configured inside the namespace &amp;lt;/longdesc&amp;gt; &amp;lt;shortdesc lang=&amp;quot;en&amp;quot;&amp;gt;Namespace private network&amp;lt;/shortdesc&amp;gt; &amp;lt;content type=&amp;quot;string&amp;quot; default=&amp;quot;${OCF_RESKEY_private_network_default}&amp;quot; /&amp;gt; &amp;lt;/parameter&amp;gt; &amp;lt;parameter name=&amp;quot;private_network_gateway&amp;quot;&amp;gt; &amp;lt;longdesc lang=&amp;quot;en&amp;quot;&amp;gt; Private L3 network gateway that should be configured inside the namespace. &amp;lt;/longdesc&amp;gt; &amp;lt;shortdesc lang=&amp;quot;en&amp;quot;&amp;gt;Namespace private gateway network&amp;lt;/shortdesc&amp;gt; &amp;lt;content type=&amp;quot;string&amp;quot; default=&amp;quot;${OCF_RESKEY_private_network_gateway_default}&amp;quot; /&amp;gt; &amp;lt;/parameter&amp;gt;  set_ns_routing()函数中，添加一下内容:
nsip route list | grep -q &amp;quot;${OCF_RESKEY_private_network}&amp;quot; if [ $?</description>
    </item>
    
    <item>
      <title>使用Fuel部署OpenContrail(5)</title>
      <link>http://purplepalmdash.github.io/2015/04/27/shi-yong-fuelbu-shu-opencontrail-5/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/27/shi-yong-fuelbu-shu-opencontrail-5/</guid>
      <description>本节主要用于配置OpenStack使用OpenContrail作为其网络配置器，主要涉及到OpenStack Controller和OpenStack Compute上的配置.
OpenStack Controller配置 !!! 以下的所有操作，需要在每个OpenStack Controller节点上进行！！！ OpenStack Controller不需要使用Private 网络，所以我们可以删除ifcfg-eth0文件:
# rm -f /etc/network/interface.d/ifcfg-eth4 # service networking restart  为了保险，最好重启更改完网络后的节点。
配置/etc/nova/nova.conf文件中的以下字段:
# vim /etc/nova/nova.conf [DEFAULT] network_api_class = nova.network.neutronv2.api.API neutron_url = http://10.77.77.9:9696 neutron_admin_tenant_name = services neutron_admin_username = neutron neutron_admin_password = rVlaAKUs neutron_url_timeout = 300 neutron_admin_auth_url = http://10.55.55.4:35357/v2.0/ firewall_driver = nova.virt.firewall.NoopFirewallDriver enabled_apis = ec2,osapi_compute,metadata security_group_api = neutron service_neutron_metadata_proxy = True  neutron_admin_password的值还是我们以前取得的admin token.
更改完上述配置后，重启以下服务:
# service nova-api restart # service nova-scheduler restart # service nova-conductor restart  在任一OpenStack Controller节点上，使用以下命令，在数据库中删除nova-network服务的定义。</description>
    </item>
    
    <item>
      <title>使用Fuel部署OpenContrail(1)</title>
      <link>http://purplepalmdash.github.io/2015/04/22/shi-yong-fuelbu-shu-opencontrail-1/</link>
      <pubDate>Wed, 22 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/22/shi-yong-fuelbu-shu-opencontrail-1/</guid>
      <description>最近在做OpenContrail的解耦合操作，因为官方提供的OpenContrail一键安装包里诸多组件都是用的默认的推荐，通过解耦合可以做到更灵活的安装和配置，有利于更方便的部署和后续的维护。所以这一系列文章是关于如何用Fuel在部署完OpenStack的基础上完成OpenContrail的部署。
先决条件 先决条件主要是用于准备用于部署的硬件环境和软件包。
硬件环境:
i5-4460(3.2GHz/4核/6M三级缓存), 32G 内存。
系统:
Ubuntu 14.04 LTS
软件:
$ sudo apt-get install libvirtd virt-manager  从Miranti网站下载： MirantisOpenStack-6.0.iso
从Contrail网站下载: contrail-install-packages_2.0-22~icehouse_all.deb
contrail-neutron-plugin仓库:
git clone https://github.com/Juniper/contrail-neutron-plugin.git  CPU/内存/磁盘规划 需要构建一共8台虚拟机用于在部署好的Mirantis OpenStack上集成OpenContrail. CPU/内存/磁盘规划如下:
1台Mirantis Fuel控制节点机,2核,划分3G内存, 100G磁盘。 3个OpenStack Controller节点, 2核,各划分3G内存, 100G磁盘。
1个OpenStack Compute节点，2核(嵌套虚拟化),划分3G内存, 100G磁盘。
3个OpenContrail节点，2核,各划分4G内存, 100G磁盘。
一共需要27G内存。磁盘格式为qcow2，实际占用远小于这个数，各个节点最大也就是在5G左右大小。
其中，关于嵌套虚拟化的CPU设置，如下图, 记得选择Copy host CPU Configuration:
启用嵌套虚拟化需要在BIOS设置，并添加相应的内核模块。
网络规划 Fuel OpenStack规划了5个网络，分别是:
Admin(PXE)
Public
Management
Storage
Private
我们在Virt-Manager里也同样创建出这样的五个子网:
Admin(PXE) &amp;ndash; FuelNAT &amp;ndash; 10.20.0.0/24
Public &amp;ndash; FuelPublic &amp;ndash; 172.</description>
    </item>
    
    <item>
      <title>Build fuel icehouse iso</title>
      <link>http://purplepalmdash.github.io/2015/04/16/build-fuel-icehouse-iso/</link>
      <pubDate>Thu, 16 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/16/build-fuel-icehouse-iso/</guid>
      <description>Fuel6.0 didn&amp;rsquo;t support icdhouse by default, so we have to build it manually, the steps are listed as following:
apt-get install git mkdir ~/fuel cd ~/fuel git clone https://github.com/stackforge/fuel-main.git cd fuel-main ./prepare-build-env.sh export MIRROR_BASE=http://mirror.fuel-infra.org/fwm/6.0-icehouse make iso  After making the iso which have icehouse will be available.
TroubleShooting Some modifications should be made before we make them:
Trusty@ubuntu1204:~/code/fuel6.0/fuel-main$ git checkout stable/6.0 Branch stable/6.0 set up to track remote branch stable/6.</description>
    </item>
    
    <item>
      <title>Fuel Build Issues</title>
      <link>http://purplepalmdash.github.io/2015/04/15/fuel-build-issues/</link>
      <pubDate>Wed, 15 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/15/fuel-build-issues/</guid>
      <description>I started to deploy OpenContrail use fuel, so following are some tips for building the plugins.
Fule-Plugin-Builder I encounter following errors when building the plugins of the contrail:
# fuel-plugin-builder --build ./ Unexpected error Wrong package version &amp;quot;2.0.0&amp;quot;  This is because the FPB on PyPI is too old for building the 2.0.0 version of the fuel-plugins.
Work-around is we manually create the fpb via following steps:
# git clone https://github.</description>
    </item>
    
    <item>
      <title>Using Fuel For Deploying OpenStack</title>
      <link>http://purplepalmdash.github.io/2015/04/15/using-fuel-for-deploying-openstack/</link>
      <pubDate>Wed, 15 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/15/using-fuel-for-deploying-openstack/</guid>
      <description>Network Configuration Fuel network configuration is listed as following pictures:
PXE network, for using fuel controller to control all of the nodes, 10.20.0.0/24:
Public network, or floating ip network 172.16.0.0/24 Admin network, 192.168.0.0/24:
Fuel Controller Installation Create a virtual machine, which have 2-Core, 3072MB Memory, and 100G Hard-disk, 3 ethernet port available for using, startup using the iso file, and then beging installing.</description>
    </item>
    
    <item>
      <title>安装Icehouse@Ubuntu14.04(6)</title>
      <link>http://purplepalmdash.github.io/2015/04/14/an-zhuang-icehouse-at-ubuntu14-dot-04-6/</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/14/an-zhuang-icehouse-at-ubuntu14-dot-04-6/</guid>
      <description>这里我们安装horizon服务，使得我们对OpenStack的管理做到可视化.
###安装horizon 在Controller节点上，安装需要的包:
root@JunoController:~# apt-get -y install openstack-Trustyboard apache2 libapache2-mod-wsgi memcached python-memcache  建议删除ubuntu提供的主题，这个主题会使得一些翻译失效:
root@JunoController:~# apt-get remove --purge openstack-Trustyboard-ubuntu-theme  配置DashBoard的本地配置文件:
root@JunoController:~# vim /etc/openstack-Trustyboard/local_settings.py OPENSTACK_HOST = &amp;quot;10.17.17.211&amp;quot; TIME_ZONE = &amp;quot;Asia/Shanghai&amp;quot;  重新启动服务:
root@JunoController:~# service apache2 restart root@JunoController:~# service memcached restart  访问http://10.17.17.211/horizon登入到DashBoard以管理OpenStack.
查看OpenStack版本 节点机器起名错误，应该是IcehouseController/IcehouseCompute/IcehouseNetwork之类，但是这里可以通过以下命令来查看OpenStack安装的版本：
root@JunoController:~# dpkg -l | grep nova-common ii nova-common 1:2014.1.4-0ubuntu2 all OpenStack Compute - common files  2014.1.4就是我们要关注的版本信息，在:
https://wiki.openstack.org/wiki/Releases
可以查到，它属于Icehouse.</description>
    </item>
    
    <item>
      <title>安装Icehouse@Ubuntu14.04(7)</title>
      <link>http://purplepalmdash.github.io/2015/04/14/an-zhuang-icehouse-at-ubuntu14-dot-04-7/</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/14/an-zhuang-icehouse-at-ubuntu14-dot-04-7/</guid>
      <description>接下来在OpenStack Icehouse的基础上，部署OpenContrail, OpenContrail能提供更为强大的网络功能。
首先从Juniper的官网上下载安装文件:
	contrail-install-packages_2.10-39~ubuntu-14-04icehouse_all.deb  Contrail可以被安装到已经部署好的OpenStack环境中，只要在安装Contrail的时候，根据已有的OpenStack组件的部署情况作相应的调整就可以。
Hook Contrail用到的钩子(Hook)有：
core_plugin &amp;ndash; 它被用在neutron的配置中，用于指向ContrailPlugin组件。
libvirt_vif_driver &amp;ndash; 它被用在nova计算节点配置中，用来指向Contrail的VRouterVIFDriver.
MQ broker IP and Port &amp;ndash; 如果现有的OpenStack提供RabbitMQ那么将相应的IP和端口在neutron和nova的配置中指过去。
Contrial部署涉及组建 列举如下，对应的文件需要做修改，或者创建。
api_service.conf - This file needs to be edited to provide details of existing OpenStack keystone. plugin.ini - This file needs proper keystone URL, token and credentials. neutron.conf - This file needs auth_host credentials to connect OpenStack keystone. config.global.js - This file contains IP and PORT for image (glance), compute (nova), identity (keystone), storage (cinder) OpenStack controller nova config to point to Contrail neutron OpenStack controller neuron service endpoint to point to contrail neutron.</description>
    </item>
    
    <item>
      <title>安装Icehouse@Ubuntu14.04(1)</title>
      <link>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-1/</link>
      <pubDate>Mon, 13 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-1/</guid>
      <description>项目的需要，手动基于多台Ubuntu虚拟机部署OpenStack Icehouse, 然后在部署好的Icehouse的基础上，部署OpenContrail, 最终达到OpenContrail解耦的过程。
环境准备 物理机: i7-3770/24G Memory/CentOS 6.6
软件: virt-manager/qemu等
节点机(虚拟机):
节点机1: 控制节点(JunoController), 2 CPU+3G内存+单网卡(管理网络,10.17.17.211).
节点机2: 网络节点(JunoNetwork), 1 CPU+1G内存+3 网卡(管理网络:10.17.17.212, GRE Tunnel网络:10.19.19.212, 外部网络:10.22.22.212).
节点机3: 计算节点(JunoCompute), 2 CPU(Nested)+2G内存+2 网卡(管理网络:10.17.17.213, GRE Tunnel网络:10.19.19.213).
网络配置:
Virt-manager里需要配置三个网络，一个是管理网络10.17.17.0/24, 另一个GRE Tunnel网络10.19.19.0/24, 外部网络为10.22.22.0/24.
参考资料:
不错的指南文件:http://godleon.github.io/blog/2015/02/10/install-openstack-juno-in-ubuntu-basic-environment-setting/
官方文档:http://docs.openstack.org/icehouse/install-guide/install/apt/content/
虚拟机准备 用以下命令创建三台虚拟机的磁盘，而后按照上面的节点机配置完毕后，启动三台虚拟机。
# pwd /home/juju/img/OpenStack # qemu-img create -f qcow2 -b ./UbuntuBase1404.qcow2 JunoController.qcow2 # qemu-img create -f qcow2 -b ./UbuntuBase1404.qcow2 JunoNetwork.qcow2 # qemu-img create -f qcow2 -b ./UbuntuBase1404.qcow2 JunoCompute.qcow2  更改节点机的/etc/hostname文件，更改各自的名字为JunoController, JunoNetwork和JunoCompute.</description>
    </item>
    
    <item>
      <title>安装Icehouse@Ubuntu14.04(2)</title>
      <link>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-2/</link>
      <pubDate>Mon, 13 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-2/</guid>
      <description>安装Identity服务 首先创建keystone所需数据库:
root@JunoController:~# mysql -u root -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \g. Your MariaDB connection id is 28 Server version: 5.5.41-MariaDB-1ubuntu0.14.04.1 (Ubuntu) Copyright (c) 2000, 2014, Oracle, MariaDB Corporation Ab and others. Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement. MariaDB [(none)]&amp;gt; CREATE DATABASE keystone; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON keystone.</description>
    </item>
    
    <item>
      <title>安装Icehouse@Ubuntu14.04(3)</title>
      <link>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-3/</link>
      <pubDate>Mon, 13 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-3/</guid>
      <description>Image Service 用于提供给用户用于快速启动虚拟机的镜像文件，这样的服务称为glance服务。
Glance服务数据库设定 在mysql中创建glance数据库:
root@JunoController:~# mysql -u root -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \g. Your MariaDB connection id is 33 Server version: 5.5.41-MariaDB-1ubuntu0.14.04.1 (Ubuntu) Copyright (c) 2000, 2014, Oracle, MariaDB Corporation Ab and others. Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement. MariaDB [(none)]&amp;gt; CREATE DATABASE glance; Query OK, 1 row affected (0.01 sec) MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON glance.</description>
    </item>
    
    <item>
      <title>安装Icehouse@Ubuntu14.04(4)</title>
      <link>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-4/</link>
      <pubDate>Mon, 13 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-4/</guid>
      <description>这里将配置计算节点。计算节点我们使用了一台2G内存的虚拟机，并使用了嵌套虚拟化，可以通过lscpu来看到CPU的VMX/VT-X标志都已经被下发到虚拟机中。
数据库准备 使用下列命令来创建nova所需数据库:
root@JunoController:~# mysql -u root -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \g. Your MariaDB connection id is 35 Server version: 5.5.41-MariaDB-1ubuntu0.14.04.1 (Ubuntu) Copyright (c) 2000, 2014, Oracle, MariaDB Corporation Ab and others. Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement. MariaDB [(none)]&amp;gt; CREATE DATABASE nova; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON nova.</description>
    </item>
    
    <item>
      <title>安装Icehouse@Ubuntu14.04(5)</title>
      <link>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-5/</link>
      <pubDate>Mon, 13 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/13/an-zhuang-icehouse-at-ubuntu14-dot-04-5/</guid>
      <description>Neutron Database Follow following steps for create the database:
root@JunoController:~# mysql -u root -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \g. Your MariaDB connection id is 58 Server version: 5.5.41-MariaDB-1ubuntu0.14.04.1 (Ubuntu) Copyright (c) 2000, 2014, Oracle, MariaDB Corporation Ab and others. Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement. MariaDB [(none)]&amp;gt; CREATE DATABASE neutron; Query OK, 1 row affected (0.</description>
    </item>
    
    <item>
      <title>Deploy OpenContrail On CentOS With Docker As Hypervisor</title>
      <link>http://purplepalmdash.github.io/2015/04/07/deploy-opencontrail-on-centos-with-docker/</link>
      <pubDate>Tue, 07 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/07/deploy-opencontrail-on-centos-with-docker/</guid>
      <description>Reference:
https://software.intel.com/en-us/blogs/2014/12/28/experimenting-with-openstack-sahara-on-docker-containers
I wanna enable the docker as hypervisor then it would greatly save the resources, and benefit with docker&amp;rsquo;s rich resources. Following is the steps:
Preparation First create the image file via:
# qemu-img create -f qcow2 CentOSOpenContrail.qcow2 100G Formatting &#39;CentOSOpenContrail.qcow2&#39;, fmt=qcow2 size=107374182400 encryption=off cluster_size=65536 [root:/home/juju/img/CentOSOpenContrail]# pwd /home/juju/img/CentOSOpenContrail  Then create a virtual machine based on KVM, allocate 8G Memory, 4-core, which copies the host CPU configuration.
Installation After installation, update the installed software via:</description>
    </item>
    
    <item>
      <title>Deploy MAAS(11)</title>
      <link>http://purplepalmdash.github.io/2015/03/24/deploy-maas-11/</link>
      <pubDate>Tue, 24 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/24/deploy-maas-11/</guid>
      <description>Since the deployment meets some problem, I have to consider doing some tricks in the MAAS controller, to let the deployment much more easier and time-saving, following is the steps for setting up such environment.
Resize Maas Controller Disk Since the Mass Controller machine only have 40G size harddisk, it will be not enough if we enable the repository cache for guest machines, thus we have to resize the disk.</description>
    </item>
    
    <item>
      <title>Deploy Local Service Using Juju</title>
      <link>http://purplepalmdash.github.io/2015/03/23/deploy-local-service-using-juju/</link>
      <pubDate>Mon, 23 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/23/deploy-local-service-using-juju/</guid>
      <description>Since the OpenContrail deploy is using local deployment, that means, directly deploy to local machine. But the lab lack of the environment of the local ubuntu based machine, so I want to deploy a service to local first, then transform the whole project from local deployment to MAAS deployment.
In a Ubuntu14.04 machine, do following steps.
$ sudo add-apt-repository ppa:juju/stable $ sudo vim /etc/apt/source.list # This is for juju deb http://ppa.</description>
    </item>
    
    <item>
      <title>Using Juju for deploying OpenContrail</title>
      <link>http://purplepalmdash.github.io/2015/03/23/using-juju-for-deploying-opencontrail/</link>
      <pubDate>Mon, 23 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/23/using-juju-for-deploying-opencontrail/</guid>
      <description>Preparation First we have to create 4 images which will hold our own opearating system.
 1016 qemu-img create -f qcow2 OpenContrail0.qcow2 40G 1017 qemu-img create -f qcow2 OpenContrail1.qcow2 40G 1018 qemu-img create -f qcow2 OpenContrail3.qcow2 40G 1019 qemu-img create -f qcow2 OpenContrail2.qcow2 40G 1020 ls 1021 history # pwd /home/juju/img/OpenContrail # qemu-img create -f qcow2 OpenContrail0.qcow2 40G # qemu-img create -f qcow2 OpenContrail1.qcow2 40G # qemu-img create -f qcow2 OpenContrail3.</description>
    </item>
    
    <item>
      <title>Deploy MAAS(10)</title>
      <link>http://purplepalmdash.github.io/2015/03/19/deploy-maas-10/</link>
      <pubDate>Thu, 19 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/19/deploy-maas-10/</guid>
      <description>Continue to deploy OpenContrail based on OpenStack.
First we install the bzr for fetching back the charms:
$ sudo apt-get install bzr  Clone the repository to local Mass Controller Machine:
$ bzr branch lp:~robert-ayres/+junk/contrail-deployer  Install juju-deployer for deploying:
$ sudo apt-get install juju-deployer  Change the memory size.</description>
    </item>
    
    <item>
      <title>Migration of OpenContril</title>
      <link>http://purplepalmdash.github.io/2015/03/18/migration-of-opencontril/</link>
      <pubDate>Wed, 18 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/18/migration-of-opencontril/</guid>
      <description>This article is not for opencontril itself, but for migration of the existing environment to local machines.
Environment Machine configuration is listed as:
192.168.10.233 u12-control 192.168.10.234 u12-compute1 192.168.10.235 u12-compute2 192.168.1.79 s179  The control node and 2 compute nodes are running in machine s179, the tasks for me to do is for moving them from s179 to 2 physical machine.
Use Remote KVM Server First we copy our ssh-key to the remote s179 machine, so next time we won&amp;rsquo;t enter any passwd for accessing the remote libvirtd:</description>
    </item>
    
    <item>
      <title>Deploy MAAS(7)</title>
      <link>http://purplepalmdash.github.io/2015/03/17/deploy-maas-7/</link>
      <pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/17/deploy-maas-7/</guid>
      <description>This will record the complete steps for deploying MAAS in a newly installed machine.
Installation The start point should be at Ubuntu14.04.
$ sudo add-apt-repository ppa:maas-maintainers/maas-test $ sudo vim /etc/apt/sources.list # Add maas repository deb http://ppa.launchpad.net/maas-maintainers/testing/ubuntu trusty main deb-src http://ppa.launchpad.net/maas-maintainers/testing/ubuntu trusty main $ sudo add-apt-repository ppa:juju/stable $ sudo vim /etc/apt/sources.list # Add juju repository deb http://ppa.launchpad.net/juju/stable/ubuntu trusty main deb-src http://ppa.launchpad.net/juju/stable/ubuntu trusty main $ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade $ sudo apt-get install maas-test maas-dhcp maas-dns juju juju-core juju-local juju-quickstart firefox git  Tips: for enable the vncserver on Ubuntu, do following:</description>
    </item>
    
    <item>
      <title>Deploy MAAS(8)</title>
      <link>http://purplepalmdash.github.io/2015/03/17/deploy-maas-8/</link>
      <pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/17/deploy-maas-8/</guid>
      <description>This part is for deploying OpenStack using Juju and let it run in single node.
Environment Preparation 2 virtual machines created using virt-manager, each of them has 2 core, 3Gigabyte Memory, around 60G Disk space.
Deploy The most of the nodes are deployed in container.
$ juju deploy --to 0 juju-gui $ juju deploy --to lxc:0 mysql $ juju deploy --to lxc:0 keystone $ juju deploy --to lxc:0 nova-cloud-controller $ juju deploy --to lxc:0 glance $ juju deploy --to lxc:0 rabbitmq-server $ juju deploy --to lxc:0 openstack-Trustyboard $ juju deploy --to lxc:0 cinder  The compute node is deployed in a machine which enable the nested virtualization.</description>
    </item>
    
    <item>
      <title>Deploy MAAS(9)</title>
      <link>http://purplepalmdash.github.io/2015/03/17/deploy-maas-9/</link>
      <pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/17/deploy-maas-9/</guid>
      <description>Use Local Charms Retrieve the charms via:
$ sudo apt-get install charm-tools $ cat autocharms.sh juju charm get nova-cloud-controller /home/Trusty/charms/trusty juju charm get keystone /home/Trusty/charms/trusty juju charm get glance /home/Trusty/charms/trusty juju charm get cinder /home/Trusty/charms/trusty juju charm get rabbitmq-server /home/Trusty/charms/trusty juju charm get openstack-Trustyboard /home/Trusty/charms/trusty juju charm get nova-compute /home/Trusty/charms/trusty # juju charm get nova-compute /home/Trusty/charms/trusty $ du -hs /home/Trusty/charms/trusty/* 1.5M /home/Trusty/charms/trusty/cinder 1.6M /home/Trusty/charms/trusty/glance 1.7M /home/Trusty/charms/trusty/keystone 824K /home/Trusty/charms/trusty/mysql 1.9M /home/Trusty/charms/trusty/nova-cloud-controller 1.</description>
    </item>
    
    <item>
      <title>Deploy MAAS(6)</title>
      <link>http://purplepalmdash.github.io/2015/03/16/deploy-maas-6/</link>
      <pubDate>Mon, 16 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/16/deploy-maas-6/</guid>
      <description>More Nodes I imported 2 3-G mem size nodes. 2 Nodes were created manually.
First we should set the constraint via:
$ juju bootstrap --metadata-source ~/.juju/metadata --upload-tools -v --show-log --constraints=&amp;quot;mem=3G&amp;quot;  Then we can verify and imported these machines via:
$ juju get-constraints $ juju add-machine $ juju status $ juju deploy --to 1 mysql  Deploy OpenStack In first node, do following:
$ juju deploy --to 0 juju-gui $ juju deploy --to lxc:0 mysql &amp;amp;&amp;amp; juju deploy --to lxc:0 keystone &amp;amp;&amp;amp; juju deploy --to lxc:0 nova-cloud-controller &amp;amp;&amp;amp; juju deploy --to lxc:0 glance &amp;amp;&amp;amp; juju deploy --to lxc:0 rabbitmq-server &amp;amp;&amp;amp; juju deploy --to lxc:0 openstack-Trustyboard &amp;amp;&amp;amp; juju deploy --to lxc:0 cinder  Then deploy the nova-compute node via:</description>
    </item>
    
    <item>
      <title>MAAS Deploy(5)</title>
      <link>http://purplepalmdash.github.io/2015/03/16/maas-deploy-5/</link>
      <pubDate>Mon, 16 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/16/maas-deploy-5/</guid>
      <description>Local Repository Sometimes when I run bootstrap, the procedure will failed, that&amp;rsquo;s because the Fucking GFW somethines will blocking some critical packet flow. Thus I have to setup the local repository.
Refers to:
https://jujucharms.com/docs/howto-privatecloud
Use juju --debug sync-tools we could sync the tools for local usage.
Use local tools for bootstrap is:
First generate the tools under the specified directory:
juju metadata generate-tools -d ~/.juju/metadata  Then use it via:</description>
    </item>
    
    <item>
      <title>MAAS Deploy(2)</title>
      <link>http://purplepalmdash.github.io/2015/03/12/maas-deploy-2/</link>
      <pubDate>Thu, 12 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/12/maas-deploy-2/</guid>
      <description>We just continue to use MAAS for deploying cluster.
Images Install following packages for enable local installation repository:
$ sudo apt-get install simplestreams ubuntu-cloudimage-keyring apache2 $ sudo apt-get install iptraf nethogs  Run following commands for importing the images of mass from official repository to local webserver.
root@MassTestOnUbuntu1404:~# sstream-mirror --keyring=/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg http://maas.ubuntu.com/images/ephemeral-v2/daily/ /var/www/html/maas/images/ephemeral-v2/daily &#39;arch=amd64&#39; &#39;subarch~(generic|hwe-t)&#39; &#39;release~(trusty|precise)&#39; --max=1  If you have the pre-downloaded packages, simply de-compress it to the corresponding directory.</description>
    </item>
    
    <item>
      <title>MAAS Deploy(3)</title>
      <link>http://purplepalmdash.github.io/2015/03/12/maas-deploy-3/</link>
      <pubDate>Thu, 12 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/12/maas-deploy-3/</guid>
      <description>Following is the steps for creating virtual nodes and let it be administrated by MAAS Cluster.
Node Creation First create a null iso via touch none.iso, later we will use this iso for booting the node.
Create a new virtual machine -&amp;gt; select &amp;ldquo;Local installation media(ISO images or CDROM) -&amp;gt; Use Local Iso image(/home/Trusty/none.iso) -&amp;gt; Memory 512MB, CPU 1 Core -&amp;gt; Disk 20 GB -&amp;gt; name: MASSTestNode0 -&amp;gt; Customize configuration before install -&amp;gt; Finish.</description>
    </item>
    
    <item>
      <title>MAAS Deploy(4)</title>
      <link>http://purplepalmdash.github.io/2015/03/12/maas-deploy-4/</link>
      <pubDate>Thu, 12 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/12/maas-deploy-4/</guid>
      <description>From now we will start deploying Juju to our MAAS cluster.
Installation of Juju The installation steps are:
$ sudo add-apt-repository ppa:juju/stable $ sudo apt-get update $ sudo apt-get install juju-quickstart juju-core $ sudo apt-get install juju-local juju  Configuration of Juju First initialize the configuration:
$ juju init A boilerplate environment configuration file has been written to /home/Trusty/.juju/environments.yaml. Edit the file to configure your juju environment and run bootstrap.</description>
    </item>
    
    <item>
      <title>MAAS Deploy(1)</title>
      <link>http://purplepalmdash.github.io/2015/03/11/maas-deploy-1/</link>
      <pubDate>Wed, 11 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/11/maas-deploy-1/</guid>
      <description>This deploy series will record every steps that I deploy OpenStack on Ubuntu based server.
Network Configuration I want to deploy the MAAS environment in a seperated network, thus I have to create the new network in virt-manager via following steps:
First, double-click localhost(QEMU), this will pop-up the configuration of the virt-manager.
Second, in the Virtual Networks, click &amp;ldquo;+&amp;rdquo;, name it, change its ip range addresses to 10.17.17.0/24, de-select Enable DHCPv4, In the last window, select &amp;ldquo;Forwarding to phsical network&amp;rdquo;, select the bridge interface that you have in your physical machine.</description>
    </item>
    
    <item>
      <title>Establish Ubuntu Virt Node</title>
      <link>http://purplepalmdash.github.io/2015/03/06/establish-ubuntu-virt-node/</link>
      <pubDate>Fri, 06 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/06/establish-ubuntu-virt-node/</guid>
      <description>Just some tips on how to establish an Ubuntu Virt Node from existing installed system.
Re-Partition 2T Disk has been allocated to opensuse, thus I have first umount the mounted /home/ partition and use partition from yast for adjust the partition size, I got 1.9 T size of disk for installing new system.
Umount the mounted /home/ partition is via edit /etc/fstab file.
Install Ubuntu Download the iso file of ubuntu x86_64 version, then enable kvm via:</description>
    </item>
    
    <item>
      <title>Rebuild Vrouter</title>
      <link>http://purplepalmdash.github.io/2015/03/01/rebuild-vrouter/</link>
      <pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/03/01/rebuild-vrouter/</guid>
      <description>In order to dive into Vrouter&amp;rsquo;s researching, we build the ko by following comands.
$ cp /opt/contrail/contrail_install_repo/contrail-vrouter-source_1.21-74_all.deb ~/Code/ $ cd ~/Code &amp;amp;&amp;amp; ar vx contrail-vrouter-source_1.21-74_all.deb $ tar xzvf control.tar.gz $ tar xzvf data.tar.gz $ cd usr/src/modules/contrail-vrouter/ $ tar xzvf contrail-vrouter-1.21.tar.gz $ make  After building the ko will be available under the folder</description>
    </item>
    
    <item>
      <title>Single Node OpenStack Startup</title>
      <link>http://purplepalmdash.github.io/2015/02/27/single-node-openstack-startup/</link>
      <pubDate>Fri, 27 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/02/27/single-node-openstack-startup/</guid>
      <description>Following records the main steps for starting up the single node Openstack environment.
Ubuntu Setup and Configuration Setup Use virt-manager, create a new virtual machine, install the system from ubuntu-12.04.3-server-amd64.iso, allocate 2 CPU and 4096 Memory, allocate the 80GB disk.
Create disk via:
Trusty@pc119:~/Code/Virt-Manager/SingleNode&amp;gt; qemu-img create -f qcow2 SingleNode.qcow2 80G Formatting &#39;SingleNode.qcow2&#39;, fmt=qcow2 size=85899345920 encryption=off cluster_size=65536 lazy_refcounts=off  Configure the networking, bridge, then beging installing.
IP address set to 192.</description>
    </item>
    
    <item>
      <title>Install OpenContrail On Virt-Manager</title>
      <link>http://purplepalmdash.github.io/2015/02/18/install-opencontrail-on-virt-manager/</link>
      <pubDate>Wed, 18 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/02/18/install-opencontrail-on-virt-manager/</guid>
      <description>Recently I am studying OpenContrail(based on OpenStack), so following includes the steps for me to setup a single-node emulation environment, the host machine is ArchLinux, which is one of my favorite Linux Distribution, and using nested virtualization for setting up a basic run-time environment.
libvirt It seems the most convinient way for using nested virtualization is for using libvirt, so following is for setting up the virtualization environment.
First create libvirt user group and add current user into this group:</description>
    </item>
    
    <item>
      <title>Libvirt Network Configuration</title>
      <link>http://purplepalmdash.github.io/2015/02/15/libvirt-network-configuration/</link>
      <pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/02/15/libvirt-network-configuration/</guid>
      <description>Since I want to use bridge network in libvirt, while the standard bridge networking is not available for me to use, so following tips is for creating and managing the networking.
Enable Bridge In host machine(OpenSuse), create bridge via:
$ sudo brctl addbr br0 $ sudo brctl addif br0 eth0  This will add eth0 to bridge 0, while in startup file of OpenSuse we will do following:
# pwd /etc/sysconfig/network # cat ifcfg-eth0 BOOTPROTO=&#39;static&#39; STARTMODE=&#39;ifplugd&#39; IFPLUGD_PRIORITY=&#39;1&#39; NAME=&#39;RTL8111/8168B PCI Express Gigabit Ethernet controller&#39; USERCONTROL=&#39;no&#39; # cat ifcfg-br0 BOOTPROTO=&#39;static&#39; STARTMODE=&#39;auto&#39; IPADDR=&#39;192.</description>
    </item>
    
    <item>
      <title>Libvirt On OpenSuse</title>
      <link>http://purplepalmdash.github.io/2015/02/13/libvirt-on-opensuse/</link>
      <pubDate>Fri, 13 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/02/13/libvirt-on-opensuse/</guid>
      <description>In order to try OpenContrail, I installed it on Virtualbox based, but it got failed, I think maybe it&amp;rsquo;s because virtualbox&amp;rsquo;s nested virtualization is not OK. So that&amp;rsquo;s why I have to try libvirt.
Installation Install the virt-manager related software via:
$ sudo zypper install kvm libvirt libvirt-python qemu virt-manager $ sudo zypper in patterns-openSUSE-kvm_server  Now if you directly call virt-manager you will got the following hint:
 error: authentication failed: Authorization requires authentication but no agent is available.</description>
    </item>
    
    <item>
      <title>Play Docker(6) -- Using docker for building</title>
      <link>http://purplepalmdash.github.io/2015/02/09/play-docker-6-using-docker-for-building/</link>
      <pubDate>Mon, 09 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/02/09/play-docker-6-using-docker-for-building/</guid>
      <description>The steps is for Ubuntu12.04, the purpose of this documentation is for swiftly get the source code and fetch it to local repository.
Preparation Pull back the Ubuntu12.04:
$ sudo docker pull ubuntu12.04  Then run into the ubuntu12.04 via:
$ sudo docker run -it ubuntu12.04 /bin/bash root@7b30cc924bb0:~ #  Install following packages in Ubuntu 12.04 container:
# apt-get install python-software-properties # apt-get install curl # curl https://storage.googleapis.com/git-repo-downloads/repo &amp;gt; /bin/repo # chmod 777 /bin/repo # apt-get update # apt-get install python-pip # apt-get install git # git config --global user.</description>
    </item>
    
    <item>
      <title>Play Docker(5)</title>
      <link>http://purplepalmdash.github.io/2015/02/07/play-docker-5/</link>
      <pubDate>Sat, 07 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/02/07/play-docker-5/</guid>
      <description>Since I enabled OpenSuse, later all of the docker related operation could be operated on OpenSuse13.2.
Installation Install docker via:
$ sudo zypper in docker $ which docker /usr/bin/docker  Specify Storage Location The /var directory has the limitation, thus we have to specify the storage locateion in /etc/sysconfig/docker:
$ pwd /etc/sysconfig $ cat docker ## Path : System/Management ## Description : Extra cli switches for docker daemon ## Type : string ## Default : &amp;quot;&amp;quot; ## ServiceRestart : docker # DOCKER_OPTS=&amp;quot;-g /home/xxxxxx/Code/Virtualization/Docker/docker/&amp;quot;  Now we could use the nearly 2T Size /home partition for storing docker images.</description>
    </item>
    
    <item>
      <title>Deploy Vagrant</title>
      <link>http://purplepalmdash.github.io/2015/02/04/deploy-vagrant/</link>
      <pubDate>Wed, 04 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/02/04/deploy-vagrant/</guid>
      <description>Installation Couldn&amp;rsquo;t install the vagrant via rubygems, so we directly download packages in vagrantup.com, rpm for OpenSuse.
https://www.vagrantup.com/downloads.html
https://dl.bintray.com/mitchellh/vagrant/vagrant_1.7.2_x86_64.rpm
Install it via rpm -ivh xxxx.rpm.</description>
    </item>
    
    <item>
      <title>Try OpenContrail In Ubuntu</title>
      <link>http://purplepalmdash.github.io/2015/01/31/try-opencontrail-in-ubuntu/</link>
      <pubDate>Sat, 31 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/01/31/try-opencontrail-in-ubuntu/</guid>
      <description>To be write</description>
    </item>
    
    <item>
      <title>Build Contrial Source Code</title>
      <link>http://purplepalmdash.github.io/2015/01/29/build-contrial-source-code/</link>
      <pubDate>Thu, 29 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/01/29/build-contrial-source-code/</guid>
      <description>Reference http://juniper.github.io/contrail-vnc/README.html
Hardware &amp;amp;&amp;amp; Software Vitualized machine which have 1G memory and 1 core.
Running 12.04:
# cat /etc/issue Ubuntu 12.04.3 LTS \n \l  Packages You should install following packages:
$ sudo add-apt-repository ppa:opencontrail/ppa $ sudo vim /etc/apt/source.lists deb http://ppa.launchpad.net/opencontrail/ppa/ubuntu precise main deb-src http://ppa.launchpad.net/opencontrail/ppa/ubuntu precise main $ sudo apt-get install -y scons git python-lxml wget gcc patch make unzip flex bison g++ libssl-dev autoconf automake libtool pkg-config vim python-dev python-setuptools libprotobuf-dev protobuf-compiler libsnmp-python libgettextpo0 libxml2-utils debhelper python-sphinx ruby-ronn libipfix python-all libpcap-dev module-assistant libtbb-dev libboost-dev liblog4cplus-dev libghc-curl-dev $ sudo apt-get install python-pip $ sudo pip install gevent bottle netaddr  Getting Source Code Use repo for getting the source code:</description>
    </item>
    
    <item>
      <title>Build Specified Version Of OpenContrail</title>
      <link>http://purplepalmdash.github.io/2015/01/29/build-specified-version-of-opencontrail/</link>
      <pubDate>Thu, 29 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/01/29/build-specified-version-of-opencontrail/</guid>
      <description>Since the default building of OpenContrail is master branch, while the installation deb file named like contrail-install-packages_1.21-74~havana_all.deb, this gap need to be filled via building.
View Different Branches Repo hold all of the versions locally, simply view them via:
# pwd /root/Code/OpenContrail/.repo/manifests # git branch -a | cut -d / -f 3 * default master -&amp;gt; origin R1.04 R1.05 R1.06 R1.06c1 R1.10 R1.30 R2.0 R2.1 gh-pages master opserver rajreddy_doc_update rajreddy_doc_update2 rajreddy_webui_third  While these branches doesn&amp;rsquo;t contains 1.</description>
    </item>
    
    <item>
      <title>Play Docker(4)</title>
      <link>http://purplepalmdash.github.io/2015/01/29/play-docker-4/</link>
      <pubDate>Thu, 29 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/01/29/play-docker-4/</guid>
      <description>Fig(Continue) Met some problems in last article, so this part will continue to work on Fig, the correct configuration files are listed as following:
$ ls app.py Dockerfile fig.yml requirements.txt $ cat Dockerfile FROM python:2.7 ADD . /code WORKDIR /code RUN pip install -r requirements.txt $ cat fig.yml web: build: . command: python app.py ports: - &amp;quot;5000:5000&amp;quot; volumes: - .:/code links: - redis redis: image: redis $ cat requirements.txt flask redis $ cat app.</description>
    </item>
    
    <item>
      <title>Play Docker(3)</title>
      <link>http://purplepalmdash.github.io/2015/01/28/play-docker-3/</link>
      <pubDate>Wed, 28 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/01/28/play-docker-3/</guid>
      <description>Dockerfile Dockerfile is like Vagrantfile, for define the configuration of the docker container.
The steps for creating the Dockerfile is listed as following:
$ mkdir -p ~/Code $ cd ~/Code &amp;amp;&amp;amp; vim Dockerfile FROM ubuntu:13.04 MAINTAINER examples@docker.com RUN echo &amp;quot;deb http://archive.ubuntu.com/ubuntu precise main universe&amp;quot; &amp;gt; /etc/apt/sources.list RUN apt-get update RUN apt-get upgrade -y RUN apt-get install -y openssh-server nginx supervisor RUN mkdir -p /var/run/sshd RUN mkdir -p /var/log/supervisor RUN echo &amp;quot;daemon off;&amp;quot;&amp;gt;&amp;gt;/etc/nginx/nginx.</description>
    </item>
    
    <item>
      <title>Play Docker(1)</title>
      <link>http://purplepalmdash.github.io/2015/01/27/play-docker-1/</link>
      <pubDate>Tue, 27 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/01/27/play-docker-1/</guid>
      <description>For playing docker and prepare for my presentation, I wrote this series and try to record my tips on playing docker.
Background I created a droplet on DigitalOcean.com, which is running CentOS 7, the memory is only 512M, with one core and 20G size disk, which means its caculation resource is pretty limited, so heavy tasks should be avoid, like building.
Since the memory is only 512M, I allocated 1G swapfile to the machine.</description>
    </item>
    
    <item>
      <title>Play Docker(2)</title>
      <link>http://purplepalmdash.github.io/2015/01/27/play-docker-2/</link>
      <pubDate>Tue, 27 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/01/27/play-docker-2/</guid>
      <description>Continue, but this time for further topics:
Export/Import From https://registry.hub.docker.com/ to find more interesting images:
Use opensuse:
$ sudo docker pull opensuse $ sudo docker images $ sudo docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE opensuse latest 758c78b4040c 3 weeks ago 622.7 MB opensuse 13.2 758c78b4040c 3 weeks ago 622.7 MB opensuse harlequin 758c78b4040c 3 weeks ago 622.7 MB $ sudo docker run -i -t opensuse /bin/bash :/ # yzpper in vim  After installed commit the changes and verify:</description>
    </item>
    
    <item>
      <title>Using DO For Building OpenContrail</title>
      <link>http://purplepalmdash.github.io/2015/01/26/using-do-for-building-opencontrail/</link>
      <pubDate>Mon, 26 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/01/26/using-do-for-building-opencontrail/</guid>
      <description>Since the network environment is not good(In fact, very bad), because of the GFW, so I have to build the opencontrail packages in digitalOcean. Following is the tips and how-to.
Preparation First we have to get the Latest Ubuntu images, and let it OK for building the OpenContrail:
$ docker search ubuntu ..... $ docker pull ubuntu $ docker run -i -t ubuntu /bin/bash root@4c74f2890dbe:/# apt-get update  Now we run into the ubuntu build environment.</description>
    </item>
    
    <item>
      <title>把玩Panamax</title>
      <link>http://purplepalmdash.github.io/2014/12/11/ba-wan-panamax/</link>
      <pubDate>Thu, 11 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2014/12/11/ba-wan-panamax/</guid>
      <description>前提条件 在MAC上把玩Panamax前，需要安装Virtualbox, Vagrant, 而后, 用下列命令安装Panamax:
$ brew install http://download.panamax.io/installer/brew/panamax.rb $ panamax init  这将开始下载CoreOS镜像，需要等一段时间。
In fact the panamax could also be installed on ArchLinux rather than only in Ubuntu, simply run:
$ curl http://download.panamax.io/installer/ubuntu.sh | bash  Trouble Shooting Init failed $ panamix init A different VM with name panamax-vm has been created already. Please re-install or delete panamax-vm VM and try again.  Use following command for listing all of the virtualmachines:</description>
    </item>
    
    <item>
      <title>Use Docker for deploying WP</title>
      <link>http://purplepalmdash.github.io/2014/12/03/use-docker-for-deploying-wp/</link>
      <pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2014/12/03/use-docker-for-deploying-wp/</guid>
      <description>Just for swiftly deploy WP and test the RESTful API, I did following operations and runs a WP temporately.
PULL Pull following containers:
$ docker pull mysql $ docker pull wordpress $ sudo docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; 480ac552cd39 About an hour ago 192.8 MB mysql latest 98840bbb442c 39 hours ago 235.5 MB wordpress latest 9f51af77fd96 8 days ago 470.5 MB  Configuration Explanation for following commands, --name is the name for our container, -p 8038:80 is mapping the host machine&amp;rsquo;s 8038 port to container wordpress_1:</description>
    </item>
    
    <item>
      <title>LXC ArchLinux Tutorial</title>
      <link>http://purplepalmdash.github.io/2014/08/10/lxc-archlinux-tutorial/</link>
      <pubDate>Sun, 10 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2014/08/10/lxc-archlinux-tutorial/</guid>
      <description>Installation Install lxc, bridge-utils, netctl from the official repository.
$ sudo pacman -S lxc bridge-utils netctl  For creating ArchLinux Container, install arch-install-scripts:
$ sudo pacman -S arch-install-scripts  Check Configurations:
$ lxc-checkconfig  Network Configuration Since I use systemd for bridged network, so this step remains blank, the official Arch Wiki use netctl.
Creating Container Using Template List all of the available templates:
$ ls /usr/share/lxc/templates lxc-alpine lxc-centos lxc-fedora lxc-oracle lxc-ubuntu-cloud lxc-altlinux lxc-cirros lxc-gentoo lxc-plamo lxc-archlinux lxc-debian lxc-openmandriva lxc-sshd lxc-busybox lxc-download lxc-opensuse lxc-ubuntu  Now using the template for creating the Linux Container:</description>
    </item>
    
  </channel>
</rss>