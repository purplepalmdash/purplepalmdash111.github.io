<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Storage on Dash</title>
    <link>http://purplepalmdash.github.io/categories/storage/</link>
    <description>Recent content in Storage on Dash</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Apr 2015 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://purplepalmdash.github.io/categories/storage/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Glusterfs Howto</title>
      <link>http://purplepalmdash.github.io/2015/04/17/glusterfs-howto/</link>
      <pubDate>Fri, 17 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://purplepalmdash.github.io/2015/04/17/glusterfs-howto/</guid>
      <description>

&lt;p&gt;I want to expand my storage size on DigitalOcean, the droplet I have on DO one have 11G, and the other have 15G size, so if I could combine them together, I could do much more development on it. Following is how-to.&lt;/p&gt;

&lt;h3 id=&#34;glusterfs-setup&#34;&gt;Glusterfs Setup&lt;/h3&gt;

&lt;p&gt;Install it under Ubuntu via:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# apt-get install glusterfs-server

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In both node, install the same software, and then add following lines into your /etc/hosts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10.17.17.195    Gluster2
10.17.17.194    Gluster1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In Gluster1, probe Gluster2:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:~# gluster peer probe Gluster2
peer probe: success

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then view the gluster peer status:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:~# gluster peer status
Number of Peers: 1

Hostname: Gluster2
Port: 24007
Uuid: 881dedb8-6cd4-4127-8c96-223daef081f5
State: Peer in Cluster (Connected)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the volumn via:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:~# gluster volume create vol_replica transport tcp Gluster2:/home/glustervms Gluster1:/home/glustervms force
volume create: vol_replica: success: please start the volume to access data

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start the created vol:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:~# gluster volume start vol_replica
volume start: vol_replica: success

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View volumn info:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:~# gluster volume info 
 
Volume Name: vol_replica
Type: Distribute
Volume ID: 953456f3-0c46-4d07-ac41-591d1e398be6
Status: Started
Number of Bricks: 2
Transport-type: tcp
Bricks:
Brick1: Gluster2:/home/glustervms
Brick2: Gluster1:/home/glustervms

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create the folder and mount the glusterfs via:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home# mkdir glustervmsmnt
root@Gluster1:/home# mount -t glusterfs Gluster1:/vol_replica /home/glustervmsmnt/

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View the disk filesystem info:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Gluster1:/vol_replica   39G  4.7G   32G  13% /home/glustervmsmnt

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;glusterfs-volumn-deletion&#34;&gt;Glusterfs Volumn deletion&lt;/h3&gt;

&lt;p&gt;The replica is not the one we want, for combine two partitions, I need the Distributed stripped mode, which is the one described in:&lt;br /&gt;
&lt;a href=&#34;http://www.gluster.org/community/documentation/index.php/Gluster_3.2:_Creating_Distributed_Striped_Volumes&#34;&gt;http://www.gluster.org/community/documentation/index.php/Gluster_3.2:_Creating_Distributed_Striped_Volumes&lt;/a&gt;&lt;br /&gt;
So first I have to delete the one I&amp;rsquo;ve created in the above part.&lt;br /&gt;
First umount the one I&amp;rsquo;ve created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home/glustervms# umount /home/glustervmsmnt 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;checked via &lt;code&gt;mount&lt;/code&gt; or &lt;code&gt;df -h&lt;/code&gt; we could see the one we have mounted has been umounted.&lt;/p&gt;

&lt;p&gt;Second, stop the volumes we&amp;rsquo;ve created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home/glustervms# gluster volume stop vol_replica
Stopping volume will make its data inaccessible. Do you want to continue? (y/n) y
volume stop: vol_replica: success

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Third, delete volumn:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home/glustervms# gluster volume delete vol_replica
Deleting volume will erase all information about the volume. Do you want to continue? (y/n) y
volume delete: vol_replica: success

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the volume status:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home/glustervms# gluster volume info
No volumes present

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-the-distributed-stripped-volume&#34;&gt;Create the distributed stripped volume&lt;/h3&gt;

&lt;p&gt;Create the bigvolume:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home/glustervms# gluster volume create bigvolume transport tcp Gluster2:/home/glustervms Gluster1:/home/glustervms force

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start the volume:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home/glustervms# gluster volume start bigvolume
volume start: bigvolume: success

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View the status of the volume:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home/glustervmsmnt# gluster volume info 
 
Volume Name: bigvolume
Type: Distribute
Volume ID: 3e09f074-4675-46d3-873f-f00ef13fb509
Status: Started
Number of Bricks: 2
Transport-type: tcp
Bricks:
Brick1: Gluster2:/home/glustervms
Brick2: Gluster1:/home/glustervms

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mount it via following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mount -t glusterfs Gluster1:/bigvolume /home/glustervmsmnt/

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;trouble-shooting&#34;&gt;Trouble-Shooting&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;root@Gluster1:/home/glustervms# gluster volume create bigvolume transport tcp Gluster2:/home/glustervms Gluster1:/home/glustervms force
volume create: bigvolume: failed: /home/glustervms or a prefix of it is already part of a volume

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Resolve it via:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#  apt-get install attr
#  setfattr -x trusted.glusterfs.volume-id /home/glustervms
#  setfattr -x trusted.gfid /home/glustervms
#  rm -rf /home/glustervms/.glusterfs/

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Re-run the gluster volome create command it will create the volume which combines two folders.&lt;/p&gt;

&lt;h3 id=&#34;digital-ocean-scenario&#34;&gt;Digital Ocean Scenario&lt;/h3&gt;

&lt;p&gt;My DO droplet runs Ubuntu and CentOS, their version is Trusty(14.04) and CentOS7, so do following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CentOS # wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-epel.repo
CentOS # yum -y install glusterfs glusterfs-fuse glusterfs-server
CentOS # systemctl start glusterd
CentOS # systemctl enable glusterd
Trusty # apt-get install glusterfs-server

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add each other&amp;rsquo;s name and ip address into /etc/hosts, make sure they could ping each other and get responsible:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1xx.xx.xxx.xxx	CentOS
1xx.xx.xxx.xx	Trusty

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use Trusty as the server, so on the Trusty machine, detect the CentOS&amp;rsquo;s glusterd configuration as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Trusty # gluster peer probe CentOS
peer probe: success

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the status:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Trusty # gluster peer status
Number of Peers: 1

Hostname: CentOS
Port: 24007
Uuid: xxxxxxxx
State: Peer in Cluster (Connected)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the bigvolume, and mount it into your own directory via:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Trusty # gluster volume create bigvolume transport tcp CentOS:/home/glustervms Trusty:/home/glustervms force
volume create: bigvolume: success: please start the volume to access data
Trusty # gluster volume start bigvolume
volume start: bigvolume: success
Trusty # gluster volume info
 
Volume Name: bigvolume
Type: Distribute
Volume ID: xxxxxxxxxxxxxxxxxxxxx
Status: Started
Number of Bricks: 2
Transport-type: tcp
Bricks:
Brick1: CentOS:/home/glustervms
Brick2: Trusty:/home/glustervms
Trusty # mkdir /home/glustervmsmnt/
Trusty # mount -t glusterfs Trusty:/bigvolume /home/glustervmsmnt/
Trusty # df -h
Filesystem              Size  Used Avail Use% Mounted on
/dev/vda1                20G  9.4G  9.2G  51% /
none                    4.0K     0  4.0K   0% /sys/fs/cgroup
udev                    235M  8.0K  235M   1% /dev
tmpfs                    50M  396K   49M   1% /run
none                    5.0M     0  5.0M   0% /run/lock
none                    246M  1.1M  244M   1% /run/shm
none                    100M     0  100M   0% /run/user
Trusty:/bigvolume       40G   14G   24G  37% /home/glustervmsmnt

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you could operate under the /home/glustervmsmnt and you have 24G size partion of the disk. Enjoy them!!!&lt;/p&gt;

&lt;h3 id=&#34;trouble-shooting-1&#34;&gt;Trouble-Shooting 1&lt;/h3&gt;

&lt;p&gt;If you met &lt;code&gt;File change as we read it&lt;/code&gt; in tar something, do following things:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Trusty # gluster volume set bigvolume performance.stat-prefetch off
volume set: success

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>