<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2015-05-25T00:26:51+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[三节点搭建OpenStack Juno(2)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/24/san-jie-dian-da-jian-openstack-juno-2/"/>
    <updated>2015-05-24T22:37:28+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/24/san-jie-dian-da-jian-openstack-juno-2</id>
    <content type="html"><![CDATA[<h3>MySQL数据库</h3>

<p>绝大多数的OpenStack服务使用SQL数据库来存储信息，一般情况下数据库运行在控制节点上，这里我们使用MariaDB或者MySQL来作为SQL数据库。</p>

<p>安装, 注意安装过程中需要输入密码:</p>

<pre><code># apt-get install mariadb-server python-mysqldb
</code></pre>

<p>配置, 主要是更改了bind的地址，添加了一些有用选项，并支持UTF-8编码:</p>

<pre><code>$ sudo vim /etc/mysql/my.cnf
[mysqld]
...
bind-address = 10.55.55.2
...
default-storage-engine = innodb
innodb_file_per_table
collation-server = utf8_general_ci
init-connect = 'SET NAMES utf8'
character-set-server = utf8
</code></pre>

<p>完成安装，包括重启服务及加密数据库服务:</p>

<pre><code># service mysql restart
# mysql_secure_installation 
</code></pre>

<h3>消息服务器</h3>

<p>OpenStack使用message broker用来在各种服务器之间调度操作和协调状态信息，通常情况下消息服务器也运行在控制节点上，OpenStack支持RabbitMQ, Qpid和ZeroMQ, 这里使用RabbitMQ.</p>

<p>安装:</p>

<pre><code># apt-get install rabbitmq-server
</code></pre>

<p>配置，首先我们需要设定rabbitMQ使用的密码:</p>

<pre><code># rabbitmqctl change_password guest RABBIT_PASS
Changing password for user "guest" ...
...done.
</code></pre>

<p>如果是RabbitMQ 3.3.0或者更新的版本，则需要激活guest用户的远程访问权限。</p>

<p>检查RabbitMQ版本:</p>

<pre><code># rabbitmqctl status | grep rabbit
Status of node rabbit@Controller ...
 {running_applications,[{rabbit,"RabbitMQ","3.2.4"},
</code></pre>

<p>这里我们的版本是3.2.4所以不需要做任何修改，直接重启RabbitMQ服务即可。若是3.3.0以后的版本，则需要参考官方文档作更为详细的配置。</p>

<pre><code># service rabbitmq-server restart
</code></pre>

<h3>鉴权(Identity)服务</h3>

<p>鉴权服务的作用主要有:   <br/>
1. 跟踪用户及其权限。   <br/>
2. 提供可用服务的服务类别及API endpoint.</p>

<p>详细的关于Identity的介绍可以参见OpenStack官方文档。只有理解了其理念后才能明了OpenStack架构中各种服务的角色和地位.</p>

<p>首先创建keystone所需要的数据库:</p>

<pre><code># mysql -u root -p
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 36
Server version: 5.5.43-MariaDB-1ubuntu0.14.04.2 (Ubuntu)

Copyright (c) 2000, 2015, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE keystone;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
    -&gt; IDENTIFIED BY 'KEYSTONE_PASSWD';
Query OK, 0 rows affected (0.01 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
    -&gt; IDENTIFIED BY 'KEYSTONE_PASSWD';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; quit;
Bye
</code></pre>

<p>创建一个随机值，用于管理token在初始化配置时使用:</p>

<pre><code># openssl rand -hex 10
760bc221f4dc966693e5
</code></pre>

<p>安装和配置组件:</p>

<pre><code># apt-get install keystone python-keystoneclient
</code></pre>

<p>配置, 更改<code>admin_token</code>为刚才生成的随机数:  <br/>
<code>
$ sudo vim /etc/keystone/keystone.conf
[DEFAULT]
...
admin_token = 760bc221f4dc966693e5
...
[database]
...
connection = mysql://keystone:KEYSTONE_DBPASS@Controller/keystone
...
[token]
...
provider = keystone.token.providers.uuid.Provider
driver = keystone.token.persistence.backends.sql.Token
...
[revoke]
...
driver = keystone.contrib.revoke.backends.sql.Revoke
...
[DEFAULT]
...
verbose = True
</code></p>

<p>修改完毕后，使用以下命令来同步Identity服务数据库:</p>

<pre><code># su -s /bin/sh -c "keystone-manage db_sync" keystone
</code></pre>

<p>重启鉴权服务，删除Ubuntu使用的默认sqlite数据库, 并完成安装:</p>

<pre><code># service keystone restart
# rm -f /var/lib/keystone/keystone.db 
</code></pre>

<p>使用下列命令来激活cron任务，以便每小时判断tokens的存活时间:</p>

<pre><code># (crontab -l -u keystone 2&gt;&amp;1 | grep -q token_flush) || echo '@hourly /usr/bin/keystone-manage token_flush &gt;/var/log/keystone/keystone-tokenflush.log 2&gt;&amp;1' &gt;&gt; /var/spool/cron/crontabs/keystone
</code></pre>

<h4>创建tenants, users, roles</h4>

<pre><code># export OS_SERVICE_TOKEN=760bc221f4dc966693e5
# export OS_SERVICE_ENDPOINT=http://Controller:35357/v2.0
# keystone tenant-create --name admin --description "Admin Tenant"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |           Admin Tenant           |
|   enabled   |               True               |
|      id     | 6f5f440aa9de4b2fa205f43df073ddfa |
|     name    |              admin               |
+-------------+----------------------------------+
# keystone user-create --name admin --pass XXXXXXXXX --email xxxxxxxx@gmail.com
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |        XXXXXXXX@gmail.com        |
| enabled  |               True               |
|    id    | 7bc9be5493e345518a384383872ab274 |
|   name   |              admin               |
| username |              admin               |
+----------+----------------------------------+
# keystone role-create --name admin
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|    id    | 65b6ccaa3b434c848ccb757be43d6b41 |
|   name   |              admin               |
+----------+----------------------------------+
# keystone user-role-add --user admin --tenant admin --role admin
# keystone tenant-create --name demo --description "Demo Tenant"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |           Demo Tenant            |
|   enabled   |               True               |
|      id     | 459c25933274483fb01ce66d9514add6 |
|     name    |               demo               |
+-------------+----------------------------------+
# keystone user-create --name demo --tenant demo --pass xxxxx --email xxxxxxx@gmail.com
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |        xxxxxxx@gmail.com        |
| enabled  |               True               |
|    id    | b2f3d8a239b34edfb50fa67c5aca8f83 |
|   name   |               demo               |
| tenantId | 459c25933274483fb01ce66d9514add6 |
| username |               demo               |
+----------+----------------------------------+
# keystone tenant-create --name service --description "Service Tenant"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |          Service Tenant          |
|   enabled   |               True               |
|      id     | 08a675be93a04cca8a74159a3eefa288 |
|     name    |             service              |
+-------------+----------------------------------+
# keystone service-create --name keystone --type identity --description "OpenStack Identity"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |        OpenStack Identity        |
|   enabled   |               True               |
|      id     | bf7613d9563c47a9af80ecdb4f26f3f5 |
|     name    |             keystone             |
|     type    |             identity             |
+-------------+----------------------------------+
# keystone endpoint-create --service-id $(keystone service-list | awk '/ identity / {print $2}') --publicurl http://Controller:5000/v2.0 --internalurl http://Controller:5000/v2.0 --adminurl http://Controller:35357/v2.0 --region regionOne
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
|   adminurl  |   http://Controller:35357/v2.0   |
|      id     | c2c7a6c24b1d411b996f2e30fefc70b6 |
| internalurl |   http://Controller:5000/v2.0    |
|  publicurl  |   http://Controller:5000/v2.0    |
|    region   |            regionOne             |
|  service_id | bf7613d9563c47a9af80ecdb4f26f3f5 |
+-------------+----------------------------------+
</code></pre>

<p>验证, 详细的说明参见OpenStack官方文档:</p>

<pre><code># unset OS_SERVICE_TOKEN OS_SERVICE_ENDPOINT
# keystone --os-tenant-name admin --os-username admin --os-password xxxxx --os-auth-url http://Controller:35357/v2.0 token-get
+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
|  expires  |       2015-05-24T16:43:08Z       |
|     id    | 612b529c9c754b87a153abd39284aff6 |
| tenant_id | 6f5f440aa9de4b2fa205f43df073ddfa |
|  user_id  | 7bc9be5493e345518a384383872ab274 |
+-----------+----------------------------------+
# keystone --os-tenant-name admin --os-username admin --os-password xxxxx --os-auth-url http://Controller:35357/v2.0 tenant-list
+----------------------------------+---------+---------+
|                id                |   name  | enabled |
+----------------------------------+---------+---------+
| 6f5f440aa9de4b2fa205f43df073ddfa |  admin  |   True  |
| 459c25933274483fb01ce66d9514add6 |   demo  |   True  |
| 08a675be93a04cca8a74159a3eefa288 | service |   True  |
+----------------------------------+---------+---------+
# keystone --os-tenant-name admin --os-username admin --os-password xxxxx --os-auth-url http://Controller:35357/v2.0 user-list
+----------------------------------+-------+---------+--------------------+
|                id                |  name | enabled |       email        |
+----------------------------------+-------+---------+--------------------+
| 7bc9be5493e345518a384383872ab274 | admin |   True  | xxxxxxx@gmail.com |
| b2f3d8a239b34edfb50fa67c5aca8f83 |  demo |   True  | xxxxxxx@gmail.com |
+----------------------------------+-------+---------+--------------------+
# keystone --os-tenant-name admin --os-username admin --os-password xxxxx --os-auth-url http://Controller:35357/v2.0 role-list
+----------------------------------+----------+
|                id                |   name   |
+----------------------------------+----------+
| 9fe2ff9ee4384b1894a90878d3e92bab | _member_ |
| 65b6ccaa3b434c848ccb757be43d6b41 |  admin   |
+----------------------------------+----------+
# keystone --os-tenant-name demo --os-username demo --os-password xxxxx --os-auth-url http://controller:35357/v2.0 token-get
+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
|  expires  |       2015-05-24T16:46:34Z       |
|     id    | 0d8a9472b0f547dfabc62594b4fb146f |
| tenant_id | 459c25933274483fb01ce66d9514add6 |
|  user_id  | b2f3d8a239b34edfb50fa67c5aca8f83 |
+-----------+----------------------------------+
# keystone --os-tenant-name demo --os-username demo --os-password xxxxx --os-auth-url http://controller:35357/v2.0 user-list
You are not authorized to perform the requested action: admin_required (HTTP 403)
</code></pre>

<h3>创建脚本</h3>

<pre><code># cat admin-openrc.sh 
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=xxxxx
export OS_AUTH_URL=http://Controller:35357/v2.0
# cat demo-openrc.sh
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=xxxxx
export OS_AUTH_URL=http://Controller:5000/v2.0
</code></pre>

<p>下次使用时直接用<code>source admin-openrc.sh</code>或者<code>source demo-openrc.sh</code>即可。</p>

<h3>镜像服务</h3>

<p>添加镜像服务:</p>

<pre><code>root@Controller:~# mysql -u root -p
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 40
Server version: 5.5.43-MariaDB-1ubuntu0.14.04.2 (Ubuntu)

Copyright (c) 2000, 2015, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE glance;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'xxxxx';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'xxxxx';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; quit;
Bye
</code></pre>

<p>创建glance用户:</p>

<pre><code># source  /home/dash/admin-openrc.sh
# keystone user-create --name glance --pass xxxxx
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |                                  |
| enabled  |               True               |
|    id    | a3108e4267154acd809f3978d360e6cd |
|   name   |              glance              |
| username |              glance              |
+----------+----------------------------------+
</code></pre>

<p>赋予glance用户admin权限:</p>

<pre><code># keystone user-role-add --user glance --tenant service --role admin
</code></pre>

<p>创建service entity和service end-point:</p>

<pre><code> keystone service-create --name glance --type image --description "OpenStack Image Service"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |     OpenStack Image Service      |
|   enabled   |               True               |
|      id     | 8736ca50fdf741afb5fcc2d078b1cd9b |
|     name    |              glance              |
|     type    |              image               |
+-------------+----------------------------------+
# keystone endpoint-create --service-id $(keystone service-list | awk '/ image / {print $2}') --publicurl http://Controller:9292 --internalurl http://Controller:9292 --adminurl http://Controller:9292 --region regionOne
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
|   adminurl  |      http://Controller:9292      |
|      id     | 340f40f0558c4a5b8fa88089aee69767 |
| internalurl |      http://Controller:9292      |
|  publicurl  |      http://Controller:9292      |
|    region   |            regionOne             |
|  service_id | 8736ca50fdf741afb5fcc2d078b1cd9b |
+-------------+----------------------------------+
</code></pre>

<p>安装服务组件:</p>

<pre><code># apt-get install glance python-glanceclient
</code></pre>

<p>配置：</p>

<pre><code># vim /etc/glance/glance-api.conf
[database]
...
connection = mysql://glance:GLANCE_DBPASS@controller/glance
[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = glance
admin_password = GLANCE_PASS
[paste_deploy]
...
flavor = keystone
[glance_store]
...
default_store = file
filesystem_store_datadir = /var/lib/glance/images/
[DEFAULT]
...
notification_driver = noop
[DEFAULT]
...
verbose = True
</code></pre>

<p>配置<code>/etc/glance/glance-registry.conf</code>文件，完成以下配置:</p>

<pre><code>[database]
...
connection = mysql://glance:GLANCE_DBPASS@controller/glance

[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = glance
admin_password = GLANCE_PASS
[paste_deploy]
...
flavor = keystone
[DEFAULT]
...
notification_driver = noop
[DEFAULT]
...
notification_driver = noop
</code></pre>

<p>同步数据库:</p>

<pre><code># su -s /bin/sh -c "glance-manage db_sync" glance
</code></pre>

<p>重启服务，删除默认的sqlite数据库:</p>

<pre><code># service glance-registry restart
# service glance-api restart
# rm -f /var/lib/glance/glance.sqlite
</code></pre>

<p>验证:</p>

<pre><code># wget http://download.cirros-cloud.net/0.3.3/cirros-0.3.3-x86_64-disk.img
# source ~/admin-openrc.sh
# glance image-create --name "cirros-0.3.3-x86_64" --file ~/cirros-0.3.3-x86_64-disk.img --disk-format qcow2 --container-format bare --is-public True --progress
[=============================&gt;] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 133eae9fb1c98f45894a4e60d8736619     |
| container_format | bare                                 |
| created_at       | 2015-05-24T16:25:32                  |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 3d45ea58-731c-4eb5-bf30-db1b4bfe4f57 |
| is_public        | True                                 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros-0.3.3-x86_64                  |
| owner            | 6f5f440aa9de4b2fa205f43df073ddfa     |
| protected        | False                                |
| size             | 13200896                             |
| status           | active                               |
| updated_at       | 2015-05-24T16:25:32                  |
| virtual_size     | None                                 |
+------------------+--------------------------------------+
# glance image-list
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
| ID                                   | Name                | Disk Format | Container Format | Size     | Status |
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
| 3d45ea58-731c-4eb5-bf30-db1b4bfe4f57 | cirros-0.3.3-x86_64 | qcow2       | bare             | 13200896 | active |
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
</code></pre>

<p>控制节点基本上配置成功，明天继续。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三节点搭建OpenStack Juno(1)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/24/san-jie-dian-da-jian-openstack-juno/"/>
    <updated>2015-05-24T14:36:34+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/24/san-jie-dian-da-jian-openstack-juno</id>
    <content type="html"><![CDATA[<h3>目的</h3>

<p>最近在研究解耦OpenStack，以及OpenStack的各种网络模型，下面是一个最简单的用于搭建OpenStack Juno的过程。</p>

<h3>硬件及网络准备</h3>

<h4>物理服务器</h4>

<p>物理服务器:  i5-4460/32G 内存，128G SSD+3T IDE，事实上这个教程跑完你也用不到这么强悍的配置，理论上在8G的物理机器上就可以运行完本文。  <br/>
物理服务器操作系统: Ubuntu14.04</p>

<h4>虚拟机：</h4>

<p>虚拟机1, Controller: 1 processor, 2 GB memory, and 5 GB storage.  <br/>
虚拟机2, Network: 1 processor, 512 MB memory, and 5 GB storage.  <br/>
虚拟机3, Compute: 1 processor, 2 GB memory, and 10 GB storage.</p>

<h4>网络规划</h4>

<p>Management: 10.55.55.0/24, 只用于管理的网络，公网无法访问。简单来说，这个网络用于OpenStack各个组件之间的相互通信。      <br/>
Tunnel: 10.66.66.0/24, 用于计算节点和网络节点之间的通信。这个隧道使得虚拟机的实例可以和相互通信。  <br/>
External: 192.168.1.0/24, 用于虚拟机实例的Internet访问。 <br/>
当然我们可以添加额外的存储网络，这里为了简单起见我们不使用cinder服务，使用单纯的虚拟机镜像即可。</p>

<h4>节点网络名规划</h4>

<p>Controller节点:    controller.openstack.local, 10.55.55.2(管理网络), N/A, N/A.  <br/>
Network节点:    Network.openstack.local, 10.55.55.3(管理网络), 10.66.66.3(隧道网络), 192.168.1.3(Internet公网).
Compute节点:    Compute.openstack.local, 10.55.55.4(管理网络), 10.66.66.4(隧道网络).</p>

<p>一个参考的例图如下:  <br/>
<img src="/images/2015_05_24_14_50_37_629x551.jpg" alt="/images/2015_05_24_14_50_37_629x551.jpg" /></p>

<p>按照上述的描述我们创建三台虚拟机，并进行初始化配置。</p>

<h3>虚拟机初始化配置</h3>

<p>下面罗列的代码是我在本机上的创建过程，仅供参考:</p>

<pre><code>$ pwd
/media/repo/Image/3NodeOpenStack
$ qemu-img create -f qcow2 -b /media/repo/Image/UbuntuBase.qcow2 OpenStackController.qcow2
Formatting 'OpenStackController.qcow2', fmt=qcow2 size=107374182400 backing_file='/media/repo/Image/UbuntuBase.qcow2' encryption=off cluster_size=65536 lazy_refcounts=off 
$ qemu-img create -f qcow2 -b /media/repo/Image/UbuntuBase.qcow2 OpenStackNetwork.qcow2
Formatting 'OpenStackNetwork.qcow2', fmt=qcow2 size=107374182400 backing_file='/media/repo/Image/UbuntuBase.qcow2' encryption=off cluster_size=65536 lazy_refcounts=off 
$ qemu-img create -f qcow2 -b /media/repo/Image/UbuntuBase.qcow2 OpenStackCompute.qcow2
Formatting 'OpenStackCompute.qcow2', fmt=qcow2 size=107374182400 backing_file='/media/repo/Image/UbuntuBase.qcow2' encryption=off cluster_size=65536 lazy_refcounts=off 
</code></pre>

<p>创建虚拟机的时候， OpenStackCompute节点需要把CPU的参数带下去，如下图所示:  <br/>
<img src="/images/2015_05_24_15_00_07_517x483.jpg" alt="/images/2015_05_24_15_00_07_517x483.jpg" /></p>

<p>各个节点的network定义文件如下:</p>

<p>控制节点:</p>

<pre><code># The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 10.55.55.2
netmask 255.255.255.0
gateway 10.55.55.1
dns-nameservers 114.114.114.114
</code></pre>

<p>网络节点:</p>

<pre><code># The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 10.55.55.3
netmask 255.255.255.0
gateway 10.55.55.1
dns-nameservers 114.114.114.114


auto eth1
iface eth1 inet static
address 10.66.66.3
netmask 255.255.255.0

auto eth2
iface eth2 inet static
address 192.168.1.3
netmask 255.255.255.0
</code></pre>

<p>计算节点:</p>

<pre><code>auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 10.55.55.4
netmask 255.255.255.0
gateway 10.55.55.1
dns-nameservers 114.114.114.114

auto eth1
iface eth1 inet static
address 10.66.66.4
netmask 255.255.255.0
</code></pre>

<p>每个节点分别更改其<code>/etc/hostname</code>为对应的名字，而每台机器上的<code>/etc/hosts</code>也做对应的修改，例如Controlle节点上的例子如下:   <br/>
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat /etc/hosts
</span><span class='line'>127.0.0.1       localhost
</span><span class='line'>127.0.1.1       Controller
</span><span class='line'>10.55.55.2      Controller
</span><span class='line'>10.55.55.3      Network
</span><span class='line'>10.55.55.4      Compute
</span><span class='line'>&hellip;&hellip;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;$ cat /etc/hostname
</span><span class='line'>Controller&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;网络配置完毕后，保证可以通过`ping Controller`等命令达到互通。    
</span><span class='line'>
</span><span class='line'>在进入到后续步骤前，更新所有节点到最新状态:     
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;$ sudo apt-get update &amp;&amp; sudo apt-get upgrade &amp;&amp; sudo apt-get dist-upgrade &amp;&amp; sudo reboot</span></code></pre></td></tr></table></div></figure></p>

<h3>NTP 服务器/客户端配置</h3>

<p>使用NTP来保证各个节点之间的时间同步，对后续加入的各个节点，同样需要使用NTP来同步该节点时间。我们将Controller作为NTP服务器,在Controller上，安装和配置NTP服务器：</p>

<h4>NTP服务器</h4>

<pre><code># apt-get -y install ntp
# vim /etc/ntp.conf
    # 修改成大陆时间
    server 2.cn.pool.ntp.org
    server 1.asia.pool.ntp.org
    server 2.asia.pool.ntp.org
    # 修改 restrict 設定
    restrict -4 default kod notrap nomodify
    restrict -6 default kod notrap nomodify
# service ntp restart
</code></pre>

<h4>NTP客户端</h4>

<p>其他的节点上都需要安装NTP客户端并使用NTP服务器时间同步。</p>

<pre><code># apt-get -y install ntp
# vim /etc/ntp.conf
    # 設定 controller 為參照的 time server
    # 並將其他 server 開頭的設定進行註解
    server 10.55.55.2 iburst
# service ntp restart
</code></pre>

<p>检查结果是否正确:</p>

<pre><code>root@JunoNetwork:~# ntpq -c peers
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 Controller  59.106.180.168   3 u    1   64    1    0.239  447024.   0.049
</code></pre>

<p>接下来真正进入OpenStack的安装和配置过程。</p>

<h3>源设定</h3>

<p>Juno的源没有被包含在Ubuntu14.04的官方源中(官方源中版本为IceHouse)，所以通过下列命令添加OpenStack Juno源:   <br/>
所有节点上(Controller,Network,Compute):</p>

<pre><code>$ sudo apt-get install ubuntu-cloud-keyring
$ sudo bash
# echo "deb http://ubuntu-cloud.archive.canonical.com/ubuntu" "trusty-updates/juno main" &gt; /etc/apt/sources.list.d/cloudarchive-juno.list
# apt-get update &amp;&amp; apt-get -y dist-upgrade
</code></pre>

<p>第一部分就先到这里。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Change Cobbler Profile for Using Local Repository]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/22/change-cobbler-profile-for-using-local-repository/"/>
    <updated>2015-05-22T14:12:58+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/22/change-cobbler-profile-for-using-local-repository</id>
    <content type="html"><![CDATA[<h3>Cobbler Profiles</h3>

<p>For getting the profiles and get the detailed information of the profile.</p>

<pre><code># cobbler profile list
   ubuntu1404-x86_64
# cobbler profile help
usage
=====
cobbler profile add
cobbler profile copy
cobbler profile dumpvars
cobbler profile edit
cobbler profile find
cobbler profile getks
cobbler profile list
cobbler profile remove
cobbler profile rename
cobbler profile report
# cobbler profile report ubuntu1404-x86_64
......
Kickstart                      : /var/lib/cobbler/kickstarts/sample.seed
......
</code></pre>

<h3>Use Local Repository</h3>

<p>For adding the repository via following command, you could use your local repository:</p>

<pre><code>$ sudo cobbler repo add --name=local-trusty --breed=apt --arch=x86_64 --mirror=http://xxxxxxxxxxxxxx/ubuntu --apt-components=main,restricted,universe,multiverse --apt-dists=trusty,trusty-updates,trusty-security
$ sudo cobbler repo sync
</code></pre>

<p>After syncing, the folder will contains all of the packages:</p>

<pre><code># pwd
/var/www/cobbler/ks_mirror/ubuntu1404-x86_64
# ls
boot  dists  doc  EFI  install  isolinux  md5sum.txt  pics  pool  preseed  README.diskdefines  ubuntu
</code></pre>

<p>Edit the sample seed via:</p>

<pre><code># cp /var/lib/cobbler/kickstarts/sample.seed /var/lib/cobbler/kickstarts/local.seed
# vim /var/lib/cobbler/kickstarts/local.seed

    d-i mirror/http/hostname string $http_server
    # d-i mirror/http/directory string $install_source_directory
    d-i mirror/http/directory string /cobbler/ks_mirror/ubuntu1404-x86_64/ubuntu/
</code></pre>

<p>Then Modify the profile&rsquo;s kickstart via:</p>

<pre><code>#  cobbler profile edit --name=ubuntu1404-x86_64 --kickstart=/var/lib/cobbler/kickstarts/local.seed
</code></pre>

<p>After modification, next time if you re-install the compute, it will directly get the packages from the local repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My Configuration on Cobbler for Deploying Ubuntu12.04]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/18/my-configuration-on-cobbler-for-deploying-ubuntu12-dot-04/"/>
    <updated>2015-05-18T18:15:47+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/18/my-configuration-on-cobbler-for-deploying-ubuntu12-dot-04</id>
    <content type="html"><![CDATA[<p>Configuration file for preseed, put it under: /var/lib/cobbler/kickstarts/autoinstall.seed:</p>

<pre><code># BASIC
d-i  debian-installer/locale    string en_US.UTF-8
d-i  debian-installer/splash    boolean false
d-i  console-setup/ask_detect   boolean false
d-i  console-setup/layoutcode   string us
d-i  console-setup/variantcode  string
d-i  clock-setup/utc            boolean true
d-i  clock-setup/ntp            boolean true

# DISKPART
d-i  partman-auto/method                string regular
d-i  partman-lvm/device_remove_lvm      boolean true
d-i  partman-lvm/confirm                boolean true
d-i  partman/confirm_write_new_label    boolean true
d-i  partman/choose_partition           select Finish partitioning and write changes to disk
d-i  partman/confirm                    boolean true
d-i  partman/confirm_nooverwrite        boolean true
d-i  partman/default_filesystem         string ext3

# SOFTWARE
# /var/www/cobbler/ks_mirror/Ubuntu12.04-x86_64/ubuntu/
d-i  mirror/country             string manual
d-i  mirror/http/hostname       string $http_server
d-i  mirror/http/directory      string /cobbler/ks_mirror/Ubuntu12.04-x86_64/ubuntu
d-i  mirror/http/proxy          string
d-i  apt-setup/security_host    string $http_server
d-i  apt-setup/security_path    string /cobbler/ks_mirror/Ubuntu12.04-x86_64/ubuntu
d-i  apt-setup/services-select  multiselect none
d-i  pkgsel/upgrade             select none
d-i  pkgsel/language-packs      multiselect
d-i  pkgsel/update-policy       select none
d-i  pkgsel/updatedb            boolean true
d-i  pkgsel/include             string openssh-server

# USER
d-i  passwd/root-login                  boolean true
d-i  passwd/make-user                   boolean false
d-i  passwd/root-password               password root
d-i  passwd/root-password-again         password root
d-i  user-setup/allow-password-weak     boolean true

# FINISH
d-i  grub-installer/skip                boolean false
d-i  lilo-installer/skip                boolean false
d-i  grub-installer/only_debian         boolean true
d-i  grub-installer/with_other_os       boolean true
d-i  finish-install/keep-consoles       boolean false
d-i  finish-install/reboot_in_progress  note
d-i  cdrom-detect/eject                 boolean true
d-i  debian-installer/exit/halt         boolean false
d-i  debian-installer/exit/poweroff     boolean false

# EXTRA
d-i  preseed/late_command       string echo "UseDNS no" &gt;&gt; /target/etc/ssh/sshd_config
</code></pre>

<p>Edit the profile via:</p>

<pre><code>$ cobbler profile list
$ cobbler profile edit --name=Ubuntu12.04-x86_64 --kickstart=/var/lib/cobbler/kickstarts/autoinstall.seed 
</code></pre>

<p>Now select the installation, your installation will use local repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Fuel部署OpenContrail(6)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/06/shi-yong-fuelbu-shu-opencontrail-6/"/>
    <updated>2015-05-06T15:27:00+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/06/shi-yong-fuelbu-shu-opencontrail-6</id>
    <content type="html"><![CDATA[<p>前面在HA类型的Fuel OpenStack基础上集成了OpenContrail，然而在实际的开发和测试中，用HA类型比较浪费硬件资源，因此这次我把部署节点从7个压缩到3个，做多节点上非HA类型的OpenStack集成OpenContrail.</p>

<h3>先决条件</h3>

<p>这次只用三台机器来做部署，分别为:  <br/>
2-Core, 3G内存, 100G硬盘, 用于安装OpenStack Controller.      <br/>
2-Core, 2G内存, 100G硬盘, 用于安装OpenStack Compute. 注意这台机器需要Copy Host CPU configuration, 以激活KVM。      <br/>
2-Core, 3G内存, 100G硬盘, 用于安装Contrail.  <br/>
创建出来的两个用于部署的OpenStack环境如下:  <br/>
<img src="/images/2015_05_06_15_36_20_383x194.jpg" alt="/images/2015_05_06_15_36_20_383x194.jpg" /> <br/>
值得注意的是，在OpenStack的配置中，我们激活了Ceilometer，用于统计，所以需要额外增加一台2G内存大小的虚拟机。  <br/>
<img src="/images/2015_05_06_09_40_34_608x385.jpg" alt="/images/2015_05_06_09_40_34_608x385.jpg" /></p>

<h3>安装</h3>

<p>安装过程和HA的过程大同小异，配置好网络以后，现在I3OpenStack中部署好OpenStack，而后用provision的方式将I3Contrail中的Contrail部署节点机器安装为Ubuntu的格式。    <br/>
这里的具体配置过程可以参考《使用Fuel部署OpenContrail(1)》到《使用Fuel部署OpenContrail(3)》.  <br/>
一切就绪后，我们进入到配置过程.</p>

<h3>配置</h3>

<p>详细配置如下:</p>

<h4>(Contrail) 配置Contrail部署节点</h4>

<p>删除不用的网络端口, 并配置ifccfg-eth4后重启:</p>

<pre><code># cd /etc/network/interfaces.d/
# rm -f ifcfg-eth1 
# rm -f ifcfg-eth2 
# rm -f ifcfg-eth3
# vim ifcfg-eth4
    auto eth4
    iface eth4 inet static

    address 10.77.77.100
    netmask 255.255.255.0
    gateway 10.77.77.1

    post-up  ethtool  -K  eth4  gso off  gro off || true
# reboot
</code></pre>

<p>确保在Contrail部署节点上，可以ping通OpenStack Controller的10.55.55.0/24网络。   <br/>
配置本地安装源:</p>

<pre><code># echo 'deb http://10.20.0.2:8080/contrail/ /' &gt; /etc/apt/sources.list.d/contrail.list
# echo -e "Package: *\nPin: release l=Ubuntu\nPin-Priority: 100" &gt; /etc/apt/preferences
# &gt;/etc/apt/sources.list
# apt-get update
# apt-get install -y python-paramiko contrail-fabric-utils contrail-setup
# pip install --upgrade --no-deps --index-url=”” /opt/contrail/python_packages/Fabric-1.7.0.tar.gz
</code></pre>

<p>开始配置用于部署的testbed.py文件，可以看到，比起HA部署方式来看，我们减少了一些节点定义，去掉了HA有关的配置:</p>

<pre><code># vim  /opt/contrail/utils/fabfile/testbeds/testbed.py
    from fabric.api import env
    #Management ip addresses of hosts in the cluster
    #os_ctrl01 = 'root@10.55.55.6'
    #os_ctrl02 = 'root@10.55.55.7'
    #os_ctrl03 = 'root@10.55.55.8'
    os_ctrl01 = 'root@10.55.55.7'

    c_ctrl01 = 'root@10.77.77.100'
    #c_ctrl02 = 'root@10.77.77.11'
    #c_ctrl03 = 'root@10.77.77.12'
    c_db01 = 'root@10.77.77.100'
    #c_db02 = 'root@10.77.77.11'
    #c_db03 = 'root@10.77.77.12'
    #External routers
    # ext_routers = [('gateway01', '&lt;Gateway_node1_LOOPBACK_ip&gt;'), ('gateway02', '&lt;Gateway_node2_LOOPBACK_ip&gt;')]
    #Autonomous system number
    router_asn = 64512
    #Host from which the fab commands are triggered to install and provision
    deploy_node = 'root@10.77.77.100'
    #Role definition of the hosts.
    env.roledefs = {
    'all': [c_ctrl01, c_db01],
    'cfgm': [c_ctrl01],
    'openstack': [os_ctrl01],
    'control': [c_ctrl01],
    'compute': [],
    'collector': [c_ctrl01],
    'webui': [c_ctrl01],
    'database': [c_db01],
    'build': [deploy_node],
    'storage-master': [],
    'storage-compute': [],
    }
    #Openstack admin password
    env.openstack_admin_password = 'admin'
    env.password = 'r00tme'
    #Passwords of each host
    env.passwords = {
    os_ctrl01: 'r00tme',
    # os_ctrl02: 'r00tme',
    # os_ctrl03: 'r00tme',
    c_ctrl01: 'r00tme',
    #c_ctrl02: 'r00tme',
    #c_ctrl03: 'r00tme',
    c_db01: 'r00tme',
    # c_db02: 'r00tme',
    # c_db03: 'r00tme',
    deploy_node: 'r00tme',
    }
    #For reimage purpose
    env.ostypes = {
    os_ctrl01: 'ubuntu',
    # os_ctrl02: 'ubuntu',
    # os_ctrl03: 'ubuntu',
    c_ctrl01: 'ubuntu',
    # c_ctrl02: 'ubuntu',
    # c_ctrl03: 'ubuntu',
    c_db01: 'ubuntu',
    # c_db02: 'ubuntu',
    # c_db03: 'ubuntu',
    deploy_node: 'ubuntu',
    }
    env.openstack = {
    'service_token' : 'xqnCCCs2'
    }
    # env.ha = {
    # 'internal_vip': '10.55.55.4',
    # 'external_vip': '172.16.0.4',
    # 'contrail_internal_vip': '10.77.77.9',
    # 'contrail_external_vip': '10.77.77.9',
    # }
    env.keystone = {
    'service_tenant': 'services',
    'admin_token': 'xqnCCCs2',
    }
    multi_tenancy = True
</code></pre>

<p>从Fuel节点控制机上拷贝公钥文件，用于快速部署</p>

<pre><code># scp 10.20.0.2:/root/.ssh/id_rsa /root/.ssh/id_rsa
# chmod 0600 /root/.ssh/id_rsa
</code></pre>

<p>在节点上部署仓库，安装必要包，同意SUN协议:</p>

<pre><code># fab -P -R control -w -- 'ls /etc/apt/preferences || echo -e "Package: *\nPin: release \
l=Ubuntu\nPin-Priority: 100" &gt; /etc/apt/preferences'
# fab -P -R control -w -- 'DEBIAN_FRONTEND=noninteractive apt-get -y --force-yes \
--allow-unauthenticated install python-crypto python-netaddr python-paramiko \
contrail-fabric-utils contrail-setup'
# fab -P -R control -w -- 'pip install --upgrade --no-deps --index-url="" \
/opt/contrail/python_packages/ecdsa-0.10.tar.gz'
# fab -P -R control -w -- 'pip install --upgrade --no-deps --index-url="" \
/opt/contrail/python_packages/Fabric-1.7.0.tar.gz'
# fab -P -R control -w -- 'echo "sun-java6-plugin shared/accepted-sun-dlj-v1-1 boolean \
true" | /usr/bin/debconf-set-selections' &amp;&amp; fab -P -R control -w -- 'echo "sun-java6-bin shared/accepted-sun-dlj-v1-1 boolean \
 true" | /usr/bin/debconf-set-selections' &amp;&amp; fab -P -R control -w -- 'echo "debconf shared/accepted-oracle-license-v1-1 select \
true" | sudo debconf-set-selections' &amp;&amp; fab -P -R control -w -- 'echo "debconf shared/accepted-oracle-license-v1-1 seen \
 true" | sudo debconf-set-selections'
</code></pre>

<p>安装特定版本的tzdata， 安装和配置数据库，并检查状态：</p>

<pre><code># fab -P -R control -w -- 'DEBIAN_FRONTEND=noninteractive apt-get -y --force-yes \
 --allow-unauthenticated install tzdata=2014e-0ubuntu0.12.04' &amp;&amp; fab install_database &amp;&amp; fab setup_database &amp;&amp; fab -R database -w -- "contrail-status"
# nodetool status
</code></pre>

<p>安装和配置cfgm, control, collector, webui，keepalived等, 并配置tenant服务:</p>

<pre><code># fab install_cfgm &amp;&amp; fab install_control &amp;&amp; fab install_collector &amp;&amp; fab install_webui &amp;&amp; fab setup_contrail_keepalived
# fab -P -R control -w -- 'service keepalived restart'
# fab -P -R control -w -- "sed -i '49s/service/services/g' \
/usr/local/lib/python2.7/dist-packages/contrail_provisioning/config/quantum_in_keystone_setup.py"
# fab setup_cfgm
# fab setup_control &amp;&amp; fab setup_collector &amp;&amp; fab setup_webui
</code></pre>

<p>(OpenStack Controller节点)检查neutron endpoint的方法，看是否有10.77.77.100的字段出现：</p>

<pre><code># keystone service-list
# keystone endpoint-list
</code></pre>

<p>(OpenStack Controller节点)顺便，我们要拿到rabbit_hosts的密码，供下面使用:</p>

<pre><code># cat /etc/rabbitmq/rabbitmq.config | grep default_pass
    {default_pass,        &lt;&lt;"nFyBhsrP"&gt;&gt;},
</code></pre>

<p>配置rabbit:</p>

<pre><code># fab -P -R control -w -- 'openstack-config --del /etc/neutron/neutron.conf DEFAULT rabbit_host'
# fab -P -R control -w -- 'openstack-config --set /etc/neutron/neutron.conf DEFAULT rabbit_hosts 10.55.55.7:5672'
# fab -P -R control -w -- 'openstack-config --set /etc/neutron/neutron.conf DEFAULT rabbit_userid \
   nova'
# fab -P -R control -w -- 'openstack-config --set /etc/neutron/neutron.conf DEFAULT \
  rabbit_password nFyBhsrP'
# fab -P -R control -w -- 'service neutron-server restart'
</code></pre>

<p>配置contrail-api使用OpenStack Controller上的rabbit服务:</p>

<pre><code># fab -P -R control -w -- 'perl -pi -e \
 "s/rabbit_server.*$/rabbit_server=10.55.55.7/" /etc/contrail/contrail-api.conf'
# fab -P -R control -w -- 'perl -pi -e "s/rabbit_port.*$/rabbit_port=5672/" \
 /etc/contrail/contrail-api.conf'
# fab -R control -w -- "perl -pi -e 'print \"rabbit_password=nFyBhsrP\n\" \
 if \$_ =~ rabbit_port' /etc/contrail/contrail-api.conf"
# fab -P -R control -w -- "perl -pi -e 'print \"rabbit_user=nova\n\" if \$_ =~ rabbit_port' \
 /etc/contrail/contrail-api.conf"
# fab -P -R control -w -- "service contrail-api restart"
</code></pre>

<p>替换neutron的插件为OpenContrail：</p>

<pre><code># cp -r contrail-repo/neutron_plugin_contrail/plugins/opencontrail /usr/share/pyshared/neutron_plugin_contrail/plugins/
# cd /opt/contrail/utils
# fab -P -R cfgm -w -- 'service neutron-server restart'
</code></pre>

<p>重启BGP,METADATA,ENCAPSULATION:</p>

<pre><code># fab prov_control_bgp &amp;&amp; fab prov_metadata_services &amp;&amp; fab prov_encap_type
</code></pre>

<p>验证:</p>

<pre><code># fab verify_cfgm
# fab verify_control
# fab verify_collector
# fab verify_webui
# fab -R control -w -- "contrail-status"
# fab -P -R control -w -- 'update-rc.d supervisor-support-service disable'
</code></pre>

<p>现在访问:     <br/>
<a href="https://10.77.77.100:8143">https://10.77.77.100:8143</a>    <br/>
Contrail的组件已经被配置完毕，接下来配置Compute节点，以引入Vrouter等。</p>

<h4>(OpenStack Controller节点)</h4>

<p>删除ifcfg-eth4的配置后重启OpenStack Controller节点, 修改nova.conf文件:</p>

<pre><code># vim /etc/nova/nova.conf
[DEFAULT]
network_api_class = nova.network.neutronv2.api.API
neutron_url = http://10.77.77.100:9696
neutron_admin_tenant_name = services
neutron_admin_username = neutron
neutron_admin_password = xqnCCCs2
neutron_url_timeout = 300
neutron_admin_auth_url = http://10.55.55.7:35357/v2.0/
firewall_driver = nova.virt.firewall.NoopFirewallDriver
enabled_apis = ec2,osapi_compute,metadata
security_group_api = neutron
service_neutron_metadata_proxy = True
</code></pre>

<p>重启服务:</p>

<pre><code># service nova-api restart
# service nova-scheduler restart
# service nova-conductor restart
</code></pre>

<p>删除已注册的nova-network组件:</p>

<pre><code># source ~/openrc
# for i in $(nova service-list|grep nova-network|awk '{print $2}'); \
do nova service-delete $i;done
</code></pre>

<p>接下来配置Compute节点.</p>

<h4>(Compute节点)</h4>

<p>引入本地安装仓库:</p>

<pre><code>#  echo 'deb http://10.20.0.2:8080/contrail/ /' &gt;/etc/apt/sources.list.d/contrail.list
# echo -e "Package: *\nPin: release l=Ubuntu\nPin-Priority: 100" &gt; /etc/apt/preferences
# &gt;/etc/apt/sources.list
# apt-get update
</code></pre>

<p>删除已有的vswitch模块，并验证:</p>

<pre><code># apt-get purge -y openvswitch-switch nova-network nova-api
# apt-get purge -y  nova-network nova-api
# aptitude search -F '%p' '~i' | grep openvswitch
</code></pre>

<p>删除OVS内核模块:</p>

<pre><code># lsmod | grep openvswitch &amp;&amp; rmmod openvswitch
</code></pre>

<p>删除virtual网络,即virbr0端口:</p>

<pre><code># virsh net-destroy default
# virsh net-undefine default
</code></pre>

<p>删除除ifcfg-eth4和ifcfg-eth0的其他端口，并重启，重启后用下列命令检查是否有iptables NAT规则存在，理论上应该是空的:</p>

<pre><code># iptables -L -t nat
</code></pre>

<p>安装vrouter:</p>

<pre><code># apt-get install -y contrail-openstack-vrouter
</code></pre>

<p>配置vhosts,vrouter需要使用这个端口,指定IP地址为10.77.77.101:</p>

<pre><code># vim /etc/network/interfaces.d/ifcfg-vhost0 
auto vhost0
iface vhost0 inet static
    netmask 255.255.255.0
    network_name application
    address 10.77.77.101
    gateway 10.77.77.1
    mtu 1300
# vim /etc/network/interfaces.d/ifcfg-eth4 
auto eth4
iface eth4 inet manual

up ip l set eth4 up
down ip l set eth4 down

post-up  ethtool  -K  eth4  gso off  gro off || true
</code></pre>

<p>创建agent-param文件:</p>

<pre><code># mv /etc/contrail/agent_param.tmpl /etc/contrail/agent_param
# vim /etc/contrail/agent_param
dev=eth4
</code></pre>

<p>设置vroute-agent配置:</p>

<pre><code># vim /etc/contrail/contrail-vrouter-agent.conf
[DEFAULT]
headless_mode=true
[DISCOVERY]
server=10.77.77.100
max_control_nodes=1
[HYPERVISOR]
type=kvm
[NETWORKS]
control_network_ip=10.77.77.101
[VIRTUAL-HOST-INTERFACE]
name=vhost0
ip=10.77.77.101/24
gateway=10.77.77.1
physical_interface=eth4
</code></pre>

<p>配置节点管理参数,地址指向Contrail控制器的IP:</p>

<pre><code># vim /etc/contrail/vrouter_nodemgr_param
DISCOVERY=10.77.77.100
</code></pre>

<p>配置nova-compute:</p>

<pre><code> # openstack-config --set /etc/nova/nova.conf DEFAULT neutron_url http://10.77.77.100:9696
 # openstack-config --set /etc/nova/nova.conf DEFAULT neutron_admin_auth_url http://10.55.55.7:35357/v2.0/
 # openstack-config --set /etc/nova/nova.conf DEFAULT network_api_class nova_contrail_vif.contrailvif.ContrailNetworkAPI
 # openstack-config --set /etc/nova/nova.conf DEFAULT neutron_admin_tenant_name services
 # openstack-config --set /etc/nova/nova.conf DEFAULT neutron_admin_username neutron
 # openstack-config --set /etc/nova/nova.conf DEFAULT neutron_admin_password xqnCCCs2
 # openstack-config --set /etc/nova/nova.conf DEFAULT neutron_url_timeout 300
 # openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
 # openstack-config --set /etc/nova/nova.conf DEFAULT security_group_api neutron
 # service supervisor-vrouter restart
</code></pre>

<p>验证所有的vrouter服务都是active状态的:</p>

<pre><code># contrail-status 
== Contrail vRouter ==
supervisor-vrouter:           active
contrail-vrouter-agent        active              
contrail-vrouter-nodemgr      active              
</code></pre>

<p>更改/etc/libvirt/qemu.confg中的cgroup_device_acl部分:</p>

<pre><code>cgroup_device_acl = [
"/dev/null", "/dev/full", "/dev/zero",
"/dev/random", "/dev/urandom",
"/dev/ptmx", "/dev/kvm", "/dev/kqemu",
"/dev/rtc", "/dev/hpet","/dev/net/tun",
]
</code></pre>

<p>在每个OpenStack Compute节点上，添加iptables规则如下并保存:</p>

<pre><code># iptables -I INPUT 1 -s 169.254.0.0/16 -i vhost0 -j ACCEPT -m comment --comment "metadata service"
# iptables -I INPUT 1 -p tcp -m multiport --destination-ports 2049,8085,9090,8102,33617,39704,44177,55970,60663 -j ACCEPT -m comment --comment "juniper contrail rules"
# iptables-save &gt; /etc/iptables/rules.v4
</code></pre>

<p>重启libvirt-bin和nova-compute服务:</p>

<pre><code># service libvirt-bin restart
# service nova-compute restart
</code></pre>

<p>(Contrail Controller节点)更改vrouter的配置, ！！！注意，这是在Contrail Deploy的那个节点运行的！！！！, host_name的结果可以在compute节点上通过hostname命令来获得 ：</p>

<pre><code># python /opt/contrail/utils/provision_vrouter.py --host_name node-18 --host_ip 10.77.77.101 --api_server_ip 10.77.77.100 --admin_user neutron --admin_password xqnCCCs2 --admin_tenant_name services --oper add
</code></pre>

<h4>VGW配置</h4>

<p>OpenContrail支持多种配置，例如Juniper vSRX, Juniper MX, Cisco ASR等，但这些都需要专有硬件的支持（路由器），我们仅仅采用软件路由器Vrouter, 这里我们配置VGW:</p>

<pre><code># export PYTHONPATH=/usr/lib/python2.7/dist-packages/contrail_vrouter_api/gen_py/instance_service
# python /opt/contrail/utils/provision_vgw_interface.py --oper create --interface vgw --subnets 10.88.88.0/24 --routes 0.0.0.0/0 --vrf default-domain:admin:ext:ext
</code></pre>

<p>更新/etc/contrail/contrail-vrouter-agent.con中的[GATEWAY-0]部分:</p>

<pre><code>[GATEWAY-0]
routing_instance=default-domain:admin:ext:ext
interface=vgw
ip_blocks=10.88.88.0/24
routes=0.0.0.0/0
</code></pre>

<p>重新启动supervisor-vrouter进程:</p>

<pre><code># service supervisor-vrouter restart
</code></pre>

<p>重启其他所有的encapsulation方法，除了MPLS On UDP:  <br/>
<img src="/images/2015_04_27_22_45_01_799x306.jpg" alt="/images/2015_04_27_22_45_01_799x306.jpg" /></p>

<p>最后结果如下:   <br/>
<img src="/images/2015_05_06_16_24_52_891x430.jpg" alt="/images/2015_05_06_16_24_52_891x430.jpg" /></p>

<h3>总结</h3>

<p>非HA方式部署，需要花费内存为:   <br/>
3+3+2+2=10G, 再加上Fuel Controller本身的3G,在16G的台式机上可以做到。</p>
]]></content>
  </entry>
  
</feed>
