<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2015-10-18T13:49:32+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CloudStackOnUbuntuIssue]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/18/cloudstackonubuntuissue/"/>
    <updated>2015-10-18T07:34:13+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/18/cloudstackonubuntuissue</id>
    <content type="html"><![CDATA[<h3>前提</h3>

<p>可能我们会碰到这样一种情形，自己的工作机位于某一很复杂的网络环境中，需要在工作
机上搭建出一个网络资源隔离的CloudStack实验环境。而同时我们又希望能最大限度的利
用工作机的性能，尽量让虚拟化中的性能损耗达到最小。那么以下的解决方案可能就是我
们所需要的：物理机上启动一个CloudStack Management的虚拟机，而后在虚拟机的控制
台里将配置好CloudStack Agent的主机加入到CloudStack环境中。而物理机和虚拟机之间
的管理网络为物理机上的一个虚拟网络。这样既做到了硬件资源的最大利用，也有效的杜
绝了本地环境对整个复杂网络环境的影响。</p>

<h3>硬件及网络规划</h3>

<p>物理机: 4核，支持虚拟化， 内存8G，IP地址为192.168.88.0/24网段里的某台机器。   <br/>
虚拟机: 分配2核CPU，内存3G。    <br/>
物理机与虚拟机之间，通过虚拟的网络桥接。地址段被规划为172.16.16.0/24，当然你可
以更改为其他网段。</p>

<h3>物理机准备</h3>

<p>物理机运行Ubuntu 14.04.03 64位版本。</p>

<h4>Root登录权限</h4>

<p>首先打开root的ssh登录权限:</p>

<pre><code>$ sudo apt-get install -y openssh-server
$ sudo vi /etc/ssh/sshd_config
PermitRootLogin yes
$ sudo bash
# passwd
Enter new UNIX password: 
Retype new UNIX password: 
passwd: password updated successfully
# service ssh restart
</code></pre>

<p>现在重新登录，即可以使用root直接登录进系统。</p>

<h4>安装软件</h4>

<p>安装所需要的软件:</p>

<pre><code># apt-get update
# apt-get install virt-manager qemu vim wget git nfs-kernel-server
</code></pre>

<h4>创建两个网桥</h4>

<p>更改Ubuntu的网络配置文件后，重新启动物理机。这里我们创建了br0和cloudbr0两个网
桥, 注意我们设置cloudbr0的IP地址为172.16.16.1/24。这个网络将作为CloudStack
Management虚拟机与物理机之间的管理网络。</p>

<pre><code># cat /etc/network/interfaces
    # This file describes the network interfaces available on your system
    # and how to activate them. For more information, see interfaces(5).

    # The loopback network interface
    auto lo
    iface lo inet loopback

    # The primary network interface
    auto eth0
    iface eth0 inet manual

    auto br0
    iface br0 inet dhcp
    bridge_ports eth0

    auto cloudbr0
    iface cloudbr0 inet static
    bridge_ports none
    bridge_fd 5
    bridge_stp off
    bridge_maxwait 1
    address 172.16.16.1
    netmask 255.255.255.0
</code></pre>

<p>创建完毕后，用<code>ip addr</code>命令将可以看到5个网络，分别为
lo/eth0/br0/cloudbr0/virbr0， 其中virbr0由libvirtd所创建，这里不用涉及。</p>

<h4>配置cloudbr0的NAT转发</h4>

<p>打开内核的转发功能:</p>

<pre><code># vim /etc/sysctl.conf
net.ipv4.ip_forward=1
</code></pre>

<p>由于我们的虚拟机需要访问Internet以便安装包，因而我们需要开启cloudbr0上的转发，
配置iptables规则如下:</p>

<pre><code># iptables -t nat -A POSTROUTING -s \ 
172.16.16.0/24 ! -d 172.16.16.0/24 -j MASQUERADE
</code></pre>

<p>这样主机端就开启了172.16.16.0/24网段的转发。</p>

<p>保存我们配置的iptables规则:</p>

<pre><code>
# iptables-save &gt;/etc/iptables-save
# echo "pre-up iptables-restore &lt; /etc/iptables-save"&gt;&gt;/etc/network/interfaces
</code></pre>

<h4>准备NFS存储</h4>

<p>NFS存储可以被用作一级存储或者二级存储,我们这里使用它作为二级存储,主存储即一级
存储我们将使用物理机上的本地存储.</p>

<p>首先准备存储共享目录:</p>

<pre><code># mkdir -p /export/primary /export/secondary
</code></pre>

<p>引出该目录:</p>

<pre><code># cat &gt;&gt;/etc/exports &lt;&lt;EOM
/export  *(rw,async,no_root_squash,no_subtree_check)
EOM
</code></pre>

<p>在指定端口配置NFS的statd:</p>

<pre><code># cp /etc/default/nfs-common /etc/default/nfs-common.orig
# sed -i '/NEED_STATD=/ a NEED_STATD=yes' /etc/default/nfs-common
# sed -i '/STATDOPTS=/ a STATDOPTS="--port 662 \
--outgoing-port 2020"' /etc/default/nfs-common
# diff -du /etc/default/nfs-common.orig /etc/default/nfs-common
</code></pre>

<p>配置lockd:</p>

<pre><code># cat &gt;&gt; /etc/modprobe.d/lockd.conf &lt;&lt;EOM
 options lockd nlm_udpport=32769 nlm_tcpport=32803
 EOM
</code></pre>

<p>重启nfs server:</p>

<pre><code># service nfs-kernel-server restart
</code></pre>

<h4>网络名配置</h4>

<p>配置物理机的网络名如下:</p>

<pre><code># vim /etc/hostname
physicalnode
# vim /etc/hosts
127.0.0.1       localhost
172.16.16.1     physicalnode
172.16.16.2     cloudstackmgmt

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
</code></pre>

<h4>创建CloudStack Management虚拟机</h4>

<p>创建一台Ubuntu 14.04 64位虚拟机，特别注意的是其网络设置， 在virt-manager中，手
动选择如下：</p>

<p><img src="/images/2015_10_18_08_16_55_654x246.jpg" alt="/images/2015_10_18_08_16_55_654x246.jpg" /></p>

<p>这样虚拟机会选择将自己桥接到物理机上的cloudbr0接口。这时候如果在物理机运行
<code>brctl show</code>可以看到如下结果:</p>

<pre><code># brctl show
bridge name     bridge id               STP enabled     interfaces
br0             8000.52540002d56f       no              eth0
cloudbr0                8000.fe54004d6663       no              vnet0
virbr0          8000.000000000000       yes
</code></pre>

<p>虚拟机启动完毕后是没有IP地址的，因为cloudbr0上没有提供dhcp服务，我们可以手动配
置IP地址如下, 这里我们把虚拟机的IP地址配置为172.16.16.2:</p>

<pre><code>$ cat /etc/network/interfaces
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 172.16.16.2
netmask 255.255.255.0
gateway 172.16.16.1
dns-nameservers 223.5.5.5
</code></pre>

<p>配置完IP地址后，重启后可以验证是否连接到Internet.</p>

<h3>虚拟机配置</h3>

<p>我们先配置好CloudStack Management虚拟机, 而后再将物理机加入到CloudStack虚拟机
管理网络里.</p>

<h4>开启root登录</h4>

<p>步骤和上面物理机上开启root登录一样.</p>

<h4>安装源配置</h4>

<p>预先取得用于Ubuntu 14.04 CloudStack环境的安装包, 建立本地源,而后添加为:</p>

<pre><code># vim /etc/apt/sources.list
.....
deb http://192.168.1.13/iso/    cloudstackdeb/
</code></pre>

<h4>安装包</h4>

<p>安装CloudStack Management节点所需的包如下:</p>

<pre><code># apt-get update
# apt-get install vim wget openntpd cloudstack-management mysql-server
</code></pre>

<p>注意在安装mysql的时候,需要指定root用户所需的管理密码,这个密码会在后面配置
CloudStack时被用到.</p>

<h4>主机名配置</h4>

<p>配置主机名:</p>

<pre><code># vim /etc/hostname 
CloudStackMgmt
</code></pre>

<p>配置FQDN所需的主机名:</p>

<pre><code># vim /etc/hosts
127.0.0.1       localhost
172.16.16.1     physicalnode
172.16.16.2     cloudstackmgmt

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
</code></pre>

<p>重启后验证是否被设置正确:</p>

<pre><code>root@cloudstackmgmt:~# hostname
cloudstackmgmt
root@cloudstackmgmt:~# hostname --fqdn
cloudstackmgmt
</code></pre>

<h4>创建数据库</h4>

<p>创建CloudStack所需的配置文件,并重启mysql服务:</p>

<pre><code>root@cloudstackmgmt:~# cat /etc/mysql/conf.d/cloudstack.cnf 
    [mysqld]
    innodb_rollback_on_timeout=1
    innodb_lock_wait_timeout=600
    max_connections=350
    log-bin=mysql-bin
    binlog-format = 'ROW'
root@cloudstackmgmt:~# service mysql restart
    mysql stop/waiting
    mysql start/running, process 5812
</code></pre>

<p>创建CloudStack所需的数据库:</p>

<pre><code>root@cloudstackmgmt:~# cloudstack-setup-databases \
    cloud:engine@localhost \
    --deploy-as=root:xxxxxx \
    -e file -m mymskey44 -k mydbkey00
</code></pre>

<p>参数说明如下:</p>

<pre><code># mysql root 密码: xxxxxxx
# cloud user 密码: engine
# management_server_key: mymskey44
# database_key: mydbkey00
</code></pre>

<h4>安装CloudStack系统虚拟机模板</h4>

<p>加载NFS共享目录到本地,而后用以下命令安装预先下载好的kvm系统虚拟机所需的模板文
件:</p>

<pre><code># mount -t nfs 172.16.16.1:/export/secondary /mnt
# /usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt \
-m /mnt -u http://192.168.1.13/iso/systemvm64template-4.5-kvm.qcow2.bz2  -h \
kvm -F
# umount /mnt
</code></pre>

<h4>初始化配置CloudStack</h4>

<p>打开浏览器访问<a href="http://172.16.16.2:8080/client">http://172.16.16.2:8080/client</a></p>

<p>如果碰到以下错误, 则看后面的解决步骤:</p>

<p><img src="/images/2015_10_18_09_24_35_421x225.jpg" alt="/images/2015_10_18_09_24_35_421x225.jpg" /></p>

<p>关闭tomcat6服务后重启cloudstack-management服务:</p>

<pre><code># service tomcat6 stop
# service cloudstack-management restart
</code></pre>

<p>下面的Workaround可以每次重启时自动重启该服务:</p>

<pre><code># crontab -e
@reboot /bin/RestartCloudStack.sh
# vim /bin/RestartCloudStack.sh
service tomcat6 stop &amp;&amp; service cloudstack-management start
</code></pre>

<p>我们要更改CloudStack使用本地存储,并更改其镜像下载地址:  <br/>
本地存储:</p>

<p><img src="/images/2015_10_18_09_33_24_669x263.jpg" alt="/images/2015_10_18_09_33_24_669x263.jpg" /></p>

<p>镜像下载地址:</p>

<p><img src="/images/2015_10_18_09_35_09_774x233.jpg" alt="/images/2015_10_18_09_35_09_774x233.jpg" /></p>

<p>更改完毕后,<code>service coudstack-management restart</code>重启服务</p>

<h3>添加CloudStack Agent</h3>

<h4>物理机端配置</h4>

<p>在物理机上,安装和配置CloudStack Agent以及libvirtd, 同样需要配置好本地
CloudStack安装源:</p>

<pre><code># apt-get install cloudstack-agent
</code></pre>

<p>配置libvirtd:</p>

<pre><code>$ cp /etc/libvirt/libvirtd.conf /etc/libvirt/libvirtd.conf.orig

$ sed -i '/#listen_tls = 0/ a listen_tls = 0' /etc/libvirt/libvirtd.conf
$ sed -i '/#listen_tcp = 1/ a listen_tcp = 1' /etc/libvirt/libvirtd.conf
$ sed -i '/#tcp_port = "16509"/ a tcp_port = "16509"' /etc/libvirt/libvirtd.conf
$ sed -i '/#auth_tcp = "sasl"/ a auth_tcp = "none"' /etc/libvirt/libvirtd.conf
$ diff -du /etc/libvirt/libvirtd.conf.orig /etc/libvirt/libvirtd.conf
</code></pre>

<p>Patch libvirt-bin.conf:</p>

<pre><code>$ cp /etc/default/libvirt-bin /etc/default/libvirt-bin.orig
$ sed -i -e 's/libvirtd_opts="-d"/libvirtd_opts="-d -l"/' /etc/default/libvirt-bin
$ diff -du /etc/default/libvirt-bin.orig /etc/default/libvirt-bin
$ service libvirt-bin restart
</code></pre>

<p>Patch qemu.conf以监听所有端口:</p>

<pre><code>$ cp /etc/libvirt/qemu.conf /etc/libvirt/qemu.conf.orig
$ sed -i '/#vnc_listen = "0.0.0.0"/ a vnc_listen = "0.0.0.0"' /etc/libvirt/qemu.conf
$ diff -du /etc/libvirt/qemu.conf.orig /etc/libvirt/qemu.conf
$ service libvirt-bin restart
</code></pre>

<p>关闭AppArmor:</p>

<pre><code>$ ln -s /etc/apparmor.d/usr.sbin.libvirtd /etc/apparmor.d/disable/
$ ln -s /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper /etc/apparmor.d/disable/
$ apparmor_parser -R /etc/apparmor.d/usr.sbin.libvirtd
$ apparmor_parser -R /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper
$ service libvirt-bin restart
</code></pre>

<p>配置防火墙并打开以下端口:</p>

<pre><code>$ ufw allow proto tcp from any to any port 22
$ ufw allow proto tcp from any to any port 1798
$ ufw allow proto tcp from any to any port 16509
$ ufw allow proto tcp from any to any port 5900:6100
$ ufw allow proto tcp from any to any port 49152:49216
</code></pre>

<h4>配置CloudStack</h4>

<p>访问<code>http://172.16.16.2:8080/client</code>, 点击<code>I have used CloudStack before, skip
this guide</code>.</p>

<p>Infrastructure -> Zones(View All), 在点开的页面里,点击<code>Add Zone</code>.</p>

<p>选择<code>Basic</code>, Next</p>

<p><img src="/images/2015_10_18_10_20_49_521x551.jpg" alt="/images/2015_10_18_10_20_49_521x551.jpg" /></p>

<p>选择本地存储,Next:</p>

<p><img src="/images/2015_10_18_10_21_48_484x289.jpg" alt="/images/2015_10_18_10_21_48_484x289.jpg" /></p>

<p>跳过配置网络后, 配置Pod IP:</p>

<p><img src="/images/2015_10_18_10_23_02_474x419.jpg" alt="/images/2015_10_18_10_23_02_474x419.jpg" /></p>

<p>Guest Traffic配置:</p>

<p><img src="/images/2015_10_18_10_24_04_477x337.jpg" alt="/images/2015_10_18_10_24_04_477x337.jpg" /></p>

<p>Cluster名字为:</p>

<p><img src="/images/2015_10_18_10_25_05_504x207.jpg" alt="/images/2015_10_18_10_25_05_504x207.jpg" /></p>

<p>添加Host:</p>

<p><img src="/images/2015_10_18_10_26_18_469x345.jpg" alt="/images/2015_10_18_10_26_18_469x345.jpg" /></p>

<p>添加二级存储:</p>

<p><img src="/images/2015_10_18_10_27_50_501x341.jpg" alt="/images/2015_10_18_10_27_50_501x341.jpg" /></p>

<p>点击<code>Enable Zone</code>后可以激活该Zone.</p>

<p>等待系统虚拟机启动完毕后就可以使用了.</p>

<h3>已知问题</h3>

<p>CloudStack Agent启动时, 会添加iptables规则,这会造成我们前面加入的转发链失效.</p>

<p>解决方案: 手动运行命令:</p>

<pre><code># iptables -t nat -A POSTROUTING -s 172.16.16.0/24 ! -d 172.16.16.0/24 -j \ 
MASQUERADE &amp;&amp; iptables -t filter -I FORWARD -j ACCEPT
</code></pre>

<p>这将使能转发.从而172.16.16.0/24网段的机器能上网.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tips on Setting Virtual Machine Networking]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/17/tips-on-setting-virtual-machine-networking/"/>
    <updated>2015-10-17T08:11:29+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/17/tips-on-setting-virtual-machine-networking</id>
    <content type="html"><![CDATA[<h3>目的</h3>

<p>主要涉及到CloudStack的网络模型，最近在搭建一个环境时有诸多不了解，索性开始做实
验，在虚拟机里验证虚拟机的网络模型。</p>

<h3>环境准备</h3>

<p>内存 16G 的嵌套虚拟机一台，安装<code>Ubuntu 14.04 x86_64</code>版本。 安装完毕后，开启桌
面环境Xubuntu，及安装virt-manager等常用的管理虚拟机的组件。</p>

<p>网络配置如下:</p>

<pre><code>$ sudo vim /etc/network/interfaces
    # The loopback network interface
    auto lo
    iface lo inet loopback

    # The primary network interface
    auto eth0
    iface eth0 inet manual

    auto br0
    iface br0 inet dhcp
    bridge_ports eth0

    auto isolationbr
    iface isolationbr inet static
    bridge_ports none
    bridge_fd 5
    bridge_stp off
    bridge_maxwait 1
    address 172.16.0.1
    netmask 255.255.0.0
</code></pre>

<p>这里我们将eth0桥接到br0，并建立一个物理上隔绝的网桥isolationbr, 手动配置其地址
为172.16.0.1/16。</p>

<p>开启嵌套虚拟机的内核包转发功能:</p>

<pre><code># vim /etc/sysctl.conf
net.ipv4.ip_forward=1
# sysctl -w net.ipv4.ip_forward=1
# reboot
</code></pre>

<p>重启后验证内核包转发功能:</p>

<pre><code># cat /proc/sys/net/ipv4/ip_forward 
1
# sysctl -p
net.ipv4.ip_forward = 1
</code></pre>

<h3>验证网桥1</h3>

<p>用已经创建好的qcow2文件创建一台虚拟机，选择网络配置如下：</p>

<p><img src="/images/2015_10_17_08_47_08_808x488.jpg" alt="/images/2015_10_17_08_47_08_808x488.jpg" /></p>

<p>注意我们将虚拟机的第一块网卡桥接到我们刚才创建的isolationbr网桥上，当然这时候
虚拟机启动时是没有IP地址的，我们手动配置如下:</p>

<pre><code>$ sudo vim /etc/network/interfaces
    # The primary network interface
    auto eth0
    iface eth0 inet static
    address 172.16.0.2
    netmask 255.255.0.0
    gateway 172.16.0.1
</code></pre>

<p>重新启动虚拟机后，我们可以ping通嵌套虚拟机<code>172.16.0.1</code>但是无法ping通局域网内其
他机器。</p>

<h3>验证转发</h3>

<p>在嵌套虚拟机上，打开iptables的转发规则即可实现创建出的虚拟机与Internet的连接:</p>

<pre><code># iptables -t nat -A POSTROUTING -s 172.16.0.0/16 ! -d 172.16.0.0/16 -j
MASQUERADE
</code></pre>

<p>虚拟机上，ping外网地址:</p>

<pre><code>$ ping 223.5.5.5
PING 223.5.5.5 (223.5.5.5) 56(84) bytes of data.
64 bytes from 223.5.5.5: icmp_seq=1 ttl=50 time=37.1 ms
64 bytes from 223.5.5.5: icmp_seq=2 ttl=50 time=35.4 ms
64 bytes from 223.5.5.5: icmp_seq=3 ttl=50 time=35.6 ms
</code></pre>

<h3>存储转发规则</h3>

<p>Ubuntu14.04可以用下列命令将iptables设置的规则永久保存，每次开机启动:</p>

<pre><code># iptables-save&gt;/etc/iptables.rules
# vim /etc/network/interfaces
    pre-up iptables-restore &lt; /etc/iptables.rules
</code></pre>

<p>这样在重启宿主机后，我们设置的转发规则依然生效。</p>

<p>如果需要动态配置IP地址，则启动嵌套虚拟机端的dhcpd或者dnsmasq服务即可。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tips on Cloud-Init and CloudStack(2)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/14/tips-on-cloud-init-and-cloudstack/"/>
    <updated>2015-10-14T11:36:51+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/14/tips-on-cloud-init-and-cloudstack</id>
    <content type="html"><![CDATA[<h3>Cloudmonkey Resize</h3>

<p>First deploy the VM in <code>stopped</code> status:</p>

<pre><code>(local) mycloudmonkey&gt; deploy virtualmachine startvm=false
serviceofferingid=683f31f8-a939-468e-b4de-4512a8ccff8e
templateid=13fb2961-533e-4a7d-80f9-21d860269aad
zoneid=78509dc3-c828-429c-8154-9fffbc09384c
networkids=7c6e7e6b-6aa2-4f95-a835-8d18bf930061 name=testuserdata50G userdata='...`
</code></pre>

<p>Now resize the root volume into 50G size:</p>

<pre><code>
deploy virtualmachine serviceofferingid=ff775183-282b-48d7-b08e-eff51fef7683
templateid=67ca66ea-b021-4f91-ac8c-ff95f2576c9d
zoneid=1a258852-831c-4612-94f4-2551a98667bb name=testuserdata
userdata='Q29udGVudC1UeXBlOiBtdWx0aXBhcnQvbWl4ZWQ7IGJvdW5kYXJ5PSI9PT09PT09PT09PT09PT0xOTk5MDU5OTcyMjA5ODg1MjY2PT0iCk1JTUUtVmVyc2lvbjogMS4wCgotLT09PT09PT09PT09PT09PTE5OTkwNTk5NzIyMDk4ODUyNjY9PQpDb250ZW50LVR5cGU6IHRleHQveC1zaGVsbHNjcmlwdDsgY2hhcnNldD0idXMtYXNjaWkiCk1JTUUtVmVyc2lvbjogMS4wCkNvbnRlbnQtVHJhbnNmZXItRW5jb2Rpbmc6IDdiaXQKQ29udGVudC1EaXNwb3NpdGlvbjogYXR0YWNobWVudDsgZmlsZW5hbWU9ImhlbGxvX3dvcmxkLnNoIgoKIyEvYmluL2Jhc2gKZWNobyAiaGVsbG8gd29ybGQhIiA+PiAvcm9vdC90ZXN0CgotLT09PT09PT09PT09PT09PTE5OTkwNTk5NzIyMDk4ODUyNjY9PQpDb250ZW50LVR5cGU6IHRleHQvY2xvdWQtY29uZmlnOyBjaGFyc2V0PSJ1cy1hc2NpaSIKTUlNRS1WZXJzaW9uOiAxLjAKQ29udGVudC1UcmFuc2Zlci1FbmNvZGluZzogN2JpdApDb250ZW50LURpc3Bvc2l0aW9uOiBhdHRhY2htZW50OyBmaWxlbmFtZT0ibXktdXNlci1kYXRhIgoKI2Nsb3VkLWNvbmZpZwpncm93cGFydDoKICBtb2RlOiBhdXRvCmNocGFzc3dkOiB7IGV4cGlyZTogRmFsc2UgfQpzc2hfcHdhdXRoOiBUcnVlCgpzc2hfYXV0aG9yaXplZF9rZXlzOgogLSBzc2gtcnNhIEFBQUFCM056YUMxeWMyRUFBQUFEQVFBQkFBQUJBUUNzMFA4aFNCM05qN2tmd2lRT01PQ0Z2RXVqd3JLZjVuUFdmdzdzbmplVzd3TnhCYi9pTHhqbGxIK0tJdjdpS0dRaGI5WGtpZ3dXelhjdktSRk9OQTF0UU5CUHBsUE9RQXhHYUpoYzcxYlhZTVRabWsxcmZ5L0U4bUZIQmJ3U0trdm04Z3oxaFVqQWFITHdiZ21iaUE3eUNDUkVXbVR1SWpudm1FZnJXYU92WERRZFFPb2RkSzFhZThKM3BnRUNtQ21mRldrQmR3Y1JaN05jTUxBSkVkYTNpYWtJbWdaR2NqTWNCc1hjUUNOcjN1RGlKbERvc1V6Mjg4L3grTnZteTlzcHZnc2x4RXVUV0VQWFRGY1l5eVBrUHdkTnlpQm5TaWFoZTExcUdUZkk0Z2IyWllEb3JDZU5Ca1QxdkVaY0psL1JqT3NKRUFXT04rbno3Nm16MmdhZCByb290QHItOS1WTSAKCnRpbWV6b25lOiBBc2lhL0Nob25ncWluZwoKLS09PT09PT09PT09PT09PT0xOTk5MDU5OTcyMjA5ODg1MjY2PT0tLQo='
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tips on Cloud-Init and CloudStack]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/13/tips-on-cloud-init-and-cloudstack/"/>
    <updated>2015-10-13T16:22:16+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/13/tips-on-cloud-init-and-cloudstack</id>
    <content type="html"><![CDATA[<h3>CloudStack VR VM</h3>

<p>For accessing the VR VM and view the user-data and meta-data, like following, on the
instance ssh, you could get the user-data and meta-data:</p>

<pre><code>[root@testfuck ~]# curl http://10.1.1.1/latest/user-data
[root@testfuck ~]# curl http://10.1.1.1/latest/meta-data
service-offering
availability-zone
local-ipv4
local-hostname
public-ipv4
public-hostname
instance-id
vm-id
public-keys
cloud-identifier
# curl http://10.1.1.1/latest/meta-data/vm-id
c6e1165e-f19a-4c77-9d96-dfd48f8ce944
</code></pre>

<p>user-data could not be fetched directly, you could only manage it via cloud-monkey when
deploying the VMs.</p>

<h3>user-data Simple Example</h3>

<p>Before user-data injected:</p>

<pre><code>Login Using password:   

root@r-9-VM:~# ssh root@10.1.1.162
root@10.1.1.162's password: 

[root@testfuck etc]# date +%Z
BST
[root@testfuck etc]# cat /etc/sysconfig/clock 
ZONE="Europe/London"
</code></pre>

<p>But We want to change! First let VR VM could directly login to the instance, second, we
change the timezone into Asia/Chongqing.</p>

<p>Generate the user-data:</p>

<pre><code>$ cat hello_world.sh
#!/bin/bash
echo "hello world!" &gt;&gt; /root/test
$ cat my-user-data
#cloud-config
growpart:
  mode: auto
chpasswd: { expire: False }
ssh_pwauth: True

ssh_authorized_keys:
 - ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxx

timezone: Asia/Chongqing
$ $ write-mime-multipart --output=combined-userdata.txt \ 
hello_world.sh:text/x-shellscript my-user-data
$ ls -l -h combined-userdata.txt 
-rw-rw-r-- 1 dash dash 1.1K Oct 13 17:14 combined-userdata.txt
$ cat combined-userdata.txt | base64
</code></pre>

<p>Deploy via:</p>

<pre><code>cloudmonkey deploy virtualmachine
serviceofferingid=683f31f8-a939-468e-b4de-4512a8ccff8e
templateid=13fb2961-533e-4a7d-80f9-21d860269aad
zoneid=78509dc3-c828-429c-8154-9fffbc09384c
networkids=7c6e7e6b-6aa2-4f95-a835-8d18bf930061 name=testuserdata
userdata='Q29udGVudC1UeXBlOiBtdWx0aXBhcnQvbWl4ZWQ7IGJvdW5kYXJ5PSI9PT09PT09PT09PT09PT0xOTk5MDU5OTcyMjA5ODg1MjY2PT0iCk1JTUUtVmVyc2lvbjogMS4wCgotLT09PT09PT09PT09PT09PTE5OTkwNTk5NzIyMDk4ODUyNjY9PQpDb250ZW50LVR5cGU6IHRleHQveC1zaGVsbHNjcmlwdDsgY2hhcnNldD0idXMtYXNjaWkiCk1JTUUtVmVyc2lvbjogMS4wCkNvbnRlbnQtVHJhbnNmZXItRW5jb2Rpbmc6IDdiaXQKQ29udGVudC1EaXNwb3NpdGlvbjogYXR0YWNobWVudDsgZmlsZW5hbWU9ImhlbGxvX3dvcmxkLnNoIgoKIyEvYmluL2Jhc2gKZWNobyAiaGVsbG8gd29ybGQhIiA+PiAvcm9vdC90ZXN0CgotLT09PT09PT09PT09PT09PTE5OTkwNTk5NzIyMDk4ODUyNjY9PQpDb250ZW50LVR5cGU6IHRleHQvY2xvdWQtY29uZmlnOyBjaGFyc2V0PSJ1cy1hc2NpaSIKTUlNRS1WZXJzaW9uOiAxLjAKQ29udGVudC1UcmFuc2Zlci1FbmNvZGluZzogN2JpdApDb250ZW50LURpc3Bvc2l0aW9uOiBhdHRhY2htZW50OyBmaWxlbmFtZT0ibXktdXNlci1kYXRhIgoKI2Nsb3VkLWNvbmZpZwpncm93cGFydDoKICBtb2RlOiBhdXRvCmNocGFzc3dkOiB7IGV4cGlyZTogRmFsc2UgfQpzc2hfcHdhdXRoOiBUcnVlCgpzc2hfYXV0aG9yaXplZF9rZXlzOgogLSBzc2gtcnNhIEFBQUFCM056YUMxeWMyRUFBQUFEQVFBQkFBQUJBUUNzMFA4aFNCM05qN2tmd2lRT01PQ0Z2RXVqd3JLZjVuUFdmdzdzbmplVzd3TnhCYi9pTHhqbGxIK0tJdjdpS0dRaGI5WGtpZ3dXelhjdktSRk9OQTF0UU5CUHBsUE9RQXhHYUpoYzcxYlhZTVRabWsxcmZ5L0U4bUZIQmJ3U0trdm04Z3oxaFVqQWFITHdiZ21iaUE3eUNDUkVXbVR1SWpudm1FZnJXYU92WERRZFFPb2RkSzFhZThKM3BnRUNtQ21mRldrQmR3Y1JaN05jTUxBSkVkYTNpYWtJbWdaR2NqTWNCc1hjUUNOcjN1RGlKbERvc1V6Mjg4L3grTnZteTlzcHZnc2x4RXVUV0VQWFRGY1l5eVBrUHdkTnlpQm5TaWFoZTExcUdUZkk0Z2IyWllEb3JDZU5Ca1QxdkVaY0psL1JqT3NKRUFXT04rbno3Nm16MmdhZCByb290QHItOS1WTSAKCnRpbWV6b25lOiBBc2lhL0Nob25ncWluZwoKLS09PT09PT09PT09PT09PT0xOTk5MDU5OTcyMjA5ODg1MjY2PT0tLQo='
</code></pre>

<h3>Cloudmonkey</h3>

<p>Get the Key Steps:</p>

<p><img src="/images/2015_10_14_09_54_16_704x205.jpg" alt="/images/2015_10_14_09_54_16_704x205.jpg" /></p>

<p>Click &ldquo;Admin&rdquo;, and Click &ldquo;View User&rdquo;:</p>

<p><img src="/images/2015_10_14_10_02_28_706x328.jpg" alt="/images/2015_10_14_10_02_28_706x328.jpg" /></p>

<p>Generate the keys:</p>

<p><img src="/images/2015_10_14_10_05_07_676x341.jpg" alt="/images/2015_10_14_10_05_07_676x341.jpg" /></p>

<p>View the generated keys:</p>

<p><img src="/images/2015_10_14_10_08_43_408x419.jpg" alt="/images/2015_10_14_10_08_43_408x419.jpg" /></p>

<p>Configure the Cloudmonkey:</p>

<pre><code>(local) mycloudmonkey&gt; set history_file /usr/share/cloudmonkey_history
(local) mycloudmonkey&gt; set log_file /var/log/cloudmonkey               
(local) mycloudmonkey&gt; set host 10.100.168.2                           
This option has been deprecated, please set 'url' instead              
This server url will be used: http://10.100.168.2:8080/client/api      
(local) mycloudmonkey&gt; set url http://localhost:8080/client/api     
(local) mycloudmonkey&gt; set apikey
fIcD-UNUmfJG_B0oFOiVLNitFJxC7UM3ZyjlOdDUijJ4mJEgjjNQjULMQHhm69QP9IKUOiIX41-B2p1ebxhHdA                      
(local) mycloudmonkey&gt; set secretkey
s4rubpT6rR3sjMDs9LALe21puxO0GxG5-KKsLoHLy3sn7uQDiUs01BGbS4cT6BZ3FtKVPRey5ersJ4iXH7saag                   
(local) mycloudmonkey&gt; set prompt mycloudmonkey
(local) mycloudmonkey&gt; sync
</code></pre>

<p>Now run the deploy command, you could get the VM runs as the cloud-init will enlarge
the partition into the largest amount.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloud-Init Grow Partition]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/13/cloud-init-grow-partition/"/>
    <updated>2015-10-13T14:37:57+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/13/cloud-init-grow-partition</id>
    <content type="html"><![CDATA[<h3>Resize QCOW2</h3>

<p>Resize qcow2 file via:</p>

<pre><code>$ qemu-img info my_vm.img
image: my_vm.img
file format: qcow2
virtual size: 2.2G (2361393152 bytes)
disk size: 904M
cluster_size: 65536
Format specific information:
    compat: 1.1
    lazy refcounts: false
$ qemu-img resize my_vm.img +100G
Image resized.
$ qemu-img info my_vm.img
image: my_vm.img
file format: qcow2
virtual size: 102G (109735575552 bytes)
disk size: 904M
cluster_size: 65536
Format specific information:
    compat: 1.1
    lazy refcounts: false
</code></pre>

<h3>Enlarge Partition</h3>

<p>Modify the meta-data, and enable the partition grow options in user-data:</p>

<pre><code>$ vim my-user-data
#cloud-config
+ growpart:
+   mode: auto
password: xxxxxxxx
chpasswd: { expire: False }
ssh_pwauth: True

ssh_authorized_keys:
 - ssh-rsa xxxxxxxxxxx
timezone: Asia/Chongqing

$ cat my-meta-data 
instance-id: d59656d7-b365-4940-ae95-9168c32c68b7
$ echo "instance-id: $(uuidgen || echo i-abcdefg)" &gt; my-meta-data
$ cat my-meta-data 
instance-id: 5cad0dc4-facb-4079-b045-fbbc05caaf4a
</code></pre>

<p>Regenerate the image:</p>

<pre><code>$ rm -f my-seed.img
$ cloud-localds my-seed.img my-user-data my-meta-data
</code></pre>

<p>Restart the vm then you could get the disk resized.</p>
]]></content>
  </entry>
  
</feed>
