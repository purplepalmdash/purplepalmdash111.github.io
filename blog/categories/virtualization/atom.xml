<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2015-06-17T18:58:49+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[WH Worktips(1)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/06/17/wh-worktips-1/"/>
    <updated>2015-06-17T14:55:39+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/06/17/wh-worktips-1</id>
    <content type="html"><![CDATA[<h3>Preparation</h3>

<p>Hardware: 2G Memory, 1-Core, the Cobbler Server, which runs CentOS6.6.   <br/>
Network: Use a 10.47.58.0/24(Its name is WHNetwork), no dhcp server in this network.</p>

<h3>Cobbler Server Preparation</h3>

<p>First Change its IP address to 10.47.58.2, gateway to 10.47.58.1.</p>

<pre><code>[root@z_WHServer ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 
DEVICE=eth0
TYPE=Ethernet
UUID=a6e5b56f-661f-4128-ab8c-c575a9623245
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=none
IPADDR=10.47.58.2
GATEWAY=10.47.58.1
......

[root@z_WHServer ~]# cat /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=z_WHServer
# vim /etc/selinux/config
    #SELINUX=enforcing
    SELINUX=disabled 
# reboot
</code></pre>

<p>Install and configure Cobbler Server via:</p>

<pre><code># yum -y update &amp;&amp; yum install -y cobbler cobbler-web
# reboot
# openssl passwd -1                                                                                                     │
Password:                                                                                                                                 │
Verifying - Password:                                                                                                                     │
igaowugoauwgoueougo
# vim /etc/cobbler/settings
    default_password_crypted: "agowuoguwoawoguwoe"
    # default, localhost
    server: 10.47.58.2
    # default, localhost
    next_server: 10.47.58.2
    manage_dhcp: 1
</code></pre>

<p>Edit the dhcp template:</p>

<pre><code>####  subnet 192.168.1.0 netmask 255.255.255.0 {
####       option routers             192.168.1.5;
####       option domain-name-servers 192.168.1.1;
####       option subnet-mask         255.255.255.0;
####       range dynamic-bootp        192.168.1.100 192.168.1.254;
####       default-lease-time         21600;
####       max-lease-time             43200;
####       next-server                $next_server;
subnet 10.47.58.0 netmask 255.255.255.0 {
     option routers             10.47.58.1; 
     range dynamic-bootp        10.47.58.3 10.47.58.254;
     option domain-name-servers 114.114.114.114, 180.76.76.76;     
     option subnet-mask         255.255.255.0;         
     filename                   "/pxelinux.0";       
     default-lease-time         21600;           
     max-lease-time             43200;      
     next-server                $next_server; 

     class "pxeclients" {
</code></pre>

<p>Check via:</p>

<pre><code>[root@z_WHServer ~]# service cobblerd start
Starting cobbler daemon:                                   [  OK  ]
[root@z_WHServer ~]# chkconfig cobblerd on
[root@z_WHServer ~]# chkconfig httpd on
[root@z_WHServer ~]# service cobblerd status
cobblerd (pid 5421) is running...
</code></pre>

<p>Reboot and Check via:</p>

<pre><code># cobbler check
</code></pre>

<p>You will get lots of the errors, first solve the dhcpd issue, notice the following dhcpd configuration file is temporarily used.</p>

<pre><code># yum install -y dhcp
# vim /etc/dhcp/dhcpd.conf
#
# DHCP Server Configuration file.
#   see /usr/share/doc/dhcp*/dhcpd.conf.sample
#   see 'man 5 dhcpd.conf'
#
# create new
# specify domain name
option domain-name "server.world";
# specify name server's hostname or IP address
option domain-name-servers dlp.server.world;
# default lease time
default-lease-time 600;
# max lease time
max-lease-time 7200;
# this DHCP server to be declared valid
authoritative;
# specify network address and subnet mask
subnet 10.47.58.0 netmask 255.255.255.0 {
    # specify the range of lease IP address
    range dynamic-bootp 10.47.58.200 10.47.58.254;
    # specify broadcast address
    option broadcast-address 10.47.58.255;
    # specify default gateway
    option routers 10.47.58.1;
}
# service dhcpd start
# chkconfig dhcpd on
# chkconfig xinetd on
# reboot
</code></pre>

<p>Get loaders to download the loaders to <code>/var/lib/cobbler/loaders</code>:  <br/>
!!! Notice, this step maybe failed because of networking issues!!!!</p>

<pre><code>$ cobbler get-loaders
</code></pre>

<p>Change xinted:</p>

<pre><code># cat  /etc/xinetd.d/rsync 
# default: off
# description: The rsync server is a good addition to an ftp server, as it \
#       allows crc checksumming etc.
service rsync
{
        disable = no
</code></pre>

<p>Edit iptables:</p>

<pre><code>$ sudo vim /etc/sysconfig/iptables
:OUTPUT ACCEPT [0:0]
-A INPUT -p udp -m multiport --dports 69,80,443,25151 -j ACCEPT 
-A INPUT -p tcp -m multiport --dports 69,80,443,25151 -j ACCEPT 
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
$ sudo reboot
</code></pre>

<p>Install packages:</p>

<pre><code># yum install -y debmirror pykickstart cman
# cobbler check
# cobbler sync
</code></pre>

<h3>Import Systems</h3>

<p>Import 2 DVD iso via:</p>

<pre><code># mount -o loop -t iso9660 CentOS-6.5-x86_64-bin-DVD1.iso  /mnt1/
# cobbler import --name=CentOS-6.5 --arch=x86_64 --path=/mnt1
# mount -o loop -t iso9660 CentOS-6.5-x86_64-bin-DVD2.iso  /mnt2
# rsync -a '/mnt2/' /var/www/cobbler/ks_mirror/CentOS-6.5-x86_64/ --exclude-from=/etc/cobbler/rsync.exclude --progress
# COMPSXML=$(ls /var/www/cobbler/ks_mirror/CentOS-6.5-x86_64/repodata/*comps*.xml)
# createrepo -c cache -s sha --update --groupfile ${COMPSXML} /var/www/cobbler/ks_mirror/CentOS-6.5-x86_64/ 
</code></pre>

<p>Verify it via:</p>

<pre><code>[root@z_WHServer repodata]# cobbler distro list
   CentOS-6.5-x86_64
[root@z_WHServer repodata]# cobbler profile list
   CentOS-6.5-x86_64
[root@z_WHServer repodata]# cobbler distro report --name=CentOS-6.5-x86_64
Name                           : CentOS-6.5-x86_64
Architecture                   : x86_64
TFTP Boot Files                : {}
Breed                          : redhat
Comment                        : 
Fetchable Files                : {}
Initrd                         : /var/www/cobbler/ks_mirror/CentOS-6.5-x86_64/images/pxeboot/initrd.img
Kernel                         : /var/www/cobbler/ks_mirror/CentOS-6.5-x86_64/images/pxeboot/vmlinuz
</code></pre>

<p>Check if your tftp working:</p>

<pre><code># yum install tftp-server
# vim /etc/xinetd.d/tftp
# /sbin/chkconfig tftp on
# service xinetd start
# netstat -anp | grep 69
# tftp 10.47.58.2
get pxelinux.0
</code></pre>

<p>If successful, the pxelinux.0 will downloaded to your directory.</p>

<h3>Install New Systems</h3>

<p>Use a machine, configure to the same network, then start from pxe.</p>

<h3>Customize the KS file</h3>

<p>Generate kickstart configuration file via:</p>

<pre><code># system-config-kickstart 
</code></pre>

<p>Add a new profile via:</p>

<pre><code>[root@z_WHServer kickstarts]# cobbler profile add --name=CentOS6.5-Desktop --kickstart=/var/lib/cobbler/kickstarts/CentOS-6.5-x86_64/C
sktop.cfg --distro=CentOS-6.5-x86_64
[root@z_WHServer kickstarts]# cobbler profile list
   CentOS-6.5-x86_64
   CentOS6.5-Desktop
</code></pre>

<h3>Cobbler Web Interface</h3>

<pre><code>$ htdigest /etc/cobbler/users.digest "Cobbler" cobbler 
Which will prompt you for a new password. 
Once you have updated the password remember to run
$ cobbler sync
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[XenServer Tips]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/06/16/xenserver-tips/"/>
    <updated>2015-06-16T14:52:20+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/06/16/xenserver-tips</id>
    <content type="html"><![CDATA[<p>Recently I want to research desktop virtualization on Xen, so this blog records all of the tips for Xen Hypervisor related info.</p>

<h3>Nested Virtualization</h3>

<p>I place 4 core(Copy Host Configuration on CPU parameter), but the XenServer refuse to start, by using a none-hosted-configuration CPU configuration, it will fail on starting the machine, So I choose to install xen hypervisor on Ubuntu14.04.</p>

<h3>Ubuntu and Xen</h3>

<p>Install via:</p>

<pre><code>$ sudo apt-get install xen-hypervisor-amd64
$ sudo reboot
</code></pre>

<p>The Ubuntu will automatically choose xen for startup, so verify it via:</p>

<pre><code>$ sudo xl list
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  7832     4     r-----      72.8
</code></pre>

<h3>Networking</h3>

<p>Since the network is pretty complicated on my own machine, I decide to use openVSwitch for managing my own networking.</p>

<pre><code>$ sudo apt-get install -y openvswitch-switch
</code></pre>

<p>Configuration of the networking:</p>

<pre><code>$ cat /etc/network/interfaces
###########################################
## By using openVswitch, we enabled the following
###########################################
allow-hotplug ovsbr0
iface ovsbr0 inet static
address 192.168.0.119
netmask 255.255.0.0
gateway 192.168.0.176
dns-nameservers 114.114.114.114
dns-nameservers 180.76.76.76

$ sudo ovs-vsctl add-br ovsbr0
$ sudo ovs-vsctl set Bridge ovsbr0 stp_enable=false other_config:stp-max-age=6 other_config:stp-forward-delay=4
$ sudo ovs-vsctl list Bridge
$ sudo ovs-vsctl add-port ovsbr0 eth0
</code></pre>

<p>Disable the netfilter on all bridges:</p>

<pre><code>$ sudo vi /etc/sysctl.conf

net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0

$ sudo sysctl -p /etc/sysctl.conf
# Note: These settings are created in /proc/sys/net. The bridge folder only appears to be created after first creating a bridge with the ''brctl' command.
</code></pre>

<h3>Administrator Tools</h3>

<pre><code>$ sudo apt-get install virt-manager
$ sudo apt-get install xen-tools
</code></pre>

<h3>Connect with virt-manager</h3>

<p>Change following parameters and re-connect again.</p>

<pre><code># vim /etc/xen/xend-config.sxp 
     xend-unix-server yes
     xend-unix-path /var/lib/xend/xend-socket
# service libvirt-bin restart
libvirt-bin stop/waiting
libvirt-bin start/running, process 5345
# service xen restart
 * Restarting Xen daemons                                                                                                             ^[[A                                                                                                                           [ OK ]
# service xendomains restart
</code></pre>

<p>Manually:</p>

<pre><code>$ sudo virt-install --connect=xen:/// --name u14.04 --ram 1024 --disk u14.04.img,size=4 --location http://ftp.ubuntu.com/ubuntu/dists/trusty/main/installer-amd64/


# http://ftp.ubuntu.com/ubuntu/dists/trusty/main/installer-amd64/
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tips on Ansible for Deploying CloudStack]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/06/15/tips-on-ansible-for-deploying-cloudstack/"/>
    <updated>2015-06-15T11:38:51+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/06/15/tips-on-ansible-for-deploying-cloudstack</id>
    <content type="html"><![CDATA[<h3>Background</h3>

<p>Use two nodes, both running CentOS6.6, the Ansible Node have 1-G memory, while the csmanger(CloudStack Manager) Node have 4-G memory.</p>

<p>Install the epel on Cloudstack manager for installing ansible.</p>

<pre><code>$ wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo
$ yum install -y ansible sshpass
</code></pre>

<h3>Get Nodes</h3>

<p>Create the definition of the ansible characters:</p>

<pre><code>[root@CentOS66Base ~]# mkdir -p ~/Code/Ansible
[root@CentOS66Base ~]# cd Code/Ansible/
[root@CentOS66Base Ansible]# vim ansible.cfg
[defaults]
hostfile=/root/Code/Ansible/hosts
[root@CentOS66Base Ansible]# vim hosts
[acs-manager]
10.55.55.72
[root@CentOS66Base Ansible]# cat /etc/hosts
.......
10.55.55.72 acs-manager
[root@CentOS66Base Ansible]# ping acs-manager
PING acs-manager (10.55.55.72) 56(84) bytes of data.
64 bytes from acs-manager (10.55.55.72): icmp_seq=1 ttl=64 time=1.16 ms
</code></pre>

<p>Add the ssh certification:</p>

<pre><code>[root@CentOS66Base Ansible]# ssh-keyscan acs-manager&gt;&gt;/root/.ssh/known_hosts 
# acs-manager SSH-2.0-OpenSSH_5.3
</code></pre>

<p>Generate the <code>id_rsa.pub</code> locally:</p>

<pre><code>[root@CentOS66Base Ansible]# ssh-keygen -t rsa -b 2048
......
[root@CentOS66Base Ansible]# ls -l -h /root/.ssh/id_rsa.pub 
-rw-r--r--. 1 root root 399 Jun 14 23:59 /root/.ssh/id_rsa.pub
</code></pre>

<p>Write key installation yml file:</p>

<pre><code>[root@CentOS66Base Ansible]# vim ssh-addkey.ymll 
---
- hosts: all
  sudo: yes
  gather_facts: no
  remote_user: root

  tasks:

  - name: install ssh key
    authorized_key: user=root
                    key="" 
                    state=present
</code></pre>

<p>!!! Be sure to disable the selinux on the acs-manager machine, then install the sshkey via:</p>

<pre><code>root@CentOS66Base Ansible]# ansible-playbook ssh-addkey.yml --ask-pass
SSH password: 

PLAY [all] ******************************************************************** 

TASK: [install ssh key] ******************************************************* 
changed: [10.55.55.72]

PLAY RECAP ******************************************************************** 
10.55.55.72                : ok=1    changed=1    unreachable=0    failed=0   
</code></pre>

<h3>Prepare the secondary storage</h3>

<pre><code>$ sudo apt-get install nfs-kernel-server
$ cd /srv 
$ mkdir nfs4
$ ls
nfs4
$ sudo chown nobody:nogroup /srv/nfs4 
$ sudo vim /etc/exports
# Share access to all networks
/srv/nfs4       *(rw,sync,no_subtree_check)
$ sudo /etc/init.d/nfs-kernel-server start
</code></pre>

<h3>Start deploying</h3>

<p>Deployment sequences:</p>

<pre><code>$ ansible-playbook -i /root/Code/Ansible/hosts  --limit=acs-manager ./cloudstack.yml --tags=base
$ ansible-playbook -i /root/Code/Ansible/hosts  --limit=acs-manager ./cloudstack.yml --tags=mysql
$ ansible-playbook -i /root/Code/Ansible/hosts  --limit=acs-manager ./cloudstack.yml --tags=mysql3306
$ ansible-playbook -i /root/Code/Ansible/hosts  --limit=acs-manager ./cloudstack.yml --tags=csmanagement
</code></pre>

<h3>Trouble Shooting</h3>

<p>Too slow connection:</p>

<pre><code>Use Redsocks. 
By pass 
iptables -t nat -I OUTPUT -d 192.168.0.0/16 -j RETURN
</code></pre>

<p>CloudMonkey setup:</p>

<pre><code>The host name of this computer does not resolve to an IP address.

[root@acs-manager ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.55.55.72     acs-manager
</code></pre>

<p>After deployment, visit:</p>

<p><a href="http://xxx.xx.x.xxx:8080/client">http://xxx.xx.x.xxx:8080/client</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenVSwitch and VXLAN How-to]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/06/08/openvswitch-and-vxlan-how-to/"/>
    <updated>2015-06-08T09:48:20+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/06/08/openvswitch-and-vxlan-how-to</id>
    <content type="html"><![CDATA[<p>Following records the steps for my setup for OpenVSwitch environment and configure VXLAN on it.</p>

<h3>Preparation</h3>

<p>I use two VMs for this experiment, created a new virtual network, it&rsquo;s 10.94.94.0/24, every vm machines adds into this network. <br/>
VM1, VM2, both have 1G Memory. 1 Core. <br/>
VM1: 10.94.94.11, VM2: 10.94.94.12.</p>

<pre><code>$ sudo apt-get update &amp;&amp; sudo apt-get -y upgrade
$ sudo apt-get install build-essential$
$ sudo reboot
$ uname -a
$ uname -a
Linux OpenVSwitchVM1 3.13.0-24-generic #47-Ubuntu SMP Fri May 2 23:30:00 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
</code></pre>

<h3>Generate DEB</h3>

<p>Following steps includes install dependencies, fetching source code, build, generate package, notice we use 2.3.0 version of the openvswitch.</p>

<pre><code>$ sudo apt-get install -y build-essential fakeroot debhelper \
                    autoconf automake bzip2 libssl-dev \
                    openssl graphviz python-all procps \
                    python-qt4 python-zopeinterface \
                    python-twisted-conch libtool
$ wget http://openvswitch.org/releases/openvswitch-2.3.0.tar.gz
$ tar xzvf openvswitch-2.3.0.tar.gz
$ cd openvsiwtch-2.3.0
$ DEB_BUILD_OPTIONS='parallel=8 nocheck' fakeroot debian/rules binary
$ cd ..
$ ls -al *.deb
openvswitch-common_2.3.0-1_amd64.deb         openvswitch-ipsec_2.3.0-1_amd64.deb   openvswitch-vtep_2.3.0-1_amd64.deb
openvswitch-datapath-dkms_2.3.0-1_all.deb    openvswitch-pki_2.3.0-1_all.deb       python-openvswitch_2.3.0-1_all.deb
openvswitch-datapath-source_2.3.0-1_all.deb  openvswitch-switch_2.3.0-1_amd64.deb
openvswitch-dbg_2.3.0-1_amd64.deb            openvswitch-test_2.3.0-1_all.deb
</code></pre>

<p>Also copy all of the deb files into another PC.</p>

<h3>Installation</h3>

<p>In two machines, do following steps for installing.</p>

<pre><code>$ sudo apt-get install -y bridge-utils
$ sudo dpkg -i openvswitch-common_2.3.1-1_amd64.deb \
         openvswitch-switch_2.3.1-1_amd64.deb
</code></pre>

<h3>VM Netorking Configuration</h3>

<p>For VM1:</p>

<pre><code>root@OpenVSwitchVM1:~# ovs-vsctl add-br br0
root@OpenVSwitchVM1:~# ovs-vsctl add-br br1
# ovs-vsctl add-port br0 eth0
# ifconfig eth0 0 up
# ifconfig br0 10.94.94.11
# route add default gw 10.94.94.1 br0
# ifconfig br1 172.10.0.1
</code></pre>

<p>For VM2:</p>

<pre><code># ovs-vsctl add-br br0
# ovs-vsctl add-br br1
# ovs-vsctl add-port br0 eth0
# ifconfig eth0 0 up &amp;&amp; ifconfig br0 10.94.94.12
# route add default gw 10.94.94.1
# ifconfig br1 172.10.1.1
</code></pre>

<p>Ping each other, we could see br1 is not OK.</p>

<h3>VXLAN Setup</h3>

<p>On VM1, do following operation, to set the vx1:</p>

<pre><code>root@OpenVSwitchVM1:~# ovs-vsctl add-port br1 vx1 -- set interface vx1 type=vxlan options:remote_ip=10.94.94.12
root@OpenVSwitchVM1:~# ovs-vsctl show
a1e9afb6-345a-4f79-8e0b-131cd43cfb67
    Bridge "br0"
        Port "eth0"
            Interface "eth0"
        Port "br0"
            Interface "br0"
                type: internal
    Bridge "br1"
        Port "br1"
            Interface "br1"
                type: internal
        Port "vx1"
            Interface "vx1"
                type: vxlan
                options: {remote_ip="10.94.94.12"}
    ovs_version: "2.3.0"
</code></pre>

<p>On VM2, do following operation, to set vx1</p>

<pre><code>root@OpenVSwitchVM2:~# ovs-vsctl add-port br1 vx1 -- set interface vx1 type=vxlan options:remote_ip=10.94.94.11
root@OpenVSwitchVM2:~# ovs-vsctl show
bce3f2b5-9b77-41dc-8130-b8922dd7ac9e
    Bridge "br1"
        Port "vx1"
            Interface "vx1"
                type: vxlan
                options: {remote_ip="10.94.94.11"}
        Port "br1"
            Interface "br1"
                type: internal
    Bridge "br0"
        Port "br0"
            Interface "br0"
                type: internal
        Port "eth0"
            Interface "eth0"
    ovs_version: "2.3.0"
</code></pre>

<p>So now you could ping each other via the br1 address.</p>

<h3>Mirror Port</h3>

<p>Do the following things for setting up the mirror port.</p>

<pre><code>#  modprobe dummy
#  ip link set up dummy0
root@OpenVSwitchVM1:~# ovs-vsctl add-port br1 dummy0
root@OpenVSwitchVM1:~# ovs-vsctl --id=@m create mirror name=mirror0 -- add bridge br1 mirrors @m
33931f5a-008f-44cf-abc6-38afb3062b5e
root@OpenVSwitchVM1:~# ovs-vsctl list port dummy0
_uuid               : 5f5fe675-b1ee-4acd-a0ab-f14e952d1603
bond_downdelay      : 0
bond_fake_iface     : false
bond_mode           : []
bond_updelay        : 0
external_ids        : {}
fake_bridge         : false
interfaces          : [a6fbabe9-790d-4be8-a362-b7cbdd46db89]
lacp                : []
mac                 : []
name                : "dummy0"
other_config        : {}
qos                 : []
statistics          : {}
status              : {}
tag                 : []
trunks              : []
vlan_mode           : []
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use Chef for Deploying CloudStack Management Node]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/06/05/use-chef-for-deploying-cloudstack-management-node/"/>
    <updated>2015-06-05T17:07:22+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/06/05/use-chef-for-deploying-cloudstack-management-node</id>
    <content type="html"><![CDATA[<p>Following record all of the necessary steps for deploying cloudstack management node on kvm based environment.</p>

<h3>Get The CookBook</h3>

<p>You need following cookbooks:</p>

<pre><code>[kkkk@~/chef-repo/cookbooks]$ ls
apt         cloudstack_wrapper      cookbook_cloudstack_wrapper  line   nfs    rbac       selinux  sudo  yum-mysql-community
cloudstack  cloudstack_wrapper_kvm  learn_chef_apache2           mysql  nginx  README.md  smf      yum
[kkkk@~/chef-repo/cookbooks]$ pwd
/home/kkkk/chef-repo/cookbooks
</code></pre>

<p>Most of the books could be downloaded from the chef supermarket, while the <code>cookbook_cloudstack_wrapper</code> is downloaded from the github, and <code>cloudstack_wrapper_kvm</code> is modified from it.</p>

<p>Note: You have to replace all of the <code>cloudstack_wrapper::</code> to <code>cloudstack_wrapper_kvm::</code> under the copied folder.</p>

<p>You have to modify the definition of the</p>

<pre><code>[dash@~/chef-repo/cookbooks]$ cat cloudstack_wrapper_kvm/recipes/management_server.rb

......

# download initial systemvm template
cloudstack_system_template 'kvm' do
  nfs_path    node['cloudstack']['secondary']['path']
  nfs_server  node['cloudstack']['secondary']['host']
  url         node['cloudstack']['systemvm']['kvm']
  db_user     node['cloudstack']['db']['username']
  db_password node['cloudstack']['db']['password']
  db_host     node['cloudstack']['db']['host']
  action :create
end
......
</code></pre>

<h3>Add Node</h3>

<p>Add a new node into the system , then you should do following steps for letting the deployment continue:</p>

<pre><code>$ proxychains4  /opt/chef/embedded/bin/gem install cloudstack_ruby_client
$ sudo apt-get update
$  sudo apt-get install nfs-common
</code></pre>

<p>Now continue until you could see the final result.</p>

<h3>Verification</h3>

<p>After deployment, visit:</p>

<p><a href="http://YourIP:8080/client">http://YourIP:8080/client</a>   admin/password</p>
]]></content>
  </entry>
  
</feed>
