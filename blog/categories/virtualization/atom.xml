<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2015-12-02T22:19:01+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spice Client]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/11/16/spice-client/"/>
    <updated>2015-11-16T21:20:00+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/11/16/spice-client</id>
    <content type="html"><![CDATA[<h3>Image Conversion</h3>

<p>Convert the vdi files into qcow2 file:</p>

<pre><code>$ qemu-img convert -f vdi -O qcow2 Windows81.vdi Windows81.qcow2
</code></pre>

<p>Then continue to create the virtual machine via importing the img.</p>

<h3>Spice Client</h3>

<p>Using virtviewer for view the remote machine.</p>

<pre><code>$ sudo pacman -S virtviewer
$ remote-viewer spice://localhost:5900
</code></pre>

<p>Or you could view the desktop via <code>spicec</code>.</p>

<p>The listening port could be view via <code>netstat -anp | grep 5900</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Moving CloudStack From VM to Physical Node]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/11/01/moving-cloudstack-from-vm-to-physical-node/"/>
    <updated>2015-11-01T08:52:08+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/11/01/moving-cloudstack-from-vm-to-physical-node</id>
    <content type="html"><![CDATA[<h3>AIM</h3>

<p>To move the All-In-One CloudStack environment from kvm VM based to physical
machine.</p>

<h3>Copy VM Disk</h3>

<h3>Based On Ubuntu15.04</h3>

<p>First get all of the deb file from
<code>http://cloudstack.apt-get.eu/ubuntu/dists/trusty/4.5/pool/</code></p>

<p>Then setup a local repository, edit its <code>/etc/apt/sources.list</code> file:</p>

<pre><code># vim /etc/apt/sources.list
deb http://192.168.1.13/        cloudstackdeb/
</code></pre>

<p>Problem, For we have setup the ovsbr0 on Ubuntu15.04, how to solve its
networking together with cloudstack?</p>

<h3>Newly installed CentOS7</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thinking in VR]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/29/thinking-in-vr/"/>
    <updated>2015-10-29T16:00:39+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/29/thinking-in-vr</id>
    <content type="html"><![CDATA[<p>Restriction of the connections.</p>

<pre><code>http://www.cnblogs.com/cmt/archive/2013/03/13/2957583.html
http://bbs.m0n0china.org/viewthread.php?tid=16459
http://my.oschina.net/u/1169079/blog/397705
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack Liberty安装(1)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/25/openstack-libertyan-zhuang-1/"/>
    <updated>2015-10-25T12:05:58+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/25/openstack-libertyan-zhuang-1</id>
    <content type="html"><![CDATA[<h3>初始化准备</h3>

<p>用Packer.io制作Ubuntu14.04的qcow2文件镜像, 设定磁盘大小为100G, lvm分区.  <br/>
网络: 在virt-manager中制作一个网段为10.0.0.0/24的网段. 所有创建虚拟机的eth0均
加入到此网络中.   <br/>
<img src="/images/2015_10_25_12_22_24_504x487.jpg" alt="/images/2015_10_25_12_22_24_504x487.jpg" /></p>

<p>To be Continued.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tips on Ceph on Docker]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/23/tips-on-ceph-on-docker/"/>
    <updated>2015-10-23T22:32:17+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/23/tips-on-ceph-on-docker</id>
    <content type="html"><![CDATA[<h3>Installation</h3>

<p>Pull the docker image via:</p>

<pre><code>$ sudo docker pull ceph/demo
</code></pre>

<h3>Run Ceph</h3>

<p>Run the container via:</p>

<pre><code># sudo docker run -d --net=host -e MON_IP=192.168.10.190 -e CEPH_NETWORK=192.168.10.0/24
ceph/demo
</code></pre>

<p>View the docker instance via:</p>

<pre><code># docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
cbe567594adb        ceph/demo           "/entrypoint.sh"    About an hour ago   Up About an hour                        furious_hopper 
# docker exec -it cbe567594adb
</code></pre>

<h3>Ceph Operation</h3>

<p>View the ceph processes:</p>

<pre><code>root@monitor:/# ps -ef | grep "ceph"                                                                                                                                 
root         1     0  0 13:16 ?        00:00:00 /usr/bin/python /usr/bin/ceph -w
root        32     1  0 13:16 ?        00:00:01 ceph-mon -i monitor --public-addr 192.168.10.190:6789
root       201     1  0 13:16 ?        00:00:06 ceph-osd -i 0 -k /var/lib/ceph/osd/ceph-0/keyring
root       412     1  0 13:16 ?        00:00:01 ceph-mds --cluster=ceph -i 0
root       470     1  0 13:16 ?        00:00:03 radosgw -c /etc/ceph/ceph.conf -n client.radosgw.gateway -k /var/lib/ceph/radosgw/monitor/keyring --rgw-socket-path= --rgw-frontends=civetweb port=80
root       473     1  0 13:16 ?        00:00:01 /usr/bin/python /usr/bin/ceph-rest-api -n client.admin
root      1703  1557  0 14:37 ?        00:00:00 grep --color=auto ceph
</code></pre>

<p>View Ceph Status:</p>

<pre><code>root@monitor:/# ceph -s
    cluster c3470b36-8d03-4dbb-8af4-d4353ea54973
     health HEALTH_OK
     monmap e1: 1 mons at {monitor=192.168.10.190:6789/0}
            election epoch 2, quorum 0 monitor
     mdsmap e5: 1/1/1 up {0=0=up:active}
     osdmap e22: 1 osds: 1 up, 1 in
      pgmap v75: 144 pgs, 11 pools, 6796 bytes data, 70 objects
            3789 MB used, 287 GB / 291 GB avail
                 144 active+clean
</code></pre>

<p>Create a new user:</p>

<pre><code># radosgw-admin user create --uid="xxxx" --display-name="XXXX YYYY" --email=xxxyyy@gmail.com
</code></pre>

<p>Remember the output of the <code>access_key</code> and <code>secret_key</code>.</p>

<h3>Configure Ceph</h3>

<p>Install softwares:</p>

<pre><code># apt-get update -y
# apt-get install -y python
# apt-get install -y python-pip
# pip install boto
# pip install ipython
# pip install s3cmd
# apt-get install -y vim
</code></pre>

<p>Use the <code>list_buckets.py</code> file:</p>

<pre><code># cat list_buckets.py
import boto
import boto.s3.connection

access_key =  '5S5YPYC46EVYG9MF0RSR'
secret_key = 'hEcgOMoNOp6jmYnt3G6qqJiT7mV5A8zBR9g6o38Z'

conn = boto.connect_s3(
        aws_access_key_id = access_key,
        aws_secret_access_key = secret_key,
        host = 'localhost',
        is_secure=False,
        calling_format = boto.s3.connection.OrdinaryCallingFormat(),
        )
</code></pre>

<p>In ipython, use following commands for create a new bucket:</p>

<pre><code>In [8]: conn.create_bucket("fuck")                                        
Out[8]: &lt;Bucket: fuck&gt;

In [9]: conn.get_all_buckets()                                                 
Out[9]: [&lt;Bucket: fuck&gt;]
</code></pre>

<p>Download the s3cfg file from :</p>

<p><a href="https://github.com/tobegit3hub/.s3cfg/blob/master/.s3cfg">https://github.com/tobegit3hub/.s3cfg/blob/master/.s3cfg</a></p>

<p>Also configure its <code>access_key</code> and <code>secret_key</code>, save it in your root
directory, run following commands, you will see the buckets:</p>

<pre><code># s3cmd ls                                                                                                                                             
2015-10-23 13:25  s3://fuck
</code></pre>

<h3>Mount Ceph In Linux</h3>

<p>In Ubuntu machine, install ceph:</p>

<pre><code>$ sudo apt-get install ceph
</code></pre>

<p>Get the admin&rsquo;s password(In Ceph Container):</p>

<pre><code># ceph-authtool --print-key /etc/ceph/ceph.client.admin.keyring 
AQAmMypWs06BGxAAQ1rQyFqFJ25xaDye4c9kyQ==
</code></pre>

<p>Now mount it via:</p>

<pre><code>$ sudo mount -t ceph 192.168.10.190:/ /mnt -o name=admin,secret=AQAmMypWs06BGxAAQ1rQyFqFJ25xaDye4c9kyQ==
$ sudo touch /mnt/abc
</code></pre>

<p>Via <code>ceph -s</code> we could see the <code>pgmap v86</code> changes, which indicates the data
has been written into the ceph.</p>
]]></content>
  </entry>
  
</feed>
