<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2015-05-25T15:22:20+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[LXCize the KVM Machine]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/25/lxcize-the-kvm-machine/"/>
    <updated>2015-05-25T11:46:28+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/25/lxcize-the-kvm-machine</id>
    <content type="html"><![CDATA[<p>In order to make exsiting kvm based machine to be lxc container, following is the steps.</p>

<h3>Convert Disk Formats</h3>

<p>First we want to convert the qcow2 format image to raw format, by following command:</p>

<pre><code>$ qemu-img convert u12-debug-ui.qcow2 Contrail.raw
</code></pre>

<p>This will take a very long time, because qcow2 file will expand to a whole images, like mine, the Contrail.raw in fact expands to 100G size.</p>

<h3>LXC the KVM</h3>

<p>Since we have the raw image file, we could create a new configuration file for starting the machine:</p>

<pre><code> myvm.conf
lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.name = eth0
lxc.network.ipv4 = xxx.xxx.10.230/24
lxc.network.ipv4.gateway = xxx.xxx.0.176
#lxc.network.link = lxcbr0
lxc.utsname = myvminlxc

lxc.tty = 4
lxc.pts = 1024
lxc.rootfs = /dev/mapper/loop0p1
lxc.arch = amd64
lxc.cap.drop = sys_module mac_admin

lxc.cgroup.devices.deny = a
# Allow any mknod (but not using the node)
lxc.cgroup.devices.allow = c *:* m
lxc.cgroup.devices.allow = b *:* m
# /dev/null and zero
lxc.cgroup.devices.allow = c 1:3 rwm
lxc.cgroup.devices.allow = c 1:5 rwm
# consoles
lxc.cgroup.devices.allow = c 5:1 rwm
lxc.cgroup.devices.allow = c 5:0 rwm
#lxc.cgroup.devices.allow = c 4:0 rwm
#lxc.cgroup.devices.allow = c 4:1 rwm
# /dev/{,u}random
lxc.cgroup.devices.allow = c 1:9 rwm
lxc.cgroup.devices.allow = c 1:8 rwm
lxc.cgroup.devices.allow = c 136:* rwm
lxc.cgroup.devices.allow = c 5:2 rwm
# rtc
lxc.cgroup.devices.allow = c 254:0 rwm
#fuse
lxc.cgroup.devices.allow = c 10:229 rwm
#tun
lxc.cgroup.devices.allow = c 10:200 rwm
#full
lxc.cgroup.devices.allow = c 1:7 rwm
#hpet
lxc.cgroup.devices.allow = c 10:228 rwm
#kvm
lxc.cgroup.devices.allow = c 10:232 rwm
</code></pre>

<p>Now setup the image file via, this will use the <code>/dev/mapper/loop0p1</code> as the root partition:</p>

<pre><code>$ sudo kpartx -a Contrail.img
</code></pre>

<p>Now start the machine via:  <br/>
<code>
$ sudo lxc-start -n myvminlxc -f ./myvm.conf
</code></p>

<p>Your machine will boot into the lxc, and its behavior is the same as the kvm based machine.</p>

<h3>TroubleShooting</h3>

<p>The root could not login into the lxc, because of the selinux enabled in the Ubuntu Host, simply disable it in <code>/etc/selinux/config</code> by:</p>

<pre><code>$ sudo vim /etc/selinux/config
#SELINUX=permissive
SELINUX=disabled
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三节点搭建OpenStack Juno(2)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/24/san-jie-dian-da-jian-openstack-juno-2/"/>
    <updated>2015-05-24T22:37:28+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/24/san-jie-dian-da-jian-openstack-juno-2</id>
    <content type="html"><![CDATA[<h3>MySQL数据库</h3>

<p>绝大多数的OpenStack服务使用SQL数据库来存储信息，一般情况下数据库运行在控制节点上，这里我们使用MariaDB或者MySQL来作为SQL数据库。</p>

<p>安装, 注意安装过程中需要输入密码:</p>

<pre><code># apt-get install mariadb-server python-mysqldb
</code></pre>

<p>配置, 主要是更改了bind的地址，添加了一些有用选项，并支持UTF-8编码:</p>

<pre><code>$ sudo vim /etc/mysql/my.cnf
[mysqld]
...
bind-address = 10.55.55.2
...
default-storage-engine = innodb
innodb_file_per_table
collation-server = utf8_general_ci
init-connect = 'SET NAMES utf8'
character-set-server = utf8
</code></pre>

<p>完成安装，包括重启服务及加密数据库服务:</p>

<pre><code># service mysql restart
# mysql_secure_installation 
</code></pre>

<h3>消息服务器</h3>

<p>OpenStack使用message broker用来在各种服务器之间调度操作和协调状态信息，通常情况下消息服务器也运行在控制节点上，OpenStack支持RabbitMQ, Qpid和ZeroMQ, 这里使用RabbitMQ.</p>

<p>安装:</p>

<pre><code># apt-get install rabbitmq-server
</code></pre>

<p>配置，首先我们需要设定rabbitMQ使用的密码:</p>

<pre><code># rabbitmqctl change_password guest RABBIT_PASS
Changing password for user "guest" ...
...done.
</code></pre>

<p>如果是RabbitMQ 3.3.0或者更新的版本，则需要激活guest用户的远程访问权限。</p>

<p>检查RabbitMQ版本:</p>

<pre><code># rabbitmqctl status | grep rabbit
Status of node rabbit@Controller ...
 {running_applications,[{rabbit,"RabbitMQ","3.2.4"},
</code></pre>

<p>这里我们的版本是3.2.4所以不需要做任何修改，直接重启RabbitMQ服务即可。若是3.3.0以后的版本，则需要参考官方文档作更为详细的配置。</p>

<pre><code># service rabbitmq-server restart
</code></pre>

<h3>鉴权(Identity)服务</h3>

<p>鉴权服务的作用主要有:   <br/>
1. 跟踪用户及其权限。   <br/>
2. 提供可用服务的服务类别及API endpoint.</p>

<p>详细的关于Identity的介绍可以参见OpenStack官方文档。只有理解了其理念后才能明了OpenStack架构中各种服务的角色和地位.</p>

<p>首先创建keystone所需要的数据库:</p>

<pre><code># mysql -u root -p
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 36
Server version: 5.5.43-MariaDB-1ubuntu0.14.04.2 (Ubuntu)

Copyright (c) 2000, 2015, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE keystone;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
    -&gt; IDENTIFIED BY 'KEYSTONE_PASSWD';
Query OK, 0 rows affected (0.01 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
    -&gt; IDENTIFIED BY 'KEYSTONE_PASSWD';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; quit;
Bye
</code></pre>

<p>创建一个随机值，用于管理token在初始化配置时使用:</p>

<pre><code># openssl rand -hex 10
760bc221f4dc966693e5
</code></pre>

<p>安装和配置组件:</p>

<pre><code># apt-get install keystone python-keystoneclient
</code></pre>

<p>配置, 更改<code>admin_token</code>为刚才生成的随机数:  <br/>
<code>
$ sudo vim /etc/keystone/keystone.conf
[DEFAULT]
...
admin_token = 760bc221f4dc966693e5
...
[database]
...
connection = mysql://keystone:KEYSTONE_DBPASS@Controller/keystone
...
[token]
...
provider = keystone.token.providers.uuid.Provider
driver = keystone.token.persistence.backends.sql.Token
...
[revoke]
...
driver = keystone.contrib.revoke.backends.sql.Revoke
...
[DEFAULT]
...
verbose = True
</code></p>

<p>修改完毕后，使用以下命令来同步Identity服务数据库:</p>

<pre><code># su -s /bin/sh -c "keystone-manage db_sync" keystone
</code></pre>

<p>重启鉴权服务，删除Ubuntu使用的默认sqlite数据库, 并完成安装:</p>

<pre><code># service keystone restart
# rm -f /var/lib/keystone/keystone.db 
</code></pre>

<p>使用下列命令来激活cron任务，以便每小时判断tokens的存活时间:</p>

<pre><code># (crontab -l -u keystone 2&gt;&amp;1 | grep -q token_flush) || echo '@hourly /usr/bin/keystone-manage token_flush &gt;/var/log/keystone/keystone-tokenflush.log 2&gt;&amp;1' &gt;&gt; /var/spool/cron/crontabs/keystone
</code></pre>

<h4>创建tenants, users, roles</h4>

<pre><code># export OS_SERVICE_TOKEN=760bc221f4dc966693e5
# export OS_SERVICE_ENDPOINT=http://Controller:35357/v2.0
# keystone tenant-create --name admin --description "Admin Tenant"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |           Admin Tenant           |
|   enabled   |               True               |
|      id     | 6f5f440aa9de4b2fa205f43df073ddfa |
|     name    |              admin               |
+-------------+----------------------------------+
# keystone user-create --name admin --pass XXXXXXXXX --email xxxxxxxx@gmail.com
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |        XXXXXXXX@gmail.com        |
| enabled  |               True               |
|    id    | 7bc9be5493e345518a384383872ab274 |
|   name   |              admin               |
| username |              admin               |
+----------+----------------------------------+
# keystone role-create --name admin
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|    id    | 65b6ccaa3b434c848ccb757be43d6b41 |
|   name   |              admin               |
+----------+----------------------------------+
# keystone user-role-add --user admin --tenant admin --role admin
# keystone tenant-create --name demo --description "Demo Tenant"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |           Demo Tenant            |
|   enabled   |               True               |
|      id     | 459c25933274483fb01ce66d9514add6 |
|     name    |               demo               |
+-------------+----------------------------------+
# keystone user-create --name demo --tenant demo --pass xxxxx --email xxxxxxx@gmail.com
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |        xxxxxxx@gmail.com        |
| enabled  |               True               |
|    id    | b2f3d8a239b34edfb50fa67c5aca8f83 |
|   name   |               demo               |
| tenantId | 459c25933274483fb01ce66d9514add6 |
| username |               demo               |
+----------+----------------------------------+
# keystone tenant-create --name service --description "Service Tenant"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |          Service Tenant          |
|   enabled   |               True               |
|      id     | 08a675be93a04cca8a74159a3eefa288 |
|     name    |             service              |
+-------------+----------------------------------+
# keystone service-create --name keystone --type identity --description "OpenStack Identity"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |        OpenStack Identity        |
|   enabled   |               True               |
|      id     | bf7613d9563c47a9af80ecdb4f26f3f5 |
|     name    |             keystone             |
|     type    |             identity             |
+-------------+----------------------------------+
# keystone endpoint-create --service-id $(keystone service-list | awk '/ identity / {print $2}') --publicurl http://Controller:5000/v2.0 --internalurl http://Controller:5000/v2.0 --adminurl http://Controller:35357/v2.0 --region regionOne
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
|   adminurl  |   http://Controller:35357/v2.0   |
|      id     | c2c7a6c24b1d411b996f2e30fefc70b6 |
| internalurl |   http://Controller:5000/v2.0    |
|  publicurl  |   http://Controller:5000/v2.0    |
|    region   |            regionOne             |
|  service_id | bf7613d9563c47a9af80ecdb4f26f3f5 |
+-------------+----------------------------------+
</code></pre>

<p>验证, 详细的说明参见OpenStack官方文档:</p>

<pre><code># unset OS_SERVICE_TOKEN OS_SERVICE_ENDPOINT
# keystone --os-tenant-name admin --os-username admin --os-password xxxxx --os-auth-url http://Controller:35357/v2.0 token-get
+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
|  expires  |       2015-05-24T16:43:08Z       |
|     id    | 612b529c9c754b87a153abd39284aff6 |
| tenant_id | 6f5f440aa9de4b2fa205f43df073ddfa |
|  user_id  | 7bc9be5493e345518a384383872ab274 |
+-----------+----------------------------------+
# keystone --os-tenant-name admin --os-username admin --os-password xxxxx --os-auth-url http://Controller:35357/v2.0 tenant-list
+----------------------------------+---------+---------+
|                id                |   name  | enabled |
+----------------------------------+---------+---------+
| 6f5f440aa9de4b2fa205f43df073ddfa |  admin  |   True  |
| 459c25933274483fb01ce66d9514add6 |   demo  |   True  |
| 08a675be93a04cca8a74159a3eefa288 | service |   True  |
+----------------------------------+---------+---------+
# keystone --os-tenant-name admin --os-username admin --os-password xxxxx --os-auth-url http://Controller:35357/v2.0 user-list
+----------------------------------+-------+---------+--------------------+
|                id                |  name | enabled |       email        |
+----------------------------------+-------+---------+--------------------+
| 7bc9be5493e345518a384383872ab274 | admin |   True  | xxxxxxx@gmail.com |
| b2f3d8a239b34edfb50fa67c5aca8f83 |  demo |   True  | xxxxxxx@gmail.com |
+----------------------------------+-------+---------+--------------------+
# keystone --os-tenant-name admin --os-username admin --os-password xxxxx --os-auth-url http://Controller:35357/v2.0 role-list
+----------------------------------+----------+
|                id                |   name   |
+----------------------------------+----------+
| 9fe2ff9ee4384b1894a90878d3e92bab | _member_ |
| 65b6ccaa3b434c848ccb757be43d6b41 |  admin   |
+----------------------------------+----------+
# keystone --os-tenant-name demo --os-username demo --os-password xxxxx --os-auth-url http://controller:35357/v2.0 token-get
+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
|  expires  |       2015-05-24T16:46:34Z       |
|     id    | 0d8a9472b0f547dfabc62594b4fb146f |
| tenant_id | 459c25933274483fb01ce66d9514add6 |
|  user_id  | b2f3d8a239b34edfb50fa67c5aca8f83 |
+-----------+----------------------------------+
# keystone --os-tenant-name demo --os-username demo --os-password xxxxx --os-auth-url http://controller:35357/v2.0 user-list
You are not authorized to perform the requested action: admin_required (HTTP 403)
</code></pre>

<h3>创建脚本</h3>

<pre><code># cat admin-openrc.sh 
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=xxxxx
export OS_AUTH_URL=http://Controller:35357/v2.0
# cat demo-openrc.sh
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=xxxxx
export OS_AUTH_URL=http://Controller:5000/v2.0
</code></pre>

<p>下次使用时直接用<code>source admin-openrc.sh</code>或者<code>source demo-openrc.sh</code>即可。</p>

<h3>镜像服务</h3>

<p>添加镜像服务:</p>

<pre><code>root@Controller:~# mysql -u root -p
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 40
Server version: 5.5.43-MariaDB-1ubuntu0.14.04.2 (Ubuntu)

Copyright (c) 2000, 2015, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE glance;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'xxxxx';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'xxxxx';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; quit;
Bye
</code></pre>

<p>创建glance用户:</p>

<pre><code># source  /home/dash/admin-openrc.sh
# keystone user-create --name glance --pass xxxxx
+----------+----------------------------------+
| Property |              Value               |
+----------+----------------------------------+
|  email   |                                  |
| enabled  |               True               |
|    id    | a3108e4267154acd809f3978d360e6cd |
|   name   |              glance              |
| username |              glance              |
+----------+----------------------------------+
</code></pre>

<p>赋予glance用户admin权限:</p>

<pre><code># keystone user-role-add --user glance --tenant service --role admin
</code></pre>

<p>创建service entity和service end-point:</p>

<pre><code> keystone service-create --name glance --type image --description "OpenStack Image Service"
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
| description |     OpenStack Image Service      |
|   enabled   |               True               |
|      id     | 8736ca50fdf741afb5fcc2d078b1cd9b |
|     name    |              glance              |
|     type    |              image               |
+-------------+----------------------------------+
# keystone endpoint-create --service-id $(keystone service-list | awk '/ image / {print $2}') --publicurl http://Controller:9292 --internalurl http://Controller:9292 --adminurl http://Controller:9292 --region regionOne
+-------------+----------------------------------+
|   Property  |              Value               |
+-------------+----------------------------------+
|   adminurl  |      http://Controller:9292      |
|      id     | 340f40f0558c4a5b8fa88089aee69767 |
| internalurl |      http://Controller:9292      |
|  publicurl  |      http://Controller:9292      |
|    region   |            regionOne             |
|  service_id | 8736ca50fdf741afb5fcc2d078b1cd9b |
+-------------+----------------------------------+
</code></pre>

<p>安装服务组件:</p>

<pre><code># apt-get install glance python-glanceclient
</code></pre>

<p>配置：</p>

<pre><code># vim /etc/glance/glance-api.conf
[database]
...
connection = mysql://glance:GLANCE_DBPASS@controller/glance
[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = glance
admin_password = GLANCE_PASS
[paste_deploy]
...
flavor = keystone
[glance_store]
...
default_store = file
filesystem_store_datadir = /var/lib/glance/images/
[DEFAULT]
...
notification_driver = noop
[DEFAULT]
...
verbose = True
</code></pre>

<p>配置<code>/etc/glance/glance-registry.conf</code>文件，完成以下配置:</p>

<pre><code>[database]
...
connection = mysql://glance:GLANCE_DBPASS@controller/glance

[keystone_authtoken]
...
auth_uri = http://controller:5000/v2.0
identity_uri = http://controller:35357
admin_tenant_name = service
admin_user = glance
admin_password = GLANCE_PASS
[paste_deploy]
...
flavor = keystone
[DEFAULT]
...
notification_driver = noop
[DEFAULT]
...
notification_driver = noop
</code></pre>

<p>同步数据库:</p>

<pre><code># su -s /bin/sh -c "glance-manage db_sync" glance
</code></pre>

<p>重启服务，删除默认的sqlite数据库:</p>

<pre><code># service glance-registry restart
# service glance-api restart
# rm -f /var/lib/glance/glance.sqlite
</code></pre>

<p>验证:</p>

<pre><code># wget http://download.cirros-cloud.net/0.3.3/cirros-0.3.3-x86_64-disk.img
# source ~/admin-openrc.sh
# glance image-create --name "cirros-0.3.3-x86_64" --file ~/cirros-0.3.3-x86_64-disk.img --disk-format qcow2 --container-format bare --is-public True --progress
[=============================&gt;] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 133eae9fb1c98f45894a4e60d8736619     |
| container_format | bare                                 |
| created_at       | 2015-05-24T16:25:32                  |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 3d45ea58-731c-4eb5-bf30-db1b4bfe4f57 |
| is_public        | True                                 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros-0.3.3-x86_64                  |
| owner            | 6f5f440aa9de4b2fa205f43df073ddfa     |
| protected        | False                                |
| size             | 13200896                             |
| status           | active                               |
| updated_at       | 2015-05-24T16:25:32                  |
| virtual_size     | None                                 |
+------------------+--------------------------------------+
# glance image-list
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
| ID                                   | Name                | Disk Format | Container Format | Size     | Status |
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
| 3d45ea58-731c-4eb5-bf30-db1b4bfe4f57 | cirros-0.3.3-x86_64 | qcow2       | bare             | 13200896 | active |
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
</code></pre>

<p>控制节点基本上配置成功，明天继续。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三节点搭建OpenStack Juno(1)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/24/san-jie-dian-da-jian-openstack-juno/"/>
    <updated>2015-05-24T14:36:34+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/24/san-jie-dian-da-jian-openstack-juno</id>
    <content type="html"><![CDATA[<h3>目的</h3>

<p>最近在研究解耦OpenStack，以及OpenStack的各种网络模型，下面是一个最简单的用于搭建OpenStack Juno的过程。</p>

<h3>硬件及网络准备</h3>

<h4>物理服务器</h4>

<p>物理服务器:  i5-4460/32G 内存，128G SSD+3T IDE，事实上这个教程跑完你也用不到这么强悍的配置，理论上在8G的物理机器上就可以运行完本文。  <br/>
物理服务器操作系统: Ubuntu14.04</p>

<h4>虚拟机：</h4>

<p>虚拟机1, Controller: 1 processor, 2 GB memory, and 5 GB storage.  <br/>
虚拟机2, Network: 1 processor, 512 MB memory, and 5 GB storage.  <br/>
虚拟机3, Compute: 1 processor, 2 GB memory, and 10 GB storage.</p>

<h4>网络规划</h4>

<p>Management: 10.55.55.0/24, 只用于管理的网络，公网无法访问。简单来说，这个网络用于OpenStack各个组件之间的相互通信。      <br/>
Tunnel: 10.66.66.0/24, 用于计算节点和网络节点之间的通信。这个隧道使得虚拟机的实例可以和相互通信。  <br/>
External: 192.168.1.0/24, 用于虚拟机实例的Internet访问。 <br/>
当然我们可以添加额外的存储网络，这里为了简单起见我们不使用cinder服务，使用单纯的虚拟机镜像即可。</p>

<h4>节点网络名规划</h4>

<p>Controller节点:    controller.openstack.local, 10.55.55.2(管理网络), N/A, N/A.  <br/>
Network节点:    Network.openstack.local, 10.55.55.3(管理网络), 10.66.66.3(隧道网络), 192.168.1.3(Internet公网).
Compute节点:    Compute.openstack.local, 10.55.55.4(管理网络), 10.66.66.4(隧道网络).</p>

<p>一个参考的例图如下:  <br/>
<img src="/images/2015_05_24_14_50_37_629x551.jpg" alt="/images/2015_05_24_14_50_37_629x551.jpg" /></p>

<p>按照上述的描述我们创建三台虚拟机，并进行初始化配置。</p>

<h3>虚拟机初始化配置</h3>

<p>下面罗列的代码是我在本机上的创建过程，仅供参考:</p>

<pre><code>$ pwd
/media/repo/Image/3NodeOpenStack
$ qemu-img create -f qcow2 -b /media/repo/Image/UbuntuBase.qcow2 OpenStackController.qcow2
Formatting 'OpenStackController.qcow2', fmt=qcow2 size=107374182400 backing_file='/media/repo/Image/UbuntuBase.qcow2' encryption=off cluster_size=65536 lazy_refcounts=off 
$ qemu-img create -f qcow2 -b /media/repo/Image/UbuntuBase.qcow2 OpenStackNetwork.qcow2
Formatting 'OpenStackNetwork.qcow2', fmt=qcow2 size=107374182400 backing_file='/media/repo/Image/UbuntuBase.qcow2' encryption=off cluster_size=65536 lazy_refcounts=off 
$ qemu-img create -f qcow2 -b /media/repo/Image/UbuntuBase.qcow2 OpenStackCompute.qcow2
Formatting 'OpenStackCompute.qcow2', fmt=qcow2 size=107374182400 backing_file='/media/repo/Image/UbuntuBase.qcow2' encryption=off cluster_size=65536 lazy_refcounts=off 
</code></pre>

<p>创建虚拟机的时候， OpenStackCompute节点需要把CPU的参数带下去，如下图所示:  <br/>
<img src="/images/2015_05_24_15_00_07_517x483.jpg" alt="/images/2015_05_24_15_00_07_517x483.jpg" /></p>

<p>各个节点的network定义文件如下:</p>

<p>控制节点:</p>

<pre><code># The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 10.55.55.2
netmask 255.255.255.0
gateway 10.55.55.1
dns-nameservers 114.114.114.114
</code></pre>

<p>网络节点:</p>

<pre><code># The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 10.55.55.3
netmask 255.255.255.0
gateway 10.55.55.1
dns-nameservers 114.114.114.114


auto eth1
iface eth1 inet static
address 10.66.66.3
netmask 255.255.255.0

auto eth2
iface eth2 inet static
address 192.168.1.3
netmask 255.255.255.0
</code></pre>

<p>计算节点:</p>

<pre><code>auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 10.55.55.4
netmask 255.255.255.0
gateway 10.55.55.1
dns-nameservers 114.114.114.114

auto eth1
iface eth1 inet static
address 10.66.66.4
netmask 255.255.255.0
</code></pre>

<p>每个节点分别更改其<code>/etc/hostname</code>为对应的名字，而每台机器上的<code>/etc/hosts</code>也做对应的修改，例如Controlle节点上的例子如下:   <br/>
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat /etc/hosts
</span><span class='line'>127.0.0.1       localhost
</span><span class='line'>127.0.1.1       Controller
</span><span class='line'>10.55.55.2      Controller
</span><span class='line'>10.55.55.3      Network
</span><span class='line'>10.55.55.4      Compute
</span><span class='line'>&hellip;&hellip;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;$ cat /etc/hostname
</span><span class='line'>Controller&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;网络配置完毕后，保证可以通过`ping Controller`等命令达到互通。    
</span><span class='line'>
</span><span class='line'>在进入到后续步骤前，更新所有节点到最新状态:     
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;$ sudo apt-get update &amp;&amp; sudo apt-get upgrade &amp;&amp; sudo apt-get dist-upgrade &amp;&amp; sudo reboot</span></code></pre></td></tr></table></div></figure></p>

<h3>NTP 服务器/客户端配置</h3>

<p>使用NTP来保证各个节点之间的时间同步，对后续加入的各个节点，同样需要使用NTP来同步该节点时间。我们将Controller作为NTP服务器,在Controller上，安装和配置NTP服务器：</p>

<h4>NTP服务器</h4>

<pre><code># apt-get -y install ntp
# vim /etc/ntp.conf
    # 修改成大陆时间
    server 2.cn.pool.ntp.org
    server 1.asia.pool.ntp.org
    server 2.asia.pool.ntp.org
    # 修改 restrict 設定
    restrict -4 default kod notrap nomodify
    restrict -6 default kod notrap nomodify
# service ntp restart
</code></pre>

<h4>NTP客户端</h4>

<p>其他的节点上都需要安装NTP客户端并使用NTP服务器时间同步。</p>

<pre><code># apt-get -y install ntp
# vim /etc/ntp.conf
    # 設定 controller 為參照的 time server
    # 並將其他 server 開頭的設定進行註解
    server 10.55.55.2 iburst
# service ntp restart
</code></pre>

<p>检查结果是否正确:</p>

<pre><code>root@JunoNetwork:~# ntpq -c peers
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 Controller  59.106.180.168   3 u    1   64    1    0.239  447024.   0.049
</code></pre>

<p>接下来真正进入OpenStack的安装和配置过程。</p>

<h3>源设定</h3>

<p>Juno的源没有被包含在Ubuntu14.04的官方源中(官方源中版本为IceHouse)，所以通过下列命令添加OpenStack Juno源:   <br/>
所有节点上(Controller,Network,Compute):</p>

<pre><code>$ sudo apt-get install ubuntu-cloud-keyring
$ sudo bash
# echo "deb http://ubuntu-cloud.archive.canonical.com/ubuntu" "trusty-updates/juno main" &gt; /etc/apt/sources.list.d/cloudarchive-juno.list
# apt-get update &amp;&amp; apt-get -y dist-upgrade
</code></pre>

<p>第一部分就先到这里。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Change Cobbler Profile for Using Local Repository]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/22/change-cobbler-profile-for-using-local-repository/"/>
    <updated>2015-05-22T14:12:58+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/22/change-cobbler-profile-for-using-local-repository</id>
    <content type="html"><![CDATA[<h3>Cobbler Profiles</h3>

<p>For getting the profiles and get the detailed information of the profile.</p>

<pre><code># cobbler profile list
   ubuntu1404-x86_64
# cobbler profile help
usage
=====
cobbler profile add
cobbler profile copy
cobbler profile dumpvars
cobbler profile edit
cobbler profile find
cobbler profile getks
cobbler profile list
cobbler profile remove
cobbler profile rename
cobbler profile report
# cobbler profile report ubuntu1404-x86_64
......
Kickstart                      : /var/lib/cobbler/kickstarts/sample.seed
......
</code></pre>

<h3>Use Local Repository</h3>

<p>For adding the repository via following command, you could use your local repository:</p>

<pre><code>$ sudo cobbler repo add --name=local-trusty --breed=apt --arch=x86_64 --mirror=http://xxxxxxxxxxxxxx/ubuntu --apt-components=main,restricted,universe,multiverse --apt-dists=trusty,trusty-updates,trusty-security
$ sudo cobbler repo sync
</code></pre>

<p>After syncing, the folder will contains all of the packages:</p>

<pre><code># pwd
/var/www/cobbler/ks_mirror/ubuntu1404-x86_64
# ls
boot  dists  doc  EFI  install  isolinux  md5sum.txt  pics  pool  preseed  README.diskdefines  ubuntu
</code></pre>

<p>Edit the sample seed via:</p>

<pre><code># cp /var/lib/cobbler/kickstarts/sample.seed /var/lib/cobbler/kickstarts/local.seed
# vim /var/lib/cobbler/kickstarts/local.seed

    d-i mirror/http/hostname string $http_server
    # d-i mirror/http/directory string $install_source_directory
    d-i mirror/http/directory string /cobbler/ks_mirror/ubuntu1404-x86_64/ubuntu/
</code></pre>

<p>Then Modify the profile&rsquo;s kickstart via:</p>

<pre><code>#  cobbler profile edit --name=ubuntu1404-x86_64 --kickstart=/var/lib/cobbler/kickstarts/local.seed
</code></pre>

<p>After modification, next time if you re-install the compute, it will directly get the packages from the local repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My Configuration on Cobbler for Deploying Ubuntu12.04]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/05/18/my-configuration-on-cobbler-for-deploying-ubuntu12-dot-04/"/>
    <updated>2015-05-18T18:15:47+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/05/18/my-configuration-on-cobbler-for-deploying-ubuntu12-dot-04</id>
    <content type="html"><![CDATA[<p>Configuration file for preseed, put it under: /var/lib/cobbler/kickstarts/autoinstall.seed:</p>

<pre><code># BASIC
d-i  debian-installer/locale    string en_US.UTF-8
d-i  debian-installer/splash    boolean false
d-i  console-setup/ask_detect   boolean false
d-i  console-setup/layoutcode   string us
d-i  console-setup/variantcode  string
d-i  clock-setup/utc            boolean true
d-i  clock-setup/ntp            boolean true

# DISKPART
d-i  partman-auto/method                string regular
d-i  partman-lvm/device_remove_lvm      boolean true
d-i  partman-lvm/confirm                boolean true
d-i  partman/confirm_write_new_label    boolean true
d-i  partman/choose_partition           select Finish partitioning and write changes to disk
d-i  partman/confirm                    boolean true
d-i  partman/confirm_nooverwrite        boolean true
d-i  partman/default_filesystem         string ext3

# SOFTWARE
# /var/www/cobbler/ks_mirror/Ubuntu12.04-x86_64/ubuntu/
d-i  mirror/country             string manual
d-i  mirror/http/hostname       string $http_server
d-i  mirror/http/directory      string /cobbler/ks_mirror/Ubuntu12.04-x86_64/ubuntu
d-i  mirror/http/proxy          string
d-i  apt-setup/security_host    string $http_server
d-i  apt-setup/security_path    string /cobbler/ks_mirror/Ubuntu12.04-x86_64/ubuntu
d-i  apt-setup/services-select  multiselect none
d-i  pkgsel/upgrade             select none
d-i  pkgsel/language-packs      multiselect
d-i  pkgsel/update-policy       select none
d-i  pkgsel/updatedb            boolean true
d-i  pkgsel/include             string openssh-server

# USER
d-i  passwd/root-login                  boolean true
d-i  passwd/make-user                   boolean false
d-i  passwd/root-password               password root
d-i  passwd/root-password-again         password root
d-i  user-setup/allow-password-weak     boolean true

# FINISH
d-i  grub-installer/skip                boolean false
d-i  lilo-installer/skip                boolean false
d-i  grub-installer/only_debian         boolean true
d-i  grub-installer/with_other_os       boolean true
d-i  finish-install/keep-consoles       boolean false
d-i  finish-install/reboot_in_progress  note
d-i  cdrom-detect/eject                 boolean true
d-i  debian-installer/exit/halt         boolean false
d-i  debian-installer/exit/poweroff     boolean false

# EXTRA
d-i  preseed/late_command       string echo "UseDNS no" &gt;&gt; /target/etc/ssh/sshd_config
</code></pre>

<p>Edit the profile via:</p>

<pre><code>$ cobbler profile list
$ cobbler profile edit --name=Ubuntu12.04-x86_64 --kickstart=/var/lib/cobbler/kickstarts/autoinstall.seed 
</code></pre>

<p>Now select the installation, your installation will use local repository.</p>
]]></content>
  </entry>
  
</feed>
