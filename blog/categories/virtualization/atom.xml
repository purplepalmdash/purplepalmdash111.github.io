<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2016-07-05T22:38:56+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[XenServer Statistics]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/07/01/xenserver-statistics/"/>
    <updated>2016-07-01T17:45:50+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/07/01/xenserver-statistics</id>
    <content type="html"><![CDATA[<p>Direct write rrd into graphite, refers to:</p>

<pre><code>$ git clone https://github.com/jgilmour/XenGraphiteIT.git
</code></pre>

<p>Then you get the storage pool information fro xsconsole via:</p>

<pre><code>$ xe vdi-list
</code></pre>

<p>Notice it will contain the hard disk and iso repositories, use harddisk.</p>

<p>Now edit the .config file:</p>

<pre><code>[XENAPI]
URL = http://192.168.10.187
USERNAME = root
PASSWORD = xxxxxxx
SR-UUID = 51977c4b-8dc2-bcff-b7ad-de7cc5c7e717

[GRAPHITE]
CARBON_HOST = 192.168.1.79
CARBON_PORT = 2003
CARBON_NAME = collectd.com.IT.servers.xen.
</code></pre>

<p>Run <code>python2 xengraphite.py</code> you could get your XenServer statistic data into your
graphite database, enjoy it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Site-to-site VPN]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/06/29/site-to-site-vpn/"/>
    <updated>2016-06-29T18:51:40+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/06/29/site-to-site-vpn</id>
    <content type="html"><![CDATA[<h3>Reference</h3>

<p>Refers to:</p>

<p><a href="https://clauseriksen.net/2011/02/02/ipsec-on-debianubuntu/">https://clauseriksen.net/2011/02/02/ipsec-on-debianubuntu/</a>  <br/>
And
<a href="http://xmodulo.com/create-site-to-site-ipsec-vpn-tunnel-openswan-linux.html">http://xmodulo.com/create-site-to-site-ipsec-vpn-tunnel-openswan-linux.html</a></p>

<h3>Network Topology</h3>

<p>The topology is listed as following:</p>

<p>Host1 &ndash; LAN1 &ndash; Router1 &ndash;[BIG, BAD INTERNET]&ndash; Router2 &ndash; LAN2 &ndash; Host2</p>

<p>Router1 and Router2 are Ubuntu14.04 machine, which runs in virt-manager,thus you have
to create 2 new networks, each in one physical machine.</p>

<p>Physical Machine 1: 192.168.1.79  <br/>
Router1:   <br/>
eth0: bridge to physical machine&rsquo;s networking. 192.168.10.100   <br/>
eth1: 10.47.70.2.  <br/>
DHCP on eth1.</p>

<p>Physical Machine 2: 192.168.1.69  <br/>
Router2:   <br/>
eth0: bridge to physical machine&rsquo;s networking. 192.168.10.200   <br/>
eth1: 10.47.67.2.  <br/>
DHCP on eth1.</p>

<h3>Router Network Configuration</h3>

<p>Router1&rsquo;s networking configuration:</p>

<pre><code>$ vim /etc/network/interfaces
    # The loopback network interface
    auto lo
    iface lo inet loopback

    # The primary network interface
    auto eth0
    iface eth0 inet static
    address 192.168.10.100
    netmask 255.255.0.0
    gateway 192.168.0.176
    dns-nameservers 223.5.5.5

    auto eth1
    iface eth1 inet static
    address 10.47.70.2
    netmask 255.255.255.0
</code></pre>

<p>Router2&rsquo;s networking configuration:</p>

<pre><code>$ vim /etc/network/interfaces
    # The loopback network interface
    auto lo
    iface lo inet loopback

    # The primary network interface
    auto eth0
    iface eth0 inet static
    address 192.168.10.200
    netmask 255.255.0.0
    gateway 192.168.0.176
    dns-nameservers 223.5.5.5
    auto eth1
    iface eth1 inet static
    address 10.47.67.2
    netmask 255.255.255.0
</code></pre>

<p>After configuration , restart the Router1 and Router2.</p>

<h3>IPSEC Configuration</h3>

<h4>Router1</h4>

<p>Install following package:</p>

<pre><code>$ sudo apt-get install -y openswan
</code></pre>

<p>Append following lines at the end of <code>/etc/sysctl.conf</code>,then run <code>sysctl -p
/etc/sysctl.conf</code> to take effects.</p>

<pre><code>$ vim /etc/sysctl.conf
net.ipv4.ip_forward=1
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.default.send_redirects = 0
net.ipv4.conf.eth0.send_redirects = 0
net.ipv4.conf.default.accept_redirects = 0
net.ipv4.conf.eth0.accept_redirects = 0
</code></pre>

<p>Also you have to disable the redirects via following commands:</p>

<pre><code>for vpn in /proc/sys/net/ipv4/conf/*;
do echo 0 &gt; $vpn/accept_redirects;
echo 0 &gt; $vpn/send_redirects;
done
</code></pre>

<p>iptables rules should be done via following:</p>

<pre><code>iptables -A INPUT -p udp --dport 500 -j ACCEPT
iptables -A INPUT -p tcp --dport 4500 -j ACCEPT
iptables -A INPUT -p udp --dport 4500 -j ACCEPT
iptables -t nat -A POSTROUTING -s 10.47.70.0/24 -d 10.47.67.0/24 -j SNAT --to 192.168.10.100
#iptables -t nat -A POSTROUTING -s site-A-private-subnet -d site-B-private-subnet -j SNAT --to site-A-Public-IP
iptables -A POSTROUTING -t nat -d 10.47.70.0/24 -o eth1 -m policy --dir out --pol ipsec -j ACCEPT
iptables -A INPUT -m policy --dir in --pol ipsec -j ACCEPT
iptables -A INPUT -p udp -m multiport --dports 500,4500 -j ACCEPT
iptables -A INPUT -p esp -j ACCEPT
iptables -A FORWARD -m policy --dir in --pol ipsec -j ACCEPT
</code></pre>

<p>Now continue to configure the ipsec:</p>

<pre><code>$ sudo vim /etc/ipsec.conf
    ## general configuration parameters ##

    config setup
            plutodebug=all
            plutostderrlog=/var/log/pluto.log
            protostack=netkey
            nat_traversal=yes
            virtual_private=%v4:10.0.0.0/8,%v4:192.168.0.0/16,%v4:172.16.0.0/12
            ## disable opportunistic encryption in Red Hat ##
            oe=off

    ## disable opportunistic encryption in Debian ##
    ## Note: this is a separate declaration statement ##
    #include /etc/ipsec.d/examples/no_oe.conf 

    ## connection definition in Debian ##
    conn demo-connection-debian
            authby=secret
            auto=start
            ## phase 1 ##
            keyexchange=ike
            ## phase 2 ##
            esp=3des-md5
            pfs=yes
            type=tunnel
            left=192.168.10.100
            leftsourceip=192.168.10.100
            leftsubnet=10.47.70.0/24
            ## for direct routing ##
            #leftsubnet=192.168.10.100/32
            #leftnexthop=%defaultroute
            leftnexthop=192.168.10.200
            right=192.168.10.200
            rightsubnet=10.47.67.0/24
</code></pre>

<p>Notice the left/right configuration, should corresponding the our definition
of the networking.</p>

<p>Now generate the pre-shared keys via:</p>

<pre><code>$ dd if=/dev/random count=24 bs=1 | xxd -ps
24+0 records in
24+0 records out
24 bytes copied, 4.5529e-05 s, 527 kB/s
cece1b0ffe27f82c27efc94339f08c418abb9e5f5c0d5bf5
</code></pre>

<p>the <code>cece1b0ffe27f82c27efc94339f08c418abb9e5f5c0d5bf5</code> is the keys we want to
fill into the secrets:</p>

<pre><code>$ sudo cat /etc/ipsec.secrets 
    # This file holds shared secrets or RSA private keys for inter-Pluto
    # authentication.  See ipsec_pluto(8) manpage, and HTML documentation.

    # RSA private key for this host, authenticating it to any other host
    # which knows the public part.  Suitable public keys, for ipsec.conf, DNS,
    # or configuration of other implementations, can be extracted conveniently
    # with "ipsec showhostkey".

    # this file is managed with debconf and will contain the automatically created RSA keys
    include /var/lib/openswan/ipsec.secrets.inc
    192.168.10.100  192.168.10.200:  PSK  "cece1b0ffe27f82c27efc94339f08c418abb9e5f5c0d5bf5"
</code></pre>

<p>Now Router1 is configured, we continue to configure Router2.</p>

<h4>Router2</h4>

<p>Ipsec and sysctl are the same as in Router1, the iptables scripts is listed as:</p>

<pre><code>iptables -A INPUT -p udp --dport 500 -j ACCEPT
iptables -A INPUT -p tcp --dport 4500 -j ACCEPT
iptables -A INPUT -p udp --dport 4500 -j ACCEPT
iptables -t nat -A POSTROUTING -s 10.47.67.0/24 -d 10.47.70.0/24 -j SNAT --to 192.168.10.200

#iptables -A POSTROUTING -t nat -d 192.168.1.0/24 -o eth0 -m policy --dir out --pol ipsec -j ACCEPT
iptables -A POSTROUTING -t nat -d 10.47.67.0/24 -o eth1 -m policy --dir out --pol ipsec -j ACCEPT
iptables -A INPUT -m policy --dir in --pol ipsec -j ACCEPT
iptables -A INPUT -p udp -m multiport --dports 500,4500 -j ACCEPT
iptables -A INPUT -p esp -j ACCEPT
iptables -A FORWARD -m policy --dir in --pol ipsec -j ACCEPT
</code></pre>

<p>Now configure the ipsec.conf like following:</p>

<pre><code>$ sudo vim /etc/ipsec.conf
## general configuration parameters ##

config setup
        plutodebug=all
        plutostderrlog=/var/log/pluto.log
        protostack=netkey
        nat_traversal=yes
        virtual_private=%v4:10.0.0.0/8,%v4:192.168.0.0/16,%v4:172.16.0.0/12
        ## disable opportunistic encryption in Red Hat ##
        oe=off

## disable opportunistic encryption in Debian ##
## Note: this is a separate declaration statement ##
#include /etc/ipsec.d/examples/no_oe.conf 

## connection definition in Debian ##
conn demo-connection-debian
        authby=secret
        auto=start
        ## phase 1 ##
        keyexchange=ike
        ## phase 2 ##
        esp=3des-md5
        pfs=yes
        type=tunnel
        left=192.168.10.200
        leftsourceip=192.168.10.200
        leftsubnet=10.47.67.0/24
        ## for direct routing ##
        #leftsubnet=192.168.10.200/32
        #leftnexthop=%defaultroute
        leftnexthop=192.168.10.100
        right=192.168.10.100
        rightsubnet=10.47.70.0/24
</code></pre>

<p>Notice the definition&rsquo;s differences comparing to Router1.</p>

<p>The ipsec.secrets is the same as Router1, but you have to change like following:</p>

<pre><code>$ sudo vim /etc/ipsec.secrets
192.168.10.200  192.168.10.100:  PSK  "3030804556207bde9fc5c9a043c6ac13fce136ce41eb98a6"
</code></pre>

<h3>Examine</h3>

<p>Restart the ipsec services on both Router.</p>

<pre><code>$ sudo  /etc/init.d/ipsec restart
</code></pre>

<p>Examine the route via:</p>

<pre><code>adminubuntu@vpn1:~$ ip route
default via 192.168.0.176 dev eth0 
10.47.67.0/24 dev eth0  scope link  src 192.168.10.100 
10.47.70.0/24 dev eth1  proto kernel  scope link  src 10.47.70.2 
192.168.0.0/16 dev eth0  proto kernel  scope link  src 192.168.10.100 
adminubuntu@vpn2:~$ ip route
default via 192.168.0.176 dev eth0 
10.47.67.0/24 dev eth1  proto kernel  scope link  src 10.47.67.2 
10.47.70.0/24 dev eth0  scope link  src 192.168.10.200 
192.168.0.0/16 dev eth0  proto kernel  scope link  src 192.168.10.200 
</code></pre>

<p>So we can see the route shows the connection of the vpn.</p>

<p>Now examine the ipsec status:</p>

<pre><code>$ sudo service ipsec status
IPsec running  - pluto pid: 930
pluto pid 930
1 tunnels up
some eroutes exist
</code></pre>

<p>More detailed infos could be examine via: <code>sudo ipsec auto --status</code>.</p>

<h3>DHCP Server</h3>

<p>Install dhcpd and configure it via following command:</p>

<pre><code>$ sudo apt-get install -y isc-dhcp-server
$ sudo vim /etc/default/isc-dhcp-server
INTERFACES="eth1"
</code></pre>

<p>Append following lines to <code>/etc/dhcp/dhcpd.conf</code>:  <br/>
Router1:</p>

<pre><code>subnet
10.47.70.0 netmask 255.255.255.0 {
# --- default gateway
option routers
10.47.70.2;
# --- Netmask
option subnet-mask
255.255.255.0;
# --- Broadcast Address
option broadcast-address
10.47.70.255;
# --- Domain name servers, tells the clients which DNS servers to use.
option domain-name-servers
223.5.5.5,180.76.76.76;
option time-offset 0;
range 10.47.70.3 10.47.70.254;
default-lease-time 1209600;
max-lease-time 1814400;
}
</code></pre>

<p>Router2:</p>

<pre><code>subnet
10.47.67.0 netmask 255.255.255.0 {
# --- default gateway
option routers
10.47.67.2;
# --- Netmask
option subnet-mask
255.255.255.0;
# --- Broadcast Address
option broadcast-address
10.47.67.255;
# --- Domain name servers, tells the clients which DNS servers to use.
option domain-name-servers
223.5.5.5,180.76.76.76;
option time-offset 0;
range 10.47.67.3 10.47.67.254;
default-lease-time 1209600;
max-lease-time 1814400;
}
</code></pre>

<p>Now your subnet is ready, restart the Router1 and Router2, next step we will
verify our site-to-site VPN.</p>

<h3>Verification</h3>

<p>Create 2 new vm on 2 physical machine, each of them attached to our Router&rsquo;s
eth1 networking. I use tinycore for experiment.</p>

<p>Tinycore Attaches to Router1:  <br/>
<img src="/images/2016_06_29_19_23_50_469x212.jpg" alt="/images/2016_06_29_19_23_50_469x212.jpg" />  <br/>
Tinycore Attaches to Router2: <br/>
<img src="/images/2016_06_29_19_25_18_497x351.jpg" alt="/images/2016_06_29_19_25_18_497x351.jpg" /></p>

<p>The picture also shows the ping each other without any problem.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RackHD Worktips]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/06/07/rackhd-worktips/"/>
    <updated>2016-06-07T17:05:19+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/06/07/rackhd-worktips</id>
    <content type="html"><![CDATA[<h3>Vagrant Preparation</h3>

<p>rackhd/rackhd vagrant box could be downloaded from following link:</p>

<p><a href="https://atlas.hashicorp.com/rackhd/boxes/rackhd">https://atlas.hashicorp.com/rackhd/boxes/rackhd</a></p>

<p>Clone the repository from the github:</p>

<pre><code>$ pwd
/home/dash/Code/Jun13
$ git clone https://github.com/RackHD/RackHD
$ cd RackHD
</code></pre>

<p>Change into the directory example, create config and run the setup command:</p>

<pre><code>$ cd example
$ cp config/monorail_rack.cfg.example config/monorail_rack.cfg
</code></pre>

<p>Edits can be made to this new file to adjust the number of pxe clients created.</p>

<pre><code>$ bin/monorail_rack
</code></pre>

<p>The <code>monorail_rack</code> script will auto-start all of the services by default, but you can also run them manually if you prefer.</p>

<pre><code>$ vagrant ssh
vagrant:~$ sudo nf start
</code></pre>

<p>Unfortunately, the vagrant machine won&rsquo;t work due to bad networking.</p>

<h3>Customization Deployment</h3>

<p>Use a trusty based vagrant box for creating the rackhd node.</p>

<pre><code>$ vagrant init trustyvirtualbox
$ vim Vagrantfile
</code></pre>

<p>Vagrantfile&rsquo;s configuration modification is listed as following:</p>

<pre><code>Vagrant.configure(2) do |config|
  # The most common configuration options are documented and commented below.
  # For a complete reference, please see the online documentation at
  # https://docs.vagrantup.com.

  # Added more disks
+   file_to_disk = File.realpath( "." ).to_s + "/disk.vdi"

  # config.vm.network "private_network", ip: "192.168.33.10"
+   config.vm.network "private_network", ip: "172.31.128.1", virtualbox__intnet:  "closednet"

+  config.vm.provider "virtualbox" do |vb|
+    if ARGV[0] == "up" &amp;&amp; ! File.exist?(file_to_disk) 
+      puts "Creating 5GB disk #{file_to_disk}."
+      vb.customize [
+        'createhd', 
+        '--filename', file_to_disk, 
+        '--format', 'VDI', 
+        '--size', 5000 * 1024 # 5 GB
+      ] 
+      vb.customize [
+        'storageattach', :id, 
+        '--storagectl', 'IDE Controller', 
+        '--port', 1, '--device', 0, 
+        '--type', 'hdd', '--medium', 
+        file_to_disk
+      ] 
+    end
+    vb.memory = "4096"
+    vb.cpus = 2
+    vb.customize ["modifyvm", :id, "--nicpromisc2", "allow-all", "--ioapic", "on"]
+  end
</code></pre>

<p>Then <code>vagrant up</code> for start up the machine.</p>

<p>Notice: the controller of the disk should be noticed very carefully, you could choose
&ldquo;IDE Controller&rdquo; Or &ldquo;SATA Controller&rdquo;, depending on your virtualbox configuration.  <br/>
Then follow the tips on:</p>

<p><a href="http://purplepalmdash.github.io/blog/2016/06/01/use-rakehd-for-deploying-systems/">http://purplepalmdash.github.io/blog/2016/06/01/use-rakehd-for-deploying-systems/</a>  <br/>
Extend the root partition of vagrant disk via:</p>

<pre><code>$ sudo pvcreate /dev/sdb
$ sudo vgextend vagrant-vg /dev/sdb
$ sudo lvextend -l +100%FREE /dev/vagrant-vg/root
$ sudo resize2fs  /dev/vagrant-vg/root
</code></pre>

<h3>Adding Ubuntu Deployment</h3>

<p>Install <code>apt-mirror</code> first, then using following mirror configuration file:</p>

<pre><code>$ vim /etc/apt/mirror.list
############# config ##################
#
# set base_path    /var/spool/apt-mirror
#
# set mirror_path  $base_path/mirror
# set skel_path    $base_path/skel
# set var_path     $base_path/var
# set cleanscript $var_path/clean.sh
# set defaultarch  &lt;running host architecture&gt;
# set postmirror_script $var_path/postmirror.sh
# set run_postmirror 0
set base_path   /var/mirrors/ubuntu/14.04
set nthreads     20
set _tilde 0
#
############# end config ##############

deb-amd64 http://mirrors.aliyun.com/ubuntu  trusty main main/debian-installers
deb http://mirrors.aliyun.com/ubuntu    trusty main/installer-amd64
deb-amd64 http://mirrors.aliyun.com/ubuntu  trusty-updates main
deb-amd64 http://mirrors.aliyun.com/ubuntu  trusty-security main
clean http://mirrors.aliyun.com/ubuntu
</code></pre>

<p>Also you have to create following script for downloading the debian-installer:</p>

<pre><code>$ vim /var/mirrors/ubuntu/14.04/var/postmirror.sh 
#!/bin/sh -x 
# the udebs script gets the actual files we need 
#/mnt/repo/apt-mirror/var/udebs.sh  
# A quick apt directory structure primer: 
# an apt server (e.g. archive.ubuntu.com) contains repositories (e.g. trusty-backports), 
# which contain archives (e.g. multiverse), which contain directories 
# a complete example - http://archive.ubuntu.com/ubuntu/dists/trusty-backports/multiverse/debian-installer/  
# With this in mind, we create bash 'arrays' of the structure: 
# server we're syncing against 
#MIRROR="cn.archive.ubuntu.com" 
MIRROR="archive.ubuntu.com" 
# repositories we're mirroring 
#REPOS="trusty trusty-updates trusty-security trusty-proposed trusty-backports" 
REPOS="trusty"
# archives in repositories 
#ARCHIVES="main multiverse restricted universe" 
ARCHIVES="main"
# installer location inside archive 
#DIRECTORIES="debian-installer dist-upgrader-all installer-amd64 installer-i386" 
DIRECTORIES="debian-installer installer-amd64"
#where we're storing it locally 
LOCALDIR="/var/mirrors/ubuntu/14.04/mirror/mirrors.aliyun.com"
#LOCALDIR="/mnt/repo/apt-mirror/mirror/archive.ubuntu.com"  
for REPO in $REPOS; do 
for ARCHIVE in $ARCHIVES; do 
for DIRECTORY in $DIRECTORIES;do 
# create directory structure 
if [ ! -e "$LOCALDIR/ubuntu/dists/$REPO/$ARCHIVE/$DIRECTORY" ]; then
mkdir -p "$LOCALDIR/ubuntu/dists/$REPO/$ARCHIVE/$DIRECTORY"
fi
# do the sync 
rsync --recursive --times --links --hard-links --delete --delete-after \
rsync://$MIRROR/ubuntu/dists/$REPO/$ARCHIVE/$DIRECTORY/ $LOCALDIR/ubuntu/dists/$REPO/$ARCHIVE/$DIRECTORY
done
done
done
</code></pre>

<p>Now run <code>sudo apt-mirror</code> for syncing the repository to local storage.</p>

<p>Also create a shortcut to the repository in RackHD System:</p>

<pre><code>$ sudo ln -s /var/mirrors/ubuntu/14.04/mirror/mirrors.aliyun.com/ubuntu/ /opt/monorail/static/http/
</code></pre>

<p>Now restart the rackhd node, the ubuntu deployment is ready for use.</p>

<h3>Ubuntu Deployment</h3>

<p>Add the json file which holds the ubuntu deployment:</p>

<pre><code>$ pwd
/home/vagrant/RackHD/example
$ vim samples/ubuntu_boot.json 
{
    "name": "Graph.InstallUbuntu",
    "options": {
        "defaults": {
            "obmServiceName": "noop-obm-service"
        },
        "install-os": {
            "repo": "/ubuntu",
            "rootPassword": "ubuntu",
            "profile": "install-trusty.ipxe",
            "completionUri": "renasar-ansible.pub"
        }
    }
}
</code></pre>

<p>In fact the <code>rootPassword</code> is not ready for use, the real password after deployment
 is <code>RackHDRocks!</code>.</p>

<p>Add one node(first you should make it pxed):</p>

<pre><code>$ curl -H "Content-Type: application/json" -X POST --data @samples/noop_body.json http://localhost:8080/api/1.1/nodes/575fce38d23ba028051b4711/obm
$ curl -H "Content-Type: application/json" -X POST --data @samples/ubuntu_boot.json http://localhost:8080/api/1.1/nodes/575fce38d23ba028051b4711/workflows
</code></pre>

<p>Then restart the machine, you will get it installing ubuntu.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use RackHD for Deploying Systems]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/06/01/use-rakehd-for-deploying-systems/"/>
    <updated>2016-06-01T09:14:18+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/06/01/use-rakehd-for-deploying-systems</id>
    <content type="html"><![CDATA[<p>Following are the steps for using the RackHD for deploying systems. Mainly refers to
<a href="http://dojoblog.emc.com/rackhd-cpi/setting-up-rackhd/">http://dojoblog.emc.com/rackhd-cpi/setting-up-rackhd/</a></p>

<p>But the tutorial from emc includes lots of mistakes, so I listed all of the steps in
following chapters.</p>

<h3>Vagrant Env Preparation</h3>

<p>Initialize the vagrant env via(ubuntu1404 is my box name):</p>

<pre><code>$ vagrant init ubuntu1404
A `Vagrantfile` has been placed in this directory. 
$ vim Vagrantfile
  config.vm.provider "virtualbox" do |vb|
  #   # Display the VirtualBox GUI when booting the machine
  #   vb.gui = true
  #
  #   # Customize the amount of memory on the VM:
    vb.memory = "4096"
    vb.cpus = 4
    vb.customize ["modifyvm", :id, "--nicpromisc2", "allow-all", "--ioapic", "on"]
  end
$ vagrant up
</code></pre>

<h3>Prerequisites</h3>

<p>Use following scripts for installing the prerequisites for RackHD(<code>pre_rackhd.sh</code>):</p>

<pre><code>#!/bin/bash 
set -­e

sudo apt-get -y update
sudo apt-get -y dist-upgrade
sudo apt-get autoremove
sudo apt-get install -y nodejs nodejs-legacy npm

# Runtime Dependencies
sudo apt-get install -y rabbitmq-server mongodb isc-dhcp-server
sudo apt-get install -y snmp ipmitool ansible amtterm apt-mirror unzip libkrb5-dev

# upstart will be installed after Ubuntu15.04.
# sudo apt-get -y install upstart­sysv
sudo update-initramfs -u

# amttool
wget http://downloads.sourceforge.net/project/amttooltng/1.7/amttool
sudo chmod 777 amttool
sudo mv amttool /usr/bin/amttooltng

# Compile Dependencies
sudo apt-get install -y git openssh-server pbuilder dh-make devscripts ubuntu-dev-tools

# Git clone all of the repositories
RACKHD_INSTALL_DIR=~;cd $RACKHD_INSTALL_DIR
git clone https://github.com/RackHD/RackHD
RACKHD_PROJECT_DIR=${RACKHD_INSTALL_DIR}/RackHD
cd $RACKHD_PROJECT_DIR
git submodule update --init --recursive
git submodule foreach git pull origin master

# Configuration files
sudo touch /etc/default/on-http
sudo touch /etc/default/on-dhcp-proxy
sudo touch /etc/default/on-taskgraph
sudo touch /etc/default/on-syslog
sudo touch /etc/default/on-tftp

# Build And Install
cd ${RACKHD_PROJECT_DIR}/on-http
./HWIMO-BUILD
sudo dpkg -i on-http_*.deb

cd ${RACKHD_PROJECT_DIR}/on-dhcp-proxy
./HWIMO-BUILD
sudo dpkg -i on-dhcp-proxy_*.deb

cd ${RACKHD_PROJECT_DIR}/on-taskgraph
./HWIMO-BUILD
sudo dpkg -i on-taskgraph_*.deb

cd ${RACKHD_PROJECT_DIR}/on-syslog
./HWIMO-BUILD
sudo dpkg -i on-syslog_*.deb

cd ${RACKHD_PROJECT_DIR}/on-tftp
./HWIMO-BUILD
sudo dpkg -i on-tftp_*.deb
</code></pre>

<p>Now shutdown the vm, add one networking interface, because we want to add PXE in this
network apdater.</p>

<pre><code>$ vim Vagrantfile
config.vm.network "private_network", ip: "172.31.128.1", virtualbox__intnet: "closednet"
</code></pre>

<p>Now startup the vagrant vm again. continue to configure the PXE.</p>

<h3>Configuration</h3>

<p>Examine the eth1 IP Configuration:</p>

<pre><code>vagrant@ubuntu-1404:~$ ifconfig eth1
eth1      Link encap:Ethernet  HWaddr 08:00:27:c2:cf:3e  
          inet addr:172.31.128.1  Bcast:172.31.128.255  Mask:255.255.255.0
</code></pre>

<p>Configure the isc-dhcp:</p>

<pre><code>$ sudo vim /etc/default/isc-dhcp-server
INTERFACES="eth1"
</code></pre>

<p>Add the following configuration into <code>/etc/dhcp/dhcp.conf</code>:</p>

<pre><code># RackHD Added Lines
deny duplicates;

ignore-client-uids true;

subnet 172.31.128.0 netmask 255.255.252.0 {
    range 172.31.128.2 172.31.131.254;
    # Use this option to signal to the PXE client that we are doing proxy DHCP
    option vendor-class-identifier "PXEClient";
}
</code></pre>

<p>Download the config.json for monorail usage, and modify its tftpRoot Configuration:</p>

<pre><code>$ cd /opt/monorail/
$ sudo wget https://raw.githubusercontent.com/RackHD/RackHD/master/packer/ansible/roles/monorail/files/config.json
$ sudo vim config.json
    "tftpRoot": "/opt/monorail/static/tftp",
</code></pre>

<p>Prepare the http and tftp folder:</p>

<pre><code>$ sudo mkdir -p /opt/monorail/static/http/common/
$ sudo mkdir -p /opt/monorail/static/tftp/
</code></pre>

<p>Download the tftp and http static files:</p>

<pre><code>$ cd /opt/monorail/static/tftp
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/ipxe/monorail.ipxe
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/ipxe/monorail-undionly.kpxe
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/ipxe/monorail-efi32-snponly.efi
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/ipxe/monorail-efi64-snponly.efi
$ cd /opt/monorail/static/http/common/
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/builds/discovery.overlay.cpio.gz
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/builds/base.trusty.3.16.0-25-generic.squashfs.img
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/builds/initrd.img-3.16.0-25-generic
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/builds/vmlinuz-3.16.0-25-generic
</code></pre>

<p>Now reboot the vm again, next step we will test the PXE boot.</p>

<h3>PXE Clients</h3>

<p>The PXE will be avaiable and after it finishes, the vm will look like:</p>

<p><img src="/images/2016_06_01_11_47_57_712x236.jpg" alt="/images/2016_06_01_11_47_57_712x236.jpg" />  <br/>
Now you could get the node info via following RESTFUL api call:</p>

<pre><code>$ curl http://localhost:8080/api/1.1/nodes | python -m json.tool
        "createdAt": "2016-06-01T03:44:09.064Z",
        "id": "574e5a0944ff724a05284005",
        "identifiers": [
            "08:00:27:02:5f:7a"
        ],
        "name": "08:00:27:02:5f:7a",
</code></pre>

<p>Examine the mac correponding node ID <code>574e5a0944ff724a05284005</code>, later we wil use this id for deploying CentOS.</p>

<h3>Add CentOS7 Deployment</h3>

<p>Add CentOS7 DVD into the deployment:</p>

<pre><code>$ sudo python ~/RackHD/on-tools/scripts/setup_iso.py /mnt/CentOS-7-x86_64-Everything-1511.iso  /opt/monorail/static/http --link=/home/vagrant/RackHD
</code></pre>

<p>This will takes around 10 mins for importing the iso into the RackHD.</p>

<p>Configure the node:</p>

<pre><code>$ curl -H "Content-Type: application/json" -X POST --data @samples/noop_body.json http://localhost:8080/api/1.1/nodes/574e5a0944ff724a05284005/obm | python -m json.tool
$ curl -H "Content-Type: application/json" -X POST --data @samples/centos_iso_boot.json http://localhost:8080/api/1.1/nodes/574e5a517c6c03440553dd0f/workflows | python -m json.tool
</code></pre>

<p><img src="/images/2016_06_01_14_12_33_449x503.jpg" alt="/images/2016_06_01_14_12_33_449x503.jpg" /> <br/>
Then added the configuration for deploying CentOS7 via:</p>

<pre><code>$ cd ~/RackHD/example/
$ curl -H "Content-Type: application/json" -X POST --data @samples/noop_body.json http://localhost:8080/api/1.1/nodes/574e6516224a4449056183e7/obm | python -m json.tool
$ curl -H "Content-Type: application/json" -X POST --data @samples/centos_iso_boot.json http://localhost:8080/api/1.1/nodes/574e6516224a4449056183e7/workflows | python -m json.tool
</code></pre>

<p>PXE the node, thus you will get a CentOS7 installed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup LXD on Ubuntu1604]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/05/11/setup-lxd-on-ubuntu1604/"/>
    <updated>2016-05-11T15:38:23+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/05/11/setup-lxd-on-ubuntu1604</id>
    <content type="html"><![CDATA[<h3>Preparation</h3>

<p>By default the lxd is installed in ubuntu1604.</p>

<h3>Image</h3>

<p>The image file are downloaded before we actually install it, install the image via:</p>

<pre><code>$ lxc image import ubuntu-16.04-server-cloudimg-amd64-lxd.tar.xz ubuntu-16.04-server-cloudimg-amd64-root.tar.xz --alias ubuntu1604
$ lxc image list
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
|    ALIAS     | FINGERPRINT  | PUBLIC |             DESCRIPTION              |  ARCH  |   SIZE   |         UPLOAD DATE          |
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
| ubuntu1604   | f4c4c60a6b75 | no     | Ubuntu 16.04 LTS server (20160420.3) | x86_64 | 137.54MB | May 10, 2016 at 2:18pm (UTC) 
</code></pre>

<h3>Start Container</h3>

<p>Start the container via:</p>

<pre><code>$ lxc launch ubuntu1604 first1404
$ lxc list
+------------+---------+------+------+------------+-----------+
|    NAME    |  STATE  | IPV4 | IPV6 |    TYPE    | SNAPSHOTS |
+------------+---------+------+------+------------+-----------+
| first1404  | RUNNING |      |      | PERSISTENT | 0         |
+------------+---------+------+------+------------+-----------+
</code></pre>

<p>Attach to the running container via:</p>

<pre><code>$ lxc exec first1404 /bin/bash
</code></pre>

<p>In this container you could do anything, for your customization of the container.</p>

<h3>More Images</h3>

<p>After your modification is done, shutdown the running container, and submit your
modification to a new container:</p>

<pre><code>$ lxc publish second1604 --alias my-new-image
$ lxc image list
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
|    ALIAS     | FINGERPRINT  | PUBLIC |             DESCRIPTION              |  ARCH  |   SIZE   |         UPLOAD DATE          |
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
| my-new-image | 67de38342bfa | no     |                                      | x86_64 | 192.29MB | May 11, 2016 at 7:07am (UTC) |
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
| ubuntu1604   | f4c4c60a6b75 | no     | Ubuntu 16.04 LTS server (20160420.3) | x86_64 | 137.54MB | May 10, 2016 at 2:18pm (UTC) |
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
</code></pre>

<h3>Container Networking</h3>

<p>The default networking is a seperated network, but we could set the lxd using the hosted
network, via following steps:</p>

<pre><code>$ cat /etc/network/interfaces

auto ens3
iface ens3 inet manual

auto containerbr 
iface containerbr inet static
address 192.168.10.193
netmask 255.255.0.0
gateway 192.168.0.176
dns-nameservers 180.76.76.76
bridge_ports ens3
</code></pre>

<p>Reboot the machine, you have the running bridge <code>containerbr</code>, now you could set your bridge to this
newly created bridge:</p>

<pre><code>$ lxc profile device set default eth0 parent containerbr
</code></pre>

<p>Via this you cuold set the same subnet networking address just as in <code>containerbr</code>.</p>
]]></content>
  </entry>
  
</feed>
