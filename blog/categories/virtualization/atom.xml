<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2016-03-17T13:16:47+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Vagrant-libvirt Playing]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/03/16/vagrant-libvirt-playing/"/>
    <updated>2016-03-16T10:31:53+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/03/16/vagrant-libvirt-playing</id>
    <content type="html"><![CDATA[<p>最终目的是用vagrant实现CloudStack+Xenserver的自动化部署。</p>

<h3>CentOS6.7 box Creating</h3>

<p>用packer生成CentOS6.7 amd64的镜像，这个镜像默认是virtualbox兼容的，用vagrant-mutate插件
将其转换为libvirt可用的box镜像:</p>

<pre><code># vagrant mutate centos-6.7.virtualbox.box libvirt
# cd /root/.vagrant.d/boxes
# ls
centos-6.7.virtualbox  trusty64
# mv centos-6.7.virtualbox/ centos6764
# vagrant box list
centos6764 (libvirt, 0)
trusty64   (libvirt, 0)
</code></pre>

<p>创建Vagrantfile文件启动一个实验性质的虚拟机：</p>

<pre><code># pwd
/media/opensusue/dash/Code/Vagrant/CentOS2New
# ls
Vagrantfile  Vagrantfile~
# cat Vagrantfile
    # -*- mode: ruby -*-
    # vi: set ft=ruby :
    Vagrant.configure(2) do |config|
      # The most common configuration options are documented and commented below.
      # For a complete reference, please see the online documentation at
      # https://docs.vagrantup.com.

      config.vm.box = "centos6764"
      # vagrant issues #1673..fixes hang with configure_networks
      config.ssh.shell = "bash -c 'BASH_ENV=/etc/profile exec bash'"
      config.vm.provider :libvirt do |domain|
        domain.memory = 512
        domain.nested = true
      end

      config.vm.define :centosnew do |centosnew|
        centosnew.vm.network :private_network, :ip =&gt; "192.168.88.2"
      end

    end
# vagrant up
</code></pre>

<p><code>vagrant up</code>以后，一个名为<code>CentOS2New_centosnew</code>的虚拟机将被创建， 命名规则为当前文件夹
名+定义的vm名称。</p>

<p>虚拟机启动完毕后，检查状态并登录到该机器:</p>

<pre><code># vagrant status
Current machine states:

centosnew                 running (libvirt)

The Libvirt domain is running. To stop this machine, you can run
`vagrant halt`. To destroy the machine, you can run `vagrant destroy`.
# vagrant ssh centosnew
Last login: Wed Mar 16 02:31:18 2016 from 192.168.121.1
[vagrant@localhost ~]$
</code></pre>

<p>我们可以检查网卡状态，是否设置为我们需要设置的地址<code>192.168.88.2</code>.</p>

<h3>更多定制化参数</h3>

<h4>嵌套虚拟化</h4>

<p>未知原因，我在CentOS6上检查嵌套虚拟化总是提示有问题，所以以下的验证是在Ubuntu系统上验证
的。</p>

<p>我们在上面的配置文件里制定了nested选项为true, 现在登录进去检查一下嵌套虚拟化是否加载成
功:</p>

<pre><code>vagrant@vagrant:~$ lsmod | grep kvm
kvm_intel             143590  0 
kvm                   452043  1 kvm_intel
vagrant@vagrant:~$ modinfo kvm_intel | grep nested
parm:           nested:bool
vagrant@vagrant:~$ cat /sys/module/kvm_intel/parameters/nested
N
vagrant@vagrant:~$ sudo modprobe -r kvm_intel
vagrant@vagrant:~$ sudo modprobe kvm_intel nested=1
vagrant@vagrant:~$ cat /sys/module/kvm_intel/parameters/nested
Y
</code></pre>

<p>改变nested选项为false后，验证步骤如下:</p>

<pre><code>$ cat /sys/module/kvm_intel/parameters/nested
N
</code></pre>

<p>值得注意的是，在虚拟机中，仍然可以通过<code>modprobe kvm_intel nested=1</code>来打开nested选项。</p>

<h4>CPU Passthrough</h4>

<p>指定参数为<code>domain.cpu_mode = 'host-passthrough'</code>:</p>

<pre><code>  config.vm.provider :libvirt do |domain|
    domain.memory = 512
    domain.nested = false
    #domain.cpu_mode = 'host-passthrough'
  end
</code></pre>

<p>未指定时:</p>

<pre><code>vagrant@vagrant:~$ cat /proc/cpuinfo  | grep -i "model name"
model name  : Intel Core i7 9xx (Nehalem Class Core i7)
</code></pre>

<p>指定后:</p>

<pre><code>[vagrant@localhost ~]$ cat /proc/cpuinfo | grep -i "model name"
model name  : Intel(R) Core(TM) i3 CPU         540  @ 3.07GHz
</code></pre>

<h4>指定hostname</h4>

<p>安装cloudstack时hostname是必要条件之一， Vagrantfile中可以指定vm的hostname:</p>

<pre><code>  config.vm.define "centosnew" do |centosnew|
    centosnew.vm.hostname = "centosnew.example.com"
  end
</code></pre>

<p>启动虚拟机后可以通过<code>hostname</code>和<code>hostname --fqdn</code>来检查结果。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用Vagrant管理libvirt]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/03/13/yong-vagrantguan-li-libvirt/"/>
    <updated>2016-03-13T16:07:59+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/03/13/yong-vagrantguan-li-libvirt</id>
    <content type="html"><![CDATA[<h3>先决条件</h3>

<p>Vagrant为0.8.1.</p>

<p>参考:  <br/>
<a href="http://linuxsimba.com/vagrant.html">http://linuxsimba.com/vagrant.html</a>  <br/>
<a href="http://linuxsimba.com/vagrant-libvirt-install/">http://linuxsimba.com/vagrant-libvirt-install/</a></p>

<h3>Ubuntu设置</h3>

<p>考虑到天朝防火墙的存在,需要经过以下命令才能安装vagrant-libvirt插件:</p>

<pre><code>$ sudo apt-get install -y libvirt-dev ruby-dev
$ gem source -r https://rubygems.org/
$ gem source -a http://mirrors.aliyun.com/rubygems/
$ gem install ruby-libvirt -v '0.6.0'
$ gem install vagrant-libvirt -v '0.0.32'
$ vagrant plugin install vagrant-libvirt
$ vagrant plugin list
vagrant-libvirt (0.0.32)
$ axel http://linuxsimba.com/vagrantbox/ubuntu-trusty.box
$ vagrant box add ./ubuntu-trusty.box --name "trusty64"
</code></pre>

<h3>ArchLinux设置</h3>

<p>按照ArchLinux wiki的方法,安装vagrant-libvirt插件:</p>

<pre><code> # in case it's already installled
 vagrant plugin uninstall vagrant-libvirt

 # vagrant's copy of curl prevents the proper installation of ruby-libvirt
 sudo mv /opt/vagrant/embedded/lib/libcurl.so{,.backup}
 sudo mv /opt/vagrant/embedded/lib/libcurl.so.4{,.backup}
 sudo mv /opt/vagrant/embedded/lib/libcurl.so.4.4.0{,.backup}
 sudo mv /opt/vagrant/embedded/lib/pkgconfig/libcurl.pc{,.backup}

 CONFIGURE_ARGS="with-libvirt-include=/usr/include/libvirt with-libvirt-lib=/usr/lib" vagrant plugin install vagrant-libvirt

 # https://github.com/pradels/vagrant-libvirt/issues/541
 export PATH=/opt/vagrant/embedded/bin:$PATH
 export GEM_HOME=~/.vagrant.d/gems
 export GEM_PATH=$GEM_HOME:/opt/vagrant/embedded/gems
 gem uninstall ruby-libvirt
 gem install ruby-libvirt

 # put vagrant's copy of curl back
 sudo mv /opt/vagrant/embedded/lib/libcurl.so{.backup,}
 sudo mv /opt/vagrant/embedded/lib/libcurl.so.4{.backup,}
 sudo mv /opt/vagrant/embedded/lib/libcurl.so.4.4.0{.backup,}
 sudo mv /opt/vagrant/embedded/lib/pkgconfig/libcurl.pc{.backup,}
</code></pre>

<h3>导入box</h3>

<p>用packer编译出来的box文件默认工作在virtualbox下,我们需要用一个插件将其转换为
libvirt可用的box:</p>

<pre><code>#  vagrant plugin install vagrant-mutate
# vagrant mutate ubuntu-14.04.virtualbox.box libvirt
Extracting box file to a temporary directory.
Converting ubuntu-14.04.virtualbox from virtualbox to libvirt.
    (100.00/100%)
Cleaning up temporary files.
The box ubuntu-14.04.virtualbox (libvirt) is now ready to use.
# cd /root/.vagrant.d/boxes/
# mv ubuntu-14.04.virtualbox/ trusty64
# vagrant box list
trusty64 (libvirt, 0)
</code></pre>

<h3>检查安装的box</h3>

<p>可以通过以下命令检查已经安装好的box:</p>

<pre><code>$ vagrant box list
trusty64    (libvirt, 0)
ubuntu1404  (virtualbox, 0)
</code></pre>

<h3>配置Vagrantfile</h3>

<p>以下是一个例子:</p>

<pre><code># -*- mode: ruby -*-
# vi: set ft=ruby :
Vagrant.configure(2) do |config|

  config.vm.box = "trusty64"
  # vagrant issues #1673..fixes hang with configure_networks
  config.ssh.shell = "bash -c 'BASH_ENV=/etc/profile exec bash'"
  config.vm.provider :libvirt do |domain|
    domain.memory = 256
    domain.nested = true
  end

# Private network using virtual network switching
  config.vm.define :vm1 do |vm1|
    vm1.vm.network :private_network, :ip =&gt; "192.168.56.11"
  end

  config.vm.define :vm2 do |vm2|
    vm2.vm.network :private_network, :ip =&gt; "192.168.56.12"
  end

  # Private network. Point to Point between 2 Guest OS using a TCP tunnel
  # Guest 1
  #config.vm.define :test_vm1 do |test_vm1|
  #  test_vm1.vm.network :private_network,
  #    :libvirt__tunnel_type =&gt; 'server',
  #    # default is 127.0.0.1 if omitted
  #    # :libvirt__tunnel_ip =&gt; '127.0.0.1',
  #    :libvirt__tunnel_port =&gt; '11111'

  # Guest 2
  #config.vm.define :test_vm2 do |test_vm2|
  #  test_vm2.vm.network :private_network,
  #    :libvirt__tunnel_type =&gt; 'client',
  #    # default is 127.0.0.1 if omitted
  #    # :libvirt__tunnel_ip =&gt; '127.0.0.1',
  #    :libvirt__tunnel_port =&gt; '11111'


  # Public Network
  config.vm.define :vm1 do |vm1|
    vm1.vm.network :public_network,
      :dev =&gt; "virbr0",
      :mode =&gt; "bridge",
      :type =&gt; "bridge"
  end
end
</code></pre>

<h3>启动虚拟机</h3>

<pre><code># vagrant up --provider=libvirt
</code></pre>

<p>启动时会出现以下问题, 解决方案为:</p>

<pre><code>$ vagrant up --provider=libvirt
....
Missing required arguments: libvirt_uri
.....
$ vagrant plugin install --plugin-version 0.0.3 fog-libvirt
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Virtio-gpu Working Tips]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/01/31/virtio-gpu-working-tips/"/>
    <updated>2016-01-31T11:49:13+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/01/31/virtio-gpu-working-tips</id>
    <content type="html"><![CDATA[<h3>System</h3>

<p>Install Fedora 23, select fedora server and install the system, then using the
lastest development kernel via:</p>

<pre><code>$ curl -s https://repos.fedorapeople.org/repos/thl/kernel-vanilla.repo | sudo tee /etc/yum.repos.d/kernel-vanilla.repo
$ sudo dnf --enablerepo=kernel-vanilla-mainline update
</code></pre>

<p>Checking the running kernel via:</p>

<pre><code>$ uname -r
4.5.0-0.rc1.git0.1.vanilla.knurd.1.fc23.x86_64
</code></pre>

<p>Running the system which have kernel version newer than 4.4  is the basis for enable the virt-io.</p>

<h3>Install packages</h3>

<pre><code>$ sudo dnf install -y gcc zlib-devel glib2-devel pixman-devel libfdt-devel \ 
lzo-devel snappy-devel bzip2-devel libseccomp-devel gtk2-devel gtk3-devel \ 
gnutls-devel vte-devel SDL-devel librdmacm-devel libuuid-devel \ 
 libcap-ng-devel libcurl-devel ceph-devel libssh2-devel libaio-devel \ 
glusterfs-devel glusterfs-api-devel numactl-devel gperftools-devel \ 
 texinfo libiscsi-devel spice-server-devel libusb-devel usbredir-devel \ 
libnfs-devel libcap-devel libattr-devel  
</code></pre>

<h3>virglrenderer</h3>

<p>coprs have the repository for this:  <br/>
<a href="https://copr.fedorainfracloud.org/coprs/kraxel/">https://copr.fedorainfracloud.org/coprs/kraxel/</a></p>

<pre><code>$ cd /etc/yum.repos.d/
$ sudo wget https://copr.fedorainfracloud.org/coprs/kraxel/virgl/repo/fedora-23/kraxel-virgl-fedora-23.repo
</code></pre>

<p>Install the virglrenderer via:</p>

<pre><code>$ sudo dnf install virglrenderer
</code></pre>

<h3>qemu</h3>

<p>Git clone the source code:</p>

<pre><code>$ git clone https://www.kraxel.org/cgit/qemu
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Read Digest on KVM]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/01/26/read-digest-on-kvm/"/>
    <updated>2016-01-26T09:12:26+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/01/26/read-digest-on-kvm</id>
    <content type="html"><![CDATA[<h3>Some Words</h3>

<p>VMM: (Virtual Machine Monitor)   <br/>
VMX: (Virtual Machine eXtensions):  instructions on processors with x86 virtualization.</p>

<p>Virtualization software:  is most often used to emulate a complete computer system in
order to allow a guest operating system to be run, for example allowing Linux to run as
a guest on top of a PC that is natively running a Microsoft Windows operating system
(or the inverse, running Windows as a guest on Linux).</p>

<p>CPU Ring:   <br/>
<a href="https://en.wikipedia.org/wiki/Protection_ring">https://en.wikipedia.org/wiki/Protection_ring</a></p>

<p><img src="/images/300px-Priv_rings.svg.png" alt="/images/300px-Priv_rings.svg.png" /></p>

<p>VT-d, I/O Hardware Virtualization.   <br/>
VT-c, Networking Hardware Virtualization.</p>

<h3>Host kickstart file</h3>

<p>Add following installation packages:</p>

<pre><code>%packages
@virtualization
@Base
@Core
@additional-devel
@base
@large-systems
@storage-client-iscsi
@systgem-management-snmp
@virtualization
@virtualization-client
@virtualization-platform
@virtualization-tools
%end
</code></pre>

<h3>Mouse On Windows Virtual Machine</h3>

<p>Add twice the usb mouse:</p>

<pre><code>&lt;input type='tablet' bus='usb'
</code></pre>

<h3>NUMA</h3>

<p>Install the numa configuration tools via:</p>

<pre><code># apt-cache search numactl
numactl - NUMA scheduling and memory placement tool
# apt-get -y install numactl
</code></pre>

<p>Command: <code>numactl --hardware</code>, <code>numastat</code>, <code>numastat -c qemu-kvm</code>.</p>

<p>Check the numa banlancing policy via:</p>

<pre><code># cat /proc/sys/kernel/numa_balancing
0
</code></pre>

<p><code>echo 1</code> for open the auto balancing.</p>

<p>KSM, could merge the same memory page, even in different NUMA node.</p>

<pre><code># cat /sys/kernel/mm/ksm/merge_across_nodes 
1
</code></pre>

<h3>CPU Binding</h3>

<p>Use <code>virsh vcpuinfo xx</code> for displaying the VCPU/CPU corresponding relationship.</p>

<pre><code>virsh # emulatorpin 79
emulator: CPU Affinity
----------------------------------
       *: 0-7
</code></pre>

<p>Change it dynamically:</p>

<pre><code>virsh # emulatorpin 79 0-3 --live

virsh # emulatorpin 79
emulator: CPU Affinity
----------------------------------
       *: 0-3
</code></pre>

<p>Now you could check the result via <code>virsh dumpxml xxx</code>:</p>

<pre><code>  &lt;vcpu placement='static'&gt;4&lt;/vcpu&gt;
  &lt;cputune&gt;
    &lt;emulatorpin cpuset='0-3'/&gt;
  &lt;/cputune&gt;
</code></pre>

<p>1-1 binding using virsh:</p>

<pre><code># virsh vcpupin 79 0 0    
# virsh vcpupin 79 1 1
# virsh vcpupin 79 2 2
# virsh vcpupin 79 3 3
# virsh dumpxml 79 | more
  &lt;cputune&gt;
    &lt;vcpupin vcpu='0' cpuset='0'/&gt;
    &lt;vcpupin vcpu='1' cpuset='1'/&gt;
    &lt;vcpupin vcpu='2' cpuset='2'/&gt;
    &lt;vcpupin vcpu='3' cpuset='3'/&gt;
</code></pre>

<p>Now view the vcpuinfo via:</p>

<pre><code># virsh vcpuinfo 79
VCPU:           0
CPU:            0
State:          running
CPU time:       9.2s
CPU Affinity:   y-------
</code></pre>

<h3>CPU Hot-Plug-in</h3>

<p>The cpu configuration info is listed as:</p>

<p><img src="/images/2016_01_26_10_54_29_285x127.jpg" alt="/images/2016_01_26_10_54_29_285x127.jpg" /></p>

<p>View the CPU infos via:</p>

<pre><code># cat /proc/interrupts 
           CPU0       CPU1       
</code></pre>

<p>Change the CPUs to 3:</p>

<pre><code>virsh # setvcpus 80 3 --live
</code></pre>

<p>Now in the vm the result should be(or detect it via <code>cat /proc/cpuinfo</code>) :</p>

<pre><code># cat /proc/interrupts 
           CPU0       CPU1       CPU2  
</code></pre>

<h3>CPU Working Mode</h3>

<p>If we select the pass-through, then the cpuinfo should be:</p>

<pre><code># virsh dumpxml xxxxx
  &lt;cpu mode='host-passthrough'&gt;
  &lt;/cpu&gt;
# cat /proc/cpuinfo
processor       : 1
vendor_id       : GenuineIntel
cpu family      : 6
model           : 58
model name      : Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz
</code></pre>

<p>If we select the host-model, will choose the most similar:  <br/>
Or If we choose <code>Copy host cpu mode</code>, like following:  <br/>
<img src="/images/2016_01_26_11_07_32_299x274.jpg" alt="/images/2016_01_26_11_07_32_299x274.jpg" /></p>

<pre><code># cat /proc/cpuinfo
....
processor       : 1
vendor_id       : GenuineIntel
cpu family      : 6
model           : 42
model name      : Intel Xeon E312xx (Sandy Bridge)
</code></pre>

<h3>Memory Balloon</h3>

<p>Change the memory balloon to 1024 or 4096 via:</p>

<pre><code># virsh qemu-monitor-command PerfTune --hmp --cmd balloon 1024
# virsh qemu-monitor-command PerfTune --hmp --cmd info balloon
balloon: actual=1024
# virsh qemu-monitor-command PerfTune --hmp --cmd balloon 4096
</code></pre>

<h3>Memory Limitation</h3>

<p>Make configuration of the memory via:</p>

<pre><code>virsh memtune PerfTune --hard-limit xxxxx --config
virsh memtune PerfTune --soft-limit xxxxx --config
virsh memtune PerfTune --swap-hard-limit xxxxx --config
virsh memtune PerfTune --min_guarantee xxxxx --config
</code></pre>

<p>&ndash;config, write to configuration xml
&ndash;live, lively write
&ndash;current ?</p>

<h3>HugePage</h3>

<p>Enable hugepage via:</p>

<pre><code># grep -i huge /proc/meminfo 
AnonHugePages:  12820480 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CloudStack使用LXC要点]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/01/14/cloudstackshi-yong-lxcyao-dian/"/>
    <updated>2016-01-14T19:41:03+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/01/14/cloudstackshi-yong-lxcyao-dian</id>
    <content type="html"><![CDATA[<h3>先决条件</h3>

<p>Cloudstack 4.6.0, Management Server搭建完毕, Agent包安装完毕.   <br/>
具体的步骤可以参考KVM的安装.</p>

<h3>Agent机器</h3>

<p>设置hypervisor的类型为LXC模式:</p>

<pre><code>$ vim /etc/cloudstack/agent/agent.properties
 hypervisor.type=lxc
$ service cloudstack-agent restart
</code></pre>

<h3>LXC系统虚拟机模板安装</h3>

<p>在CloudStack Management节点上,运行以下命令安装系统虚拟机模板:</p>

<pre><code>$ /usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt -m /mnt1 -u http://192.168.177.13/systemvm64template-4.6.0-kvm.qcow2.bz2 -h lxc -F
</code></pre>

<p>注意替换以上命令里的镜像下载地址及安装目录.</p>

<h3>添加Zone</h3>

<p>先添加一个基本的Zone,注意选择hypervisor为LXC.  <br/>
添加一级/二级存储.  <br/>
添加完毕后, enable zone后, 下载模板, 注意不要选择HVM的选项.</p>

<p>到这里就可以开始创建实例了.</p>
]]></content>
  </entry>
  
</feed>
