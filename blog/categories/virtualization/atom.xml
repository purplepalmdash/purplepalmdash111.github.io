<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2015-10-30T10:02:08+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Thinking in VR]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/29/thinking-in-vr/"/>
    <updated>2015-10-29T16:00:39+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/29/thinking-in-vr</id>
    <content type="html"><![CDATA[<p>Restriction of the connections.</p>

<pre><code>http://www.cnblogs.com/cmt/archive/2013/03/13/2957583.html
http://bbs.m0n0china.org/viewthread.php?tid=16459
http://my.oschina.net/u/1169079/blog/397705
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack Liberty安装(1)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/25/openstack-libertyan-zhuang-1/"/>
    <updated>2015-10-25T12:05:58+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/25/openstack-libertyan-zhuang-1</id>
    <content type="html"><![CDATA[<h3>初始化准备</h3>

<p>用Packer.io制作Ubuntu14.04的qcow2文件镜像, 设定磁盘大小为100G, lvm分区.  <br/>
网络: 在virt-manager中制作一个网段为10.0.0.0/24的网段. 所有创建虚拟机的eth0均
加入到此网络中.   <br/>
<img src="/images/2015_10_25_12_22_24_504x487.jpg" alt="/images/2015_10_25_12_22_24_504x487.jpg" /></p>

<p>To be Continued.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tips on Ceph on Docker]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/23/tips-on-ceph-on-docker/"/>
    <updated>2015-10-23T22:32:17+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/23/tips-on-ceph-on-docker</id>
    <content type="html"><![CDATA[<h3>Installation</h3>

<p>Pull the docker image via:</p>

<pre><code>$ sudo docker pull ceph/demo
</code></pre>

<h3>Run Ceph</h3>

<p>Run the container via:</p>

<pre><code># sudo docker run -d --net=host -e MON_IP=192.168.10.190 -e CEPH_NETWORK=192.168.10.0/24
ceph/demo
</code></pre>

<p>View the docker instance via:</p>

<pre><code># docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
cbe567594adb        ceph/demo           "/entrypoint.sh"    About an hour ago   Up About an hour                        furious_hopper 
# docker exec -it cbe567594adb
</code></pre>

<h3>Ceph Operation</h3>

<p>View the ceph processes:</p>

<pre><code>root@monitor:/# ps -ef | grep "ceph"                                                                                                                                 
root         1     0  0 13:16 ?        00:00:00 /usr/bin/python /usr/bin/ceph -w
root        32     1  0 13:16 ?        00:00:01 ceph-mon -i monitor --public-addr 192.168.10.190:6789
root       201     1  0 13:16 ?        00:00:06 ceph-osd -i 0 -k /var/lib/ceph/osd/ceph-0/keyring
root       412     1  0 13:16 ?        00:00:01 ceph-mds --cluster=ceph -i 0
root       470     1  0 13:16 ?        00:00:03 radosgw -c /etc/ceph/ceph.conf -n client.radosgw.gateway -k /var/lib/ceph/radosgw/monitor/keyring --rgw-socket-path= --rgw-frontends=civetweb port=80
root       473     1  0 13:16 ?        00:00:01 /usr/bin/python /usr/bin/ceph-rest-api -n client.admin
root      1703  1557  0 14:37 ?        00:00:00 grep --color=auto ceph
</code></pre>

<p>View Ceph Status:</p>

<pre><code>root@monitor:/# ceph -s
    cluster c3470b36-8d03-4dbb-8af4-d4353ea54973
     health HEALTH_OK
     monmap e1: 1 mons at {monitor=192.168.10.190:6789/0}
            election epoch 2, quorum 0 monitor
     mdsmap e5: 1/1/1 up {0=0=up:active}
     osdmap e22: 1 osds: 1 up, 1 in
      pgmap v75: 144 pgs, 11 pools, 6796 bytes data, 70 objects
            3789 MB used, 287 GB / 291 GB avail
                 144 active+clean
</code></pre>

<p>Create a new user:</p>

<pre><code># radosgw-admin user create --uid="xxxx" --display-name="XXXX YYYY" --email=xxxyyy@gmail.com
</code></pre>

<p>Remember the output of the <code>access_key</code> and <code>secret_key</code>.</p>

<h3>Configure Ceph</h3>

<p>Install softwares:</p>

<pre><code># apt-get update -y
# apt-get install -y python
# apt-get install -y python-pip
# pip install boto
# pip install ipython
# pip install s3cmd
# apt-get install -y vim
</code></pre>

<p>Use the <code>list_buckets.py</code> file:</p>

<pre><code># cat list_buckets.py
import boto
import boto.s3.connection

access_key =  '5S5YPYC46EVYG9MF0RSR'
secret_key = 'hEcgOMoNOp6jmYnt3G6qqJiT7mV5A8zBR9g6o38Z'

conn = boto.connect_s3(
        aws_access_key_id = access_key,
        aws_secret_access_key = secret_key,
        host = 'localhost',
        is_secure=False,
        calling_format = boto.s3.connection.OrdinaryCallingFormat(),
        )
</code></pre>

<p>In ipython, use following commands for create a new bucket:</p>

<pre><code>In [8]: conn.create_bucket("fuck")                                        
Out[8]: &lt;Bucket: fuck&gt;

In [9]: conn.get_all_buckets()                                                 
Out[9]: [&lt;Bucket: fuck&gt;]
</code></pre>

<p>Download the s3cfg file from :</p>

<p><a href="https://github.com/tobegit3hub/.s3cfg/blob/master/.s3cfg">https://github.com/tobegit3hub/.s3cfg/blob/master/.s3cfg</a></p>

<p>Also configure its <code>access_key</code> and <code>secret_key</code>, save it in your root
directory, run following commands, you will see the buckets:</p>

<pre><code># s3cmd ls                                                                                                                                             
2015-10-23 13:25  s3://fuck
</code></pre>

<h3>Mount Ceph In Linux</h3>

<p>In Ubuntu machine, install ceph:</p>

<pre><code>$ sudo apt-get install ceph
</code></pre>

<p>Get the admin&rsquo;s password(In Ceph Container):</p>

<pre><code># ceph-authtool --print-key /etc/ceph/ceph.client.admin.keyring 
AQAmMypWs06BGxAAQ1rQyFqFJ25xaDye4c9kyQ==
</code></pre>

<p>Now mount it via:</p>

<pre><code>$ sudo mount -t ceph 192.168.10.190:/ /mnt -o name=admin,secret=AQAmMypWs06BGxAAQ1rQyFqFJ25xaDye4c9kyQ==
$ sudo touch /mnt/abc
</code></pre>

<p>Via <code>ceph -s</code> we could see the <code>pgmap v86</code> changes, which indicates the data
has been written into the ceph.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On VM Performance Test]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/19/on-vm-performance-test/"/>
    <updated>2015-10-19T09:54:45+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/19/on-vm-performance-test</id>
    <content type="html"><![CDATA[<h3>AIM</h3>

<p>To build the testing Framework.</p>

<h3>Reference Material</h3>

<p><a href="http://thesai.org/Downloads/Volume5No5/Paper_16-Performance_Evaluation_of_Private_Clouds.pdf">Paper_16-Performance_Evaluation_of_Private_Clouds.pdf</a></p>

<p><a href="http://www.junauza.com/2012/05/best-system-benchmarking-tools-for.html">http://www.junauza.com/2012/05/best-system-benchmarking-tools-for.html</a></p>

<h3>Software</h3>

<p>Install following software:</p>

<pre><code>
# apt-get install -y  libx11-dev libgl1-mesa-dev libxext-dev perl perl-modules make gcc nfs-common
postgresql-9.1 postgresql-contrib-9.1 mbw iperf
</code></pre>

<h3>CPU</h3>

<p>The following software are introduced for testing CPU Performance:
* Linpack
* Lookbusy</p>

<h4>Linpack</h4>

<p>Note: Only works on INTEL CPU.</p>

<p><a href="http://registrationcenter-download.intel.com/akdlm/irc_nas/2169/l_lpk_p_10.3.4.007.tgz">Linux</a></p>

<p><a href="http://registrationcenter-download.intel.com/akdlm/irc_nas/2169/w_lpk_p_10.3.4.007.zip">Windows</a></p>

<p><a href="http://registrationcenter-download.intel.com/akdlm/irc_nas/2169/m_lpk_p_10.3.4.007.tgz">Mac</a></p>

<p>Linux Steps:</p>

<pre><code>$ tar xvf l_lpk_p_10.3.4.007.tgz
$ cd linpack_10.3.4/benchmarks/linpack
$ ./runme_xeon64
</code></pre>

<p>You can see the testing result via <code>tail -f lin_xeon64.txt</code>.</p>

<p>Note your CPU&rsquo;s temperature and your system load are changing.</p>

<h4>Lookbusy</h4>

<p><a href="https://www.devin.com/lookbusy/">https://www.devin.com/lookbusy/</a>  <br/>
Download and install it via:</p>

<pre><code># wget https://www.devin.com/lookbusy/download/lookbusy-1.4.tar.gz
# tar xzvf lookbusy-1.4.tar.gz
# cd lookbusy-1.4/
# ./configure --prefix=/usr
# make &amp;&amp; make install
# lookbusy
</code></pre>

<p>Default will add 50% load to each CPU.</p>

<h4></h4>

<h3>Memory</h3>

<h4>Stream</h4>

<p><a href="https://www.cs.virginia.edu/stream/ref.html">https://www.cs.virginia.edu/stream/ref.html</a></p>

<p>Get the source code from:
<a href="https://www.cs.virginia.edu/stream/FTP/Code/">https://www.cs.virginia.edu/stream/FTP/Code/</a></p>

<p>Then compile it and run:</p>

<pre><code># make stream_c.exe
# ./stream_c.exe
</code></pre>

<h3>Disk IO</h3>

<h4>Bonnie++</h4>

<p><a href="http://blog.csdn.net/choice_jj/article/details/8026130">测试工具Bonnie++的使用</a></p>

<h3>Network IO</h3>

<h4>Iperf</h4>

<p>To be added.</p>

<h3>OverAll</h3>

<p>System Level Testing Framework.</p>

<h4>UnixBench</h4>

<p>Download the source file, then run it, and see the result.</p>

<pre><code># wget http://byte-unixbench.googlecode.com/files/UnixBench5.1.3.tgz
# tar xvf UnixBench5.1.3.tgz
# cd UnixBench
# make
# ./Run 2&gt;&amp;1 | tee RunResult.txt
</code></pre>

<h4>LMBench</h4>

<p><a href="http://www.bitmover.com/lmbench/">http://www.bitmover.com/lmbench/</a></p>

<p><a href="http://blog.csdn.net/dianhuiren/article/details/7331777">LM Usage</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CloudStackOnUbuntuIssue]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/10/18/cloudstackonubuntuissue/"/>
    <updated>2015-10-18T07:34:13+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/10/18/cloudstackonubuntuissue</id>
    <content type="html"><![CDATA[<h3>前提</h3>

<p>可能我们会碰到这样一种情形，自己的工作机位于某一很复杂的网络环境中，需要在工作
机上搭建出一个网络资源隔离的CloudStack实验环境。而同时我们又希望能最大限度的利
用工作机的性能，尽量让虚拟化中的性能损耗达到最小。那么以下的解决方案可能就是我
们所需要的：物理机上启动一个CloudStack Management的虚拟机，而后在虚拟机的控制
台里将配置好CloudStack Agent的主机加入到CloudStack环境中。而物理机和虚拟机之间
的管理网络为物理机上的一个虚拟网络。这样既做到了硬件资源的最大利用，也有效的杜
绝了本地环境对整个复杂网络环境的影响。</p>

<h3>硬件及网络规划</h3>

<p>物理机: 4核，支持虚拟化， 内存8G，IP地址为192.168.88.0/24网段里的某台机器。   <br/>
虚拟机: 分配2核CPU，内存3G。    <br/>
物理机与虚拟机之间，通过虚拟的网络桥接。地址段被规划为172.16.16.0/24，当然你可
以更改为其他网段。</p>

<h3>物理机准备</h3>

<p>物理机运行Ubuntu 14.04.03 64位版本。</p>

<h4>Root登录权限</h4>

<p>首先打开root的ssh登录权限:</p>

<pre><code>$ sudo apt-get install -y openssh-server
$ sudo vi /etc/ssh/sshd_config
PermitRootLogin yes
$ sudo bash
# passwd
Enter new UNIX password: 
Retype new UNIX password: 
passwd: password updated successfully
# service ssh restart
</code></pre>

<p>现在重新登录，即可以使用root直接登录进系统。</p>

<h4>安装软件</h4>

<p>安装所需要的软件:</p>

<pre><code># apt-get update
# apt-get install virt-manager qemu vim wget git nfs-kernel-server
</code></pre>

<h4>创建两个网桥</h4>

<p>更改Ubuntu的网络配置文件后，重新启动物理机。这里我们创建了br0和cloudbr0两个网
桥, 注意我们设置cloudbr0的IP地址为172.16.16.1/24。这个网络将作为CloudStack
Management虚拟机与物理机之间的管理网络。</p>

<pre><code># cat /etc/network/interfaces
    # This file describes the network interfaces available on your system
    # and how to activate them. For more information, see interfaces(5).

    # The loopback network interface
    auto lo
    iface lo inet loopback

    # The primary network interface
    auto eth0
    iface eth0 inet manual

    auto br0
    iface br0 inet dhcp
    bridge_ports eth0

    auto cloudbr0
    iface cloudbr0 inet static
    bridge_ports none
    bridge_fd 5
    bridge_stp off
    bridge_maxwait 1
    address 172.16.16.1
    netmask 255.255.255.0
</code></pre>

<p>创建完毕后，用<code>ip addr</code>命令将可以看到5个网络，分别为
lo/eth0/br0/cloudbr0/virbr0， 其中virbr0由libvirtd所创建，这里不用涉及。</p>

<h4>配置cloudbr0的NAT转发</h4>

<p>打开内核的转发功能:</p>

<pre><code># vim /etc/sysctl.conf
net.ipv4.ip_forward=1
</code></pre>

<p>由于我们的虚拟机需要访问Internet以便安装包，因而我们需要开启cloudbr0上的转发，
配置iptables规则如下:</p>

<pre><code># iptables -t nat -A POSTROUTING -s \ 
172.16.16.0/24 ! -d 172.16.16.0/24 -j MASQUERADE
</code></pre>

<p>这样主机端就开启了172.16.16.0/24网段的转发。</p>

<p>保存我们配置的iptables规则:</p>

<pre><code>
# iptables-save &gt;/etc/iptables-save
# echo "pre-up iptables-restore &lt; /etc/iptables-save"&gt;&gt;/etc/network/interfaces
</code></pre>

<h4>准备NFS存储</h4>

<p>NFS存储可以被用作一级存储或者二级存储,我们这里使用它作为二级存储,主存储即一级
存储我们将使用物理机上的本地存储.</p>

<p>首先准备存储共享目录:</p>

<pre><code># mkdir -p /export/primary /export/secondary
</code></pre>

<p>引出该目录:</p>

<pre><code># cat &gt;&gt;/etc/exports &lt;&lt;EOM
/export  *(rw,async,no_root_squash,no_subtree_check)
EOM
</code></pre>

<p>在指定端口配置NFS的statd:</p>

<pre><code># cp /etc/default/nfs-common /etc/default/nfs-common.orig
# sed -i '/NEED_STATD=/ a NEED_STATD=yes' /etc/default/nfs-common
# sed -i '/STATDOPTS=/ a STATDOPTS="--port 662 \
--outgoing-port 2020"' /etc/default/nfs-common
# diff -du /etc/default/nfs-common.orig /etc/default/nfs-common
</code></pre>

<p>配置lockd:</p>

<pre><code># cat &gt;&gt; /etc/modprobe.d/lockd.conf &lt;&lt;EOM
 options lockd nlm_udpport=32769 nlm_tcpport=32803
 EOM
</code></pre>

<p>重启nfs server:</p>

<pre><code># service nfs-kernel-server restart
</code></pre>

<h4>网络名配置</h4>

<p>配置物理机的网络名如下:</p>

<pre><code># vim /etc/hostname
physicalnode
# vim /etc/hosts
127.0.0.1       localhost
172.16.16.1     physicalnode
172.16.16.2     cloudstackmgmt

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
</code></pre>

<h4>创建CloudStack Management虚拟机</h4>

<p>创建一台Ubuntu 14.04 64位虚拟机，特别注意的是其网络设置， 在virt-manager中，手
动选择如下：</p>

<p><img src="/images/2015_10_18_08_16_55_654x246.jpg" alt="/images/2015_10_18_08_16_55_654x246.jpg" /></p>

<p>这样虚拟机会选择将自己桥接到物理机上的cloudbr0接口。这时候如果在物理机运行
<code>brctl show</code>可以看到如下结果:</p>

<pre><code># brctl show
bridge name     bridge id               STP enabled     interfaces
br0             8000.52540002d56f       no              eth0
cloudbr0                8000.fe54004d6663       no              vnet0
virbr0          8000.000000000000       yes
</code></pre>

<p>虚拟机启动完毕后是没有IP地址的，因为cloudbr0上没有提供dhcp服务，我们可以手动配
置IP地址如下, 这里我们把虚拟机的IP地址配置为172.16.16.2:</p>

<pre><code>$ cat /etc/network/interfaces
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet static
address 172.16.16.2
netmask 255.255.255.0
gateway 172.16.16.1
dns-nameservers 223.5.5.5
</code></pre>

<p>配置完IP地址后，重启后可以验证是否连接到Internet.</p>

<h3>虚拟机配置</h3>

<p>我们先配置好CloudStack Management虚拟机, 而后再将物理机加入到CloudStack虚拟机
管理网络里.</p>

<h4>开启root登录</h4>

<p>步骤和上面物理机上开启root登录一样.</p>

<h4>安装源配置</h4>

<p>预先取得用于Ubuntu 14.04 CloudStack环境的安装包, 建立本地源,而后添加为:</p>

<pre><code># vim /etc/apt/sources.list
.....
deb http://192.168.1.13/iso/    cloudstackdeb/
</code></pre>

<h4>安装包</h4>

<p>安装CloudStack Management节点所需的包如下:</p>

<pre><code># apt-get update
# apt-get install vim wget openntpd cloudstack-management mysql-server
</code></pre>

<p>注意在安装mysql的时候,需要指定root用户所需的管理密码,这个密码会在后面配置
CloudStack时被用到.</p>

<h4>主机名配置</h4>

<p>配置主机名:</p>

<pre><code># vim /etc/hostname 
CloudStackMgmt
</code></pre>

<p>配置FQDN所需的主机名:</p>

<pre><code># vim /etc/hosts
127.0.0.1       localhost
172.16.16.1     physicalnode
172.16.16.2     cloudstackmgmt

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
</code></pre>

<p>重启后验证是否被设置正确:</p>

<pre><code>root@cloudstackmgmt:~# hostname
cloudstackmgmt
root@cloudstackmgmt:~# hostname --fqdn
cloudstackmgmt
</code></pre>

<h4>创建数据库</h4>

<p>创建CloudStack所需的配置文件,并重启mysql服务:</p>

<pre><code>root@cloudstackmgmt:~# cat /etc/mysql/conf.d/cloudstack.cnf 
    [mysqld]
    innodb_rollback_on_timeout=1
    innodb_lock_wait_timeout=600
    max_connections=350
    log-bin=mysql-bin
    binlog-format = 'ROW'
root@cloudstackmgmt:~# service mysql restart
    mysql stop/waiting
    mysql start/running, process 5812
</code></pre>

<p>创建CloudStack所需的数据库:</p>

<pre><code>root@cloudstackmgmt:~# cloudstack-setup-databases \
    cloud:engine@localhost \
    --deploy-as=root:xxxxxx \
    -e file -m mymskey44 -k mydbkey00
</code></pre>

<p>参数说明如下:</p>

<pre><code># mysql root 密码: xxxxxxx
# cloud user 密码: engine
# management_server_key: mymskey44
# database_key: mydbkey00
</code></pre>

<h4>安装CloudStack系统虚拟机模板</h4>

<p>加载NFS共享目录到本地,而后用以下命令安装预先下载好的kvm系统虚拟机所需的模板文
件:</p>

<pre><code># mount -t nfs 172.16.16.1:/export/secondary /mnt
# /usr/share/cloudstack-common/scripts/storage/secondary/cloud-install-sys-tmplt \
-m /mnt -u http://192.168.1.13/iso/systemvm64template-4.5-kvm.qcow2.bz2  -h \
kvm -F
# umount /mnt
</code></pre>

<h4>初始化配置CloudStack</h4>

<p>打开浏览器访问<a href="http://172.16.16.2:8080/client">http://172.16.16.2:8080/client</a></p>

<p>如果碰到以下错误, 则看后面的解决步骤:</p>

<p><img src="/images/2015_10_18_09_24_35_421x225.jpg" alt="/images/2015_10_18_09_24_35_421x225.jpg" /></p>

<p>关闭tomcat6服务后重启cloudstack-management服务:</p>

<pre><code># service tomcat6 stop
# service cloudstack-management restart
</code></pre>

<p>下面的Workaround可以每次重启时自动重启该服务:</p>

<pre><code># crontab -e
@reboot /bin/RestartCloudStack.sh
# vim /bin/RestartCloudStack.sh
service tomcat6 stop &amp;&amp; service cloudstack-management start
</code></pre>

<p>我们要更改CloudStack使用本地存储,并更改其镜像下载地址:  <br/>
本地存储:</p>

<p><img src="/images/2015_10_18_09_33_24_669x263.jpg" alt="/images/2015_10_18_09_33_24_669x263.jpg" /></p>

<p>镜像下载地址:</p>

<p><img src="/images/2015_10_18_09_35_09_774x233.jpg" alt="/images/2015_10_18_09_35_09_774x233.jpg" /></p>

<p>更改完毕后,<code>service coudstack-management restart</code>重启服务</p>

<h3>添加CloudStack Agent</h3>

<h4>物理机端配置</h4>

<p>在物理机上,安装和配置CloudStack Agent以及libvirtd, 同样需要配置好本地
CloudStack安装源:</p>

<pre><code># apt-get install cloudstack-agent
</code></pre>

<p>配置libvirtd:</p>

<pre><code>$ cp /etc/libvirt/libvirtd.conf /etc/libvirt/libvirtd.conf.orig

$ sed -i '/#listen_tls = 0/ a listen_tls = 0' /etc/libvirt/libvirtd.conf
$ sed -i '/#listen_tcp = 1/ a listen_tcp = 1' /etc/libvirt/libvirtd.conf
$ sed -i '/#tcp_port = "16509"/ a tcp_port = "16509"' /etc/libvirt/libvirtd.conf
$ sed -i '/#auth_tcp = "sasl"/ a auth_tcp = "none"' /etc/libvirt/libvirtd.conf
$ diff -du /etc/libvirt/libvirtd.conf.orig /etc/libvirt/libvirtd.conf
</code></pre>

<p>Patch libvirt-bin.conf:</p>

<pre><code>$ cp /etc/default/libvirt-bin /etc/default/libvirt-bin.orig
$ sed -i -e 's/libvirtd_opts="-d"/libvirtd_opts="-d -l"/' /etc/default/libvirt-bin
$ diff -du /etc/default/libvirt-bin.orig /etc/default/libvirt-bin
$ service libvirt-bin restart
</code></pre>

<p>Patch qemu.conf以监听所有端口:</p>

<pre><code>$ cp /etc/libvirt/qemu.conf /etc/libvirt/qemu.conf.orig
$ sed -i '/#vnc_listen = "0.0.0.0"/ a vnc_listen = "0.0.0.0"' /etc/libvirt/qemu.conf
$ diff -du /etc/libvirt/qemu.conf.orig /etc/libvirt/qemu.conf
$ service libvirt-bin restart
</code></pre>

<p>关闭AppArmor:</p>

<pre><code>$ ln -s /etc/apparmor.d/usr.sbin.libvirtd /etc/apparmor.d/disable/
$ ln -s /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper /etc/apparmor.d/disable/
$ apparmor_parser -R /etc/apparmor.d/usr.sbin.libvirtd
$ apparmor_parser -R /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper
$ service libvirt-bin restart
</code></pre>

<p>配置防火墙并打开以下端口:</p>

<pre><code>$ ufw allow proto tcp from any to any port 22
$ ufw allow proto tcp from any to any port 1798
$ ufw allow proto tcp from any to any port 16509
$ ufw allow proto tcp from any to any port 5900:6100
$ ufw allow proto tcp from any to any port 49152:49216
</code></pre>

<h4>配置CloudStack</h4>

<p>访问<code>http://172.16.16.2:8080/client</code>, 点击<code>I have used CloudStack before, skip
this guide</code>.</p>

<p>Infrastructure -> Zones(View All), 在点开的页面里,点击<code>Add Zone</code>.</p>

<p>选择<code>Basic</code>, Next</p>

<p><img src="/images/2015_10_18_10_20_49_521x551.jpg" alt="/images/2015_10_18_10_20_49_521x551.jpg" /></p>

<p>选择本地存储,Next:</p>

<p><img src="/images/2015_10_18_10_21_48_484x289.jpg" alt="/images/2015_10_18_10_21_48_484x289.jpg" /></p>

<p>跳过配置网络后, 配置Pod IP:</p>

<p><img src="/images/2015_10_18_10_23_02_474x419.jpg" alt="/images/2015_10_18_10_23_02_474x419.jpg" /></p>

<p>Guest Traffic配置:</p>

<p><img src="/images/2015_10_18_10_24_04_477x337.jpg" alt="/images/2015_10_18_10_24_04_477x337.jpg" /></p>

<p>Cluster名字为:</p>

<p><img src="/images/2015_10_18_10_25_05_504x207.jpg" alt="/images/2015_10_18_10_25_05_504x207.jpg" /></p>

<p>添加Host:</p>

<p><img src="/images/2015_10_18_10_26_18_469x345.jpg" alt="/images/2015_10_18_10_26_18_469x345.jpg" /></p>

<p>添加二级存储:</p>

<p><img src="/images/2015_10_18_10_27_50_501x341.jpg" alt="/images/2015_10_18_10_27_50_501x341.jpg" /></p>

<p>点击<code>Enable Zone</code>后可以激活该Zone.</p>

<p>等待系统虚拟机启动完毕后就可以使用了.</p>

<h3>已知问题</h3>

<p>CloudStack Agent启动时, 会添加iptables规则,这会造成我们前面加入的转发链失效.</p>

<p>解决方案: 手动运行命令:</p>

<pre><code># iptables -t nat -A POSTROUTING -s 172.16.16.0/24 ! -d 172.16.16.0/24 -j \ 
MASQUERADE &amp;&amp; iptables -t filter -I FORWARD -j ACCEPT
</code></pre>

<p>这将使能转发.从而172.16.16.0/24网段的机器能上网.</p>
]]></content>
  </entry>
  
</feed>
