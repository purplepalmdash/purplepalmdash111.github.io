<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Virtualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/virtualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2016-06-14T20:01:41+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[RackHD Worktips]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/06/07/rackhd-worktips/"/>
    <updated>2016-06-07T17:05:19+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/06/07/rackhd-worktips</id>
    <content type="html"><![CDATA[<h3>Vagrant Preparation</h3>

<p>rackhd/rackhd vagrant box could be downloaded from following link:</p>

<p><a href="https://atlas.hashicorp.com/rackhd/boxes/rackhd">https://atlas.hashicorp.com/rackhd/boxes/rackhd</a></p>

<p>Clone the repository from the github:</p>

<pre><code>$ pwd
/home/dash/Code/Jun13
$ git clone https://github.com/RackHD/RackHD
$ cd RackHD
</code></pre>

<p>Change into the directory example, create config and run the setup command:</p>

<pre><code>$ cd example
$ cp config/monorail_rack.cfg.example config/monorail_rack.cfg
</code></pre>

<p>Edits can be made to this new file to adjust the number of pxe clients created.</p>

<pre><code>$ bin/monorail_rack
</code></pre>

<p>The <code>monorail_rack</code> script will auto-start all of the services by default, but you can also run them manually if you prefer.</p>

<pre><code>$ vagrant ssh
vagrant:~$ sudo nf start
</code></pre>

<p>Unfortunately, the vagrant machine won&rsquo;t work due to bad networking.</p>

<h3>Customization Deployment</h3>

<p>Use a trusty based vagrant box for creating the rackhd node.</p>

<pre><code>$ vagrant init trustyvirtualbox
$ vim Vagrantfile
</code></pre>

<p>Vagrantfile&rsquo;s configuration modification is listed as following:</p>

<pre><code>Vagrant.configure(2) do |config|
  # The most common configuration options are documented and commented below.
  # For a complete reference, please see the online documentation at
  # https://docs.vagrantup.com.

  # Added more disks
+   file_to_disk = File.realpath( "." ).to_s + "/disk.vdi"

  # config.vm.network "private_network", ip: "192.168.33.10"
+   config.vm.network "private_network", ip: "172.31.128.1", virtualbox__intnet:  "closednet"

+  config.vm.provider "virtualbox" do |vb|
+    if ARGV[0] == "up" &amp;&amp; ! File.exist?(file_to_disk) 
+      puts "Creating 5GB disk #{file_to_disk}."
+      vb.customize [
+        'createhd', 
+        '--filename', file_to_disk, 
+        '--format', 'VDI', 
+        '--size', 5000 * 1024 # 5 GB
+      ] 
+      vb.customize [
+        'storageattach', :id, 
+        '--storagectl', 'IDE Controller', 
+        '--port', 1, '--device', 0, 
+        '--type', 'hdd', '--medium', 
+        file_to_disk
+      ] 
+    end
+    vb.memory = "4096"
+    vb.cpus = 2
+    vb.customize ["modifyvm", :id, "--nicpromisc2", "allow-all", "--ioapic", "on"]
+  end
</code></pre>

<p>Then <code>vagrant up</code> for start up the machine.</p>

<p>Notice: the controller of the disk should be noticed very carefully, you could choose
&ldquo;IDE Controller&rdquo; Or &ldquo;SATA Controller&rdquo;, depending on your virtualbox configuration.  <br/>
Then follow the tips on:</p>

<p><a href="http://purplepalmdash.github.io/blog/2016/06/01/use-rakehd-for-deploying-systems/">http://purplepalmdash.github.io/blog/2016/06/01/use-rakehd-for-deploying-systems/</a>  <br/>
Extend the root partition of vagrant disk via:</p>

<pre><code>$ sudo pvcreate /dev/sdb
$ sudo vgextend vagrant-vg /dev/sdb
$ sudo lvextend -l +100%FREE /dev/vagrant-vg/root
$ sudo resize2fs  /dev/vagrant-vg/root
</code></pre>

<h3>Adding Ubuntu Deployment</h3>

<p>Install <code>apt-mirror</code> first, then using following mirror configuration file:</p>

<pre><code>$ vim /etc/apt/mirror.list
############# config ##################
#
# set base_path    /var/spool/apt-mirror
#
# set mirror_path  $base_path/mirror
# set skel_path    $base_path/skel
# set var_path     $base_path/var
# set cleanscript $var_path/clean.sh
# set defaultarch  &lt;running host architecture&gt;
# set postmirror_script $var_path/postmirror.sh
# set run_postmirror 0
set base_path   /var/mirrors/ubuntu/14.04
set nthreads     20
set _tilde 0
#
############# end config ##############

deb-amd64 http://mirrors.aliyun.com/ubuntu  trusty main main/debian-installers
deb http://mirrors.aliyun.com/ubuntu    trusty main/installer-amd64
deb-amd64 http://mirrors.aliyun.com/ubuntu  trusty-updates main
deb-amd64 http://mirrors.aliyun.com/ubuntu  trusty-security main
clean http://mirrors.aliyun.com/ubuntu
</code></pre>

<p>Also you have to create following script for downloading the debian-installer:</p>

<pre><code>$ vim /var/mirrors/ubuntu/14.04/var/postmirror.sh 
#!/bin/sh -x 
# the udebs script gets the actual files we need 
#/mnt/repo/apt-mirror/var/udebs.sh  
# A quick apt directory structure primer: 
# an apt server (e.g. archive.ubuntu.com) contains repositories (e.g. trusty-backports), 
# which contain archives (e.g. multiverse), which contain directories 
# a complete example - http://archive.ubuntu.com/ubuntu/dists/trusty-backports/multiverse/debian-installer/  
# With this in mind, we create bash 'arrays' of the structure: 
# server we're syncing against 
#MIRROR="cn.archive.ubuntu.com" 
MIRROR="archive.ubuntu.com" 
# repositories we're mirroring 
#REPOS="trusty trusty-updates trusty-security trusty-proposed trusty-backports" 
REPOS="trusty"
# archives in repositories 
#ARCHIVES="main multiverse restricted universe" 
ARCHIVES="main"
# installer location inside archive 
#DIRECTORIES="debian-installer dist-upgrader-all installer-amd64 installer-i386" 
DIRECTORIES="debian-installer installer-amd64"
#where we're storing it locally 
LOCALDIR="/var/mirrors/ubuntu/14.04/mirror/mirrors.aliyun.com"
#LOCALDIR="/mnt/repo/apt-mirror/mirror/archive.ubuntu.com"  
for REPO in $REPOS; do 
for ARCHIVE in $ARCHIVES; do 
for DIRECTORY in $DIRECTORIES;do 
# create directory structure 
if [ ! -e "$LOCALDIR/ubuntu/dists/$REPO/$ARCHIVE/$DIRECTORY" ]; then
mkdir -p "$LOCALDIR/ubuntu/dists/$REPO/$ARCHIVE/$DIRECTORY"
fi
# do the sync 
rsync --recursive --times --links --hard-links --delete --delete-after \
rsync://$MIRROR/ubuntu/dists/$REPO/$ARCHIVE/$DIRECTORY/ $LOCALDIR/ubuntu/dists/$REPO/$ARCHIVE/$DIRECTORY
done
done
done
</code></pre>

<p>Now run <code>sudo apt-mirror</code> for syncing the repository to local storage.</p>

<p>Also create a shortcut to the repository in RackHD System:</p>

<pre><code>$ sudo ln -s /var/mirrors/ubuntu/14.04/mirror/mirrors.aliyun.com/ubuntu/ /opt/monorail/static/http/
</code></pre>

<p>Now restart the rackhd node, the ubuntu deployment is ready for use.</p>

<h3>Ubuntu Deployment</h3>

<p>Add the json file which holds the ubuntu deployment:</p>

<pre><code>$ pwd
/home/vagrant/RackHD/example
$ vim samples/ubuntu_boot.json 
{
    "name": "Graph.InstallUbuntu",
    "options": {
        "defaults": {
            "obmServiceName": "noop-obm-service"
        },
        "install-os": {
            "repo": "/ubuntu",
            "rootPassword": "ubuntu",
            "profile": "install-trusty.ipxe",
            "completionUri": "renasar-ansible.pub"
        }
    }
}
</code></pre>

<p>In fact the <code>rootPassword</code> is not ready for use, the real password after deployment
 is <code>RackHDRocks!</code>.</p>

<p>Add one node(first you should make it pxed):</p>

<pre><code>$ curl -H "Content-Type: application/json" -X POST --data @samples/noop_body.json http://localhost:8080/api/1.1/nodes/575fce38d23ba028051b4711/obm
$ curl -H "Content-Type: application/json" -X POST --data @samples/ubuntu_boot.json http://localhost:8080/api/1.1/nodes/575fce38d23ba028051b4711/workflows
</code></pre>

<p>Then restart the machine, you will get it installing ubuntu.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use RackHD for Deploying Systems]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/06/01/use-rakehd-for-deploying-systems/"/>
    <updated>2016-06-01T09:14:18+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/06/01/use-rakehd-for-deploying-systems</id>
    <content type="html"><![CDATA[<p>Following are the steps for using the RackHD for deploying systems. Mainly refers to
<a href="http://dojoblog.emc.com/rackhd-cpi/setting-up-rackhd/">http://dojoblog.emc.com/rackhd-cpi/setting-up-rackhd/</a></p>

<p>But the tutorial from emc includes lots of mistakes, so I listed all of the steps in
following chapters.</p>

<h3>Vagrant Env Preparation</h3>

<p>Initialize the vagrant env via(ubuntu1404 is my box name):</p>

<pre><code>$ vagrant init ubuntu1404
A `Vagrantfile` has been placed in this directory. 
$ vim Vagrantfile
  config.vm.provider "virtualbox" do |vb|
  #   # Display the VirtualBox GUI when booting the machine
  #   vb.gui = true
  #
  #   # Customize the amount of memory on the VM:
    vb.memory = "4096"
    vb.cpus = 4
    vb.customize ["modifyvm", :id, "--nicpromisc2", "allow-all", "--ioapic", "on"]
  end
$ vagrant up
</code></pre>

<h3>Prerequisites</h3>

<p>Use following scripts for installing the prerequisites for RackHD(<code>pre_rackhd.sh</code>):</p>

<pre><code>#!/bin/bash 
set -­e

sudo apt-get -y update
sudo apt-get -y dist-upgrade
sudo apt-get autoremove
sudo apt-get install -y nodejs nodejs-legacy npm

# Runtime Dependencies
sudo apt-get install -y rabbitmq-server mongodb isc-dhcp-server
sudo apt-get install -y snmp ipmitool ansible amtterm apt-mirror unzip libkrb5-dev

# upstart will be installed after Ubuntu15.04.
# sudo apt-get -y install upstart­sysv
sudo update-initramfs -u

# amttool
wget http://downloads.sourceforge.net/project/amttooltng/1.7/amttool
sudo chmod 777 amttool
sudo mv amttool /usr/bin/amttooltng

# Compile Dependencies
sudo apt-get install -y git openssh-server pbuilder dh-make devscripts ubuntu-dev-tools

# Git clone all of the repositories
RACKHD_INSTALL_DIR=~;cd $RACKHD_INSTALL_DIR
git clone https://github.com/RackHD/RackHD
RACKHD_PROJECT_DIR=${RACKHD_INSTALL_DIR}/RackHD
cd $RACKHD_PROJECT_DIR
git submodule update --init --recursive
git submodule foreach git pull origin master

# Configuration files
sudo touch /etc/default/on-http
sudo touch /etc/default/on-dhcp-proxy
sudo touch /etc/default/on-taskgraph
sudo touch /etc/default/on-syslog
sudo touch /etc/default/on-tftp

# Build And Install
cd ${RACKHD_PROJECT_DIR}/on-http
./HWIMO-BUILD
sudo dpkg -i on-http_*.deb

cd ${RACKHD_PROJECT_DIR}/on-dhcp-proxy
./HWIMO-BUILD
sudo dpkg -i on-dhcp-proxy_*.deb

cd ${RACKHD_PROJECT_DIR}/on-taskgraph
./HWIMO-BUILD
sudo dpkg -i on-taskgraph_*.deb

cd ${RACKHD_PROJECT_DIR}/on-syslog
./HWIMO-BUILD
sudo dpkg -i on-syslog_*.deb

cd ${RACKHD_PROJECT_DIR}/on-tftp
./HWIMO-BUILD
sudo dpkg -i on-tftp_*.deb
</code></pre>

<p>Now shutdown the vm, add one networking interface, because we want to add PXE in this
network apdater.</p>

<pre><code>$ vim Vagrantfile
config.vm.network "private_network", ip: "172.31.128.1", virtualbox__intnet: "closednet"
</code></pre>

<p>Now startup the vagrant vm again. continue to configure the PXE.</p>

<h3>Configuration</h3>

<p>Examine the eth1 IP Configuration:</p>

<pre><code>vagrant@ubuntu-1404:~$ ifconfig eth1
eth1      Link encap:Ethernet  HWaddr 08:00:27:c2:cf:3e  
          inet addr:172.31.128.1  Bcast:172.31.128.255  Mask:255.255.255.0
</code></pre>

<p>Configure the isc-dhcp:</p>

<pre><code>$ sudo vim /etc/default/isc-dhcp-server
INTERFACES="eth1"
</code></pre>

<p>Add the following configuration into <code>/etc/dhcp/dhcp.conf</code>:</p>

<pre><code># RackHD Added Lines
deny duplicates;

ignore-client-uids true;

subnet 172.31.128.0 netmask 255.255.252.0 {
    range 172.31.128.2 172.31.131.254;
    # Use this option to signal to the PXE client that we are doing proxy DHCP
    option vendor-class-identifier "PXEClient";
}
</code></pre>

<p>Download the config.json for monorail usage, and modify its tftpRoot Configuration:</p>

<pre><code>$ cd /opt/monorail/
$ sudo wget https://raw.githubusercontent.com/RackHD/RackHD/master/packer/ansible/roles/monorail/files/config.json
$ sudo vim config.json
    "tftpRoot": "/opt/monorail/static/tftp",
</code></pre>

<p>Prepare the http and tftp folder:</p>

<pre><code>$ sudo mkdir -p /opt/monorail/static/http/common/
$ sudo mkdir -p /opt/monorail/static/tftp/
</code></pre>

<p>Download the tftp and http static files:</p>

<pre><code>$ cd /opt/monorail/static/tftp
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/ipxe/monorail.ipxe
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/ipxe/monorail-undionly.kpxe
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/ipxe/monorail-efi32-snponly.efi
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/ipxe/monorail-efi64-snponly.efi
$ cd /opt/monorail/static/http/common/
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/builds/discovery.overlay.cpio.gz
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/builds/base.trusty.3.16.0-25-generic.squashfs.img
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/builds/initrd.img-3.16.0-25-generic
$ sudo wget https://bintray.com/artifact/download/rackhd/binary/builds/vmlinuz-3.16.0-25-generic
</code></pre>

<p>Now reboot the vm again, next step we will test the PXE boot.</p>

<h3>PXE Clients</h3>

<p>The PXE will be avaiable and after it finishes, the vm will look like:</p>

<p><img src="/images/2016_06_01_11_47_57_712x236.jpg" alt="/images/2016_06_01_11_47_57_712x236.jpg" />  <br/>
Now you could get the node info via following RESTFUL api call:</p>

<pre><code>$ curl http://localhost:8080/api/1.1/nodes | python -m json.tool
        "createdAt": "2016-06-01T03:44:09.064Z",
        "id": "574e5a0944ff724a05284005",
        "identifiers": [
            "08:00:27:02:5f:7a"
        ],
        "name": "08:00:27:02:5f:7a",
</code></pre>

<p>Examine the mac correponding node ID <code>574e5a0944ff724a05284005</code>, later we wil use this id for deploying CentOS.</p>

<h3>Add CentOS7 Deployment</h3>

<p>Add CentOS7 DVD into the deployment:</p>

<pre><code>$ sudo python ~/RackHD/on-tools/scripts/setup_iso.py /mnt/CentOS-7-x86_64-Everything-1511.iso  /opt/monorail/static/http --link=/home/vagrant/RackHD
</code></pre>

<p>This will takes around 10 mins for importing the iso into the RackHD.</p>

<p>Configure the node:</p>

<pre><code>$ curl -H "Content-Type: application/json" -X POST --data @samples/noop_body.json http://localhost:8080/api/1.1/nodes/574e5a0944ff724a05284005/obm | python -m json.tool
$ curl -H "Content-Type: application/json" -X POST --data @samples/centos_iso_boot.json http://localhost:8080/api/1.1/nodes/574e5a517c6c03440553dd0f/workflows | python -m json.tool
</code></pre>

<p><img src="/images/2016_06_01_14_12_33_449x503.jpg" alt="/images/2016_06_01_14_12_33_449x503.jpg" /> <br/>
Then added the configuration for deploying CentOS7 via:</p>

<pre><code>$ cd ~/RackHD/example/
$ curl -H "Content-Type: application/json" -X POST --data @samples/noop_body.json http://localhost:8080/api/1.1/nodes/574e6516224a4449056183e7/obm | python -m json.tool
$ curl -H "Content-Type: application/json" -X POST --data @samples/centos_iso_boot.json http://localhost:8080/api/1.1/nodes/574e6516224a4449056183e7/workflows | python -m json.tool
</code></pre>

<p>PXE the node, thus you will get a CentOS7 installed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setup LXD on Ubuntu1604]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/05/11/setup-lxd-on-ubuntu1604/"/>
    <updated>2016-05-11T15:38:23+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/05/11/setup-lxd-on-ubuntu1604</id>
    <content type="html"><![CDATA[<h3>Preparation</h3>

<p>By default the lxd is installed in ubuntu1604.</p>

<h3>Image</h3>

<p>The image file are downloaded before we actually install it, install the image via:</p>

<pre><code>$ lxc image import ubuntu-16.04-server-cloudimg-amd64-lxd.tar.xz ubuntu-16.04-server-cloudimg-amd64-root.tar.xz --alias ubuntu1604
$ lxc image list
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
|    ALIAS     | FINGERPRINT  | PUBLIC |             DESCRIPTION              |  ARCH  |   SIZE   |         UPLOAD DATE          |
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
| ubuntu1604   | f4c4c60a6b75 | no     | Ubuntu 16.04 LTS server (20160420.3) | x86_64 | 137.54MB | May 10, 2016 at 2:18pm (UTC) 
</code></pre>

<h3>Start Container</h3>

<p>Start the container via:</p>

<pre><code>$ lxc launch ubuntu1604 first1404
$ lxc list
+------------+---------+------+------+------------+-----------+
|    NAME    |  STATE  | IPV4 | IPV6 |    TYPE    | SNAPSHOTS |
+------------+---------+------+------+------------+-----------+
| first1404  | RUNNING |      |      | PERSISTENT | 0         |
+------------+---------+------+------+------------+-----------+
</code></pre>

<p>Attach to the running container via:</p>

<pre><code>$ lxc exec first1404 /bin/bash
</code></pre>

<p>In this container you could do anything, for your customization of the container.</p>

<h3>More Images</h3>

<p>After your modification is done, shutdown the running container, and submit your
modification to a new container:</p>

<pre><code>$ lxc publish second1604 --alias my-new-image
$ lxc image list
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
|    ALIAS     | FINGERPRINT  | PUBLIC |             DESCRIPTION              |  ARCH  |   SIZE   |         UPLOAD DATE          |
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
| my-new-image | 67de38342bfa | no     |                                      | x86_64 | 192.29MB | May 11, 2016 at 7:07am (UTC) |
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
| ubuntu1604   | f4c4c60a6b75 | no     | Ubuntu 16.04 LTS server (20160420.3) | x86_64 | 137.54MB | May 10, 2016 at 2:18pm (UTC) |
+--------------+--------------+--------+--------------------------------------+--------+----------+------------------------------+
</code></pre>

<h3>Container Networking</h3>

<p>The default networking is a seperated network, but we could set the lxd using the hosted
network, via following steps:</p>

<pre><code>$ cat /etc/network/interfaces

auto ens3
iface ens3 inet manual

auto containerbr 
iface containerbr inet static
address 192.168.10.193
netmask 255.255.0.0
gateway 192.168.0.176
dns-nameservers 180.76.76.76
bridge_ports ens3
</code></pre>

<p>Reboot the machine, you have the running bridge <code>containerbr</code>, now you could set your bridge to this
newly created bridge:</p>

<pre><code>$ lxc profile device set default eth0 parent containerbr
</code></pre>

<p>Via this you cuold set the same subnet networking address just as in <code>containerbr</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Working Tips on Mesos/Ansible]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/05/09/working-tips-on-mesos-slash-ansible/"/>
    <updated>2016-05-09T12:20:34+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/05/09/working-tips-on-mesos-slash-ansible</id>
    <content type="html"><![CDATA[<h3>Package Prepare</h3>

<p>We have the default vagrant box generated by bento, listed it via:</p>

<pre><code>➜  mesos vagrant box list | grep -i centos | grep -i virtualbox
centos72                                     (virtualbox, 0)
</code></pre>

<p>Now we want to generate a new box from it, and added our own configuration:</p>

<pre><code>$ vagrant init centos72
$ vagrant up
$ vagrant ssh
</code></pre>

<p>Edit for keeping the cache:</p>

<pre><code>$ cat /etc/yum.conf  | more
[main]
cachedir=/home/vagrant/rpms/$basearch/$releasever
#keepcache=0
keepcache=1
</code></pre>

<p>Now poweroff the machine and export it to the new box:</p>

<pre><code>$ vagrant package --output centoslocalrpm.box
$ vagrant box add centoslocalrpm centoslocalrpm.box
</code></pre>

<p>Using the new box, the rpm packages could be saved into the folder
<code>/home/vagrant/rpms</code>.</p>

<h3>Deploy Using Ansible</h3>

<p>Refers to  <br/>
<a href="https://open.mesosphere.com/advanced-course/recreating-the-cluster-using-ansible/">https://open.mesosphere.com/advanced-course/recreating-the-cluster-using-ansible/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Working Tips on Ansible-cobbler(2)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2016/05/06/working-tips-on-ansible-cobbler-2/"/>
    <updated>2016-05-06T15:03:59+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2016/05/06/working-tips-on-ansible-cobbler-2</id>
    <content type="html"><![CDATA[<h3>AIM</h3>

<p>Change the vagrant box to libvirt, and let this libvirt machine working properly.</p>

<h3>Image Transformation</h3>

<p>Transform the image via following command:</p>

<pre><code>$ VBoxManage clonehd /home/dash/VirtualBox\ VMs/ansible-cobbler_cobbler-ubuntu_1462410925173_15793/packer-virtualbox-iso-1454031074-disk1.vmdk /home/dash/output.img --format raw &amp;&amp; qemu-img convert -f raw /home/dash/output.img -O qcow2 /home/dash/ansible-cobbler.qcow2
0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%
Clone medium created in format 'raw'. UUID: 6fbb99be-8004-43b5-831b-ec794a001c10
</code></pre>

<h3>Qemu Virtual Machine</h3>

<p>Create a new machine, then configure the networking, edit the cobbler setting/dhcp setting, then <code>cobbler sync</code>, now the cobbler is adjusting to new environment.</p>

<p>More detailed info could be seen in:</p>

<p><a href="http://purplepalmdash.github.io/blog/2015/07/10/wh-worktips-9/">http://purplepalmdash.github.io/blog/2015/07/10/wh-worktips-9/</a></p>
]]></content>
  </entry>
  
</feed>
