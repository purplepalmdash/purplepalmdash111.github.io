<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Visualization | Dash]]></title>
  <link href="http://purplepalmdash.github.io/blog/categories/visualization/atom.xml" rel="self"/>
  <link href="http://purplepalmdash.github.io/"/>
  <updated>2016-05-31T21:18:11+08:00</updated>
  <id>http://purplepalmdash.github.io/</id>
  <author>
    <name><![CDATA[Dash]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[用Graphite呈现广州空气质量(4)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-4/"/>
    <updated>2015-12-16T16:29:43+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-4</id>
    <content type="html"><![CDATA[<h3>Tessera</h3>

<p>直接导入Graphite中定义好的dashboard即可，值得注意的是，如何创建模板，或者说，如何创建一
个template用于渲染我们导入的各个数据？</p>

<p>导入的时候出现了如下的问题:</p>

<p><img src="/images/2015_12_16_18_34_44_701x483.jpg" alt="/images/2015_12_16_18_34_44_701x483.jpg" /></p>

<p>可见tessera中对数据的定制化是必须的。</p>

<h3>Grafana</h3>

<p>安装及配置为自动启动:</p>

<pre><code>$ wget https://grafanarel.s3.amazonaws.com/builds/grafana_2.6.0_amd64.deb
$ sudo dpkg -i grafana_2.6.0_amd64.deb
$ sudo service grafana-server start
$ sudo update-rc.d grafana-server defaults 95 10
</code></pre>

<p>默认用户名/密码为 admin/admin.</p>

<p>现在添加graphite数据源，例如：</p>

<p><img src="/images/2015_12_16_19_20_33_703x484.jpg" alt="/images/2015_12_16_19_20_33_703x484.jpg" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用Graphite呈现广州空气质量(3)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-3/"/>
    <updated>2015-12-16T14:18:25+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-3</id>
    <content type="html"><![CDATA[<h3>当前节点无数据</h3>

<p>我们的脚本加入crontab运行后，最开始是可以得到数据的，后面两小时它挂了，查原因，有以下的
报错信息:</p>

<pre><code># /home/adminubuntu/GuangzhouPM25.py 
Traceback (most recent call last):
  File "/home/adminubuntu/GuangzhouPM25.py", line 112, in &lt;module&gt;
    airdata = get_air_data(positionsets)
  File "/home/adminubuntu/GuangzhouPM25.py", line 80, in get_air_data
    PM25 = int(pattern.match(soup.find('td',{'id': 'pmtow'}).contents[0]).group())
ValueError: invalid literal for int() with base 10: ''
</code></pre>

<p>此时selenium控制的浏览器停在以下图例:</p>

<p><img src="/images/2015_12_16_14_21_03_497x301.jpg" alt="/images/2015_12_16_14_21_03_497x301.jpg" /></p>

<p>可以看到，如果当前节点的数据为<code>--</code>， 则我们的python脚本运行会出现问题。因而我们在代码中
要加入少量修改。</p>

<h3>错误处理</h3>

<p>以下的代码更改添加了错误处理，如果该监测点的数值为空，则不提交任何数据:</p>

<pre><code>@@ -66,9 +66,9 @@ def get_air_data(positionsets):
   hourdata = {}
   # Calling selenium, need linux X
   browser = Firefox()
-  # Added 10 seconds for waiting page for loading.
-  time.delay(10)
   browser.get(URL)
+  # Added 10 seconds for waiting page for loading.
+  time.sleep(10)
   # Click button one-by-one
   for position in positionsets:
     # After clicking, should re-get the page_source.
@@ -78,33 +78,37 @@ def get_air_data(positionsets):
     soup = BeautifulSoup(page_source, 'html.parser')
     # pm2.5 value would be something like xx 微克/立方米, so we need an regex for
     # matching, example: print int(pattern.match(input).group())
-    PM25 = int(pattern.match(soup.find('td',{'id': 'pmtow'}).contents[0]).group())
-    PM25_iaqi = int(pattern.match(soup.find('td',{'id': 'pmtow_iaqi'}).contents[0]).group())
-    PM10 = int(pattern.match(soup.find('td',{'id': 'pmten'}).contents[0]).group())
-    PM10_iaqi = int(pattern.match(soup.find('td',{'id': 'pmten_iaqi'}).contents[0]).group())
-    SO2 = int(pattern.match(soup.find('td',{'id': 'sotwo'}).contents[0]).group())
-    SO2_iaqi = int(pattern.match(soup.find('td',{'id': 'sotwo_iaqi'}).contents[0]).group())
-    NO2 = int(pattern.match(soup.find('td',{'id': 'notwo'}).contents[0]).group())
-    NO2_iaqi = int(pattern.match(soup.find('td',{'id': 'notwo_iaqi'}).contents[0]).group())
-    # Special notice the CO would be float value
-    CO = float(floatpattern.match(soup.find('td',{'id': 'co'}).contents[0]).group())
-    CO_iaqi = int(pattern.match(soup.find('td',{'id': 'co_iaqi'}).contents[0]).group())
-    O3 = int(pattern.match(soup.find('td',{'id': 'othree'}).contents[0]).group())
-    O3_iaqi = int(pattern.match(soup.find('td',{'id': 'othree_iaqi'}).contents[0]).group())
-    hourdata_key = pinyin.get(position)
-    hourdata[hourdata_key] = []
-    hourdata[hourdata_key].append(PM25)
-    hourdata[hourdata_key].append(PM25_iaqi)
-    hourdata[hourdata_key].append(PM10)
-    hourdata[hourdata_key].append(PM10_iaqi)
-    hourdata[hourdata_key].append(SO2)
-    hourdata[hourdata_key].append(SO2_iaqi)
-    hourdata[hourdata_key].append(NO2)
-    hourdata[hourdata_key].append(NO2_iaqi)
-    hourdata[hourdata_key].append(CO)
-    hourdata[hourdata_key].append(CO_iaqi)
-    hourdata[hourdata_key].append(O3)
-    hourdata[hourdata_key].append(O3_iaqi)
+    try:
+      PM25 = int(pattern.match(soup.find('td',{'id': 'pmtow'}).contents[0]).group())
+      PM25_iaqi = int(pattern.match(soup.find('td',{'id': 'pmtow_iaqi'}).contents[0]).group())
+      PM10 = int(pattern.match(soup.find('td',{'id': 'pmten'}).contents[0]).group())
+      PM10_iaqi = int(pattern.match(soup.find('td',{'id': 'pmten_iaqi'}).contents[0]).group())
+      SO2 = int(pattern.match(soup.find('td',{'id': 'sotwo'}).contents[0]).group())
+      SO2_iaqi = int(pattern.match(soup.find('td',{'id': 'sotwo_iaqi'}).contents[0]).group())
+      NO2 = int(pattern.match(soup.find('td',{'id': 'notwo'}).contents[0]).group())
+      NO2_iaqi = int(pattern.match(soup.find('td',{'id': 'notwo_iaqi'}).contents[0]).group())
+      # Special notice the CO would be float value
+      CO = float(floatpattern.match(soup.find('td',{'id': 'co'}).contents[0]).group())
+      CO_iaqi = int(pattern.match(soup.find('td',{'id': 'co_iaqi'}).contents[0]).group())
+      O3 = int(pattern.match(soup.find('td',{'id': 'othree'}).contents[0]).group())
+      O3_iaqi = int(pattern.match(soup.find('td',{'id': 'othree_iaqi'}).contents[0]).group())
+      hourdata_key = pinyin.get(position)
+      hourdata[hourdata_key] = []
+      hourdata[hourdata_key].append(PM25)
+      hourdata[hourdata_key].append(PM25_iaqi)
+      hourdata[hourdata_key].append(PM10)
+      hourdata[hourdata_key].append(PM10_iaqi)
+      hourdata[hourdata_key].append(SO2)
+      hourdata[hourdata_key].append(SO2_iaqi)
+      hourdata[hourdata_key].append(NO2)
+      hourdata[hourdata_key].append(NO2_iaqi)
+      hourdata[hourdata_key].append(CO)
+      hourdata[hourdata_key].append(CO_iaqi)
+      hourdata[hourdata_key].append(O3)
+      hourdata[hourdata_key].append(O3_iaqi)
+    except ValueError, Argument:
+      # won't add the data, simply ignore this position
+      print "The argument does not contain numbers\n", Argument
   # After clicking all of the button, quit the firefox and return the dictionary
   browser.close()
   return hourdata
</code></pre>

<p>到现在为止，数据可以顺利的写入到Graphite中。</p>

<h3>Graphite Dashboard</h3>

<p>组建Graphite Dashboard可以通过图形界面来进行，举例如下:</p>

<p><img src="/images/2015_12_16_15_22_04_579x386.jpg" alt="/images/2015_12_16_15_22_04_579x386.jpg" /></p>

<p>具体的添加过程就不说了，值得注意的是，设置几个属性，时间范围为过去24小时，
双击某图片后，<code>Render Options</code>里的<code>Line Mode</code>选择<code>Connected Line</code>，
这样可以构建出连接线，比较适合我们所需要展示的数据类型。Y-Axis，即Y轴的起点(Minimal)设置为0.</p>

<p>点击DashBoard-> Edit Dashboard, 可以看到以下定义:</p>

<p><img src="/images/2015_12_16_15_25_54_789x499.jpg" alt="/images/2015_12_16_15_25_54_789x499.jpg" /></p>

<p>这个定义文件可以修改，我们将使用这个定义文件来批量制作其他十多个监测点的Dashboard.</p>

<h3>创建更多的Dashboard</h3>

<p>参考:   <br/>
<a href="http://graphite.readthedocs.org/en/latest/dashboard.html#editing-importing-and-exporting-via-json">http://graphite.readthedocs.org/en/latest/dashboard.html#editing-importing-and-exporting-via-json</a></p>

<p>将上述的dashboard定义文件存储在某个文本文件中，
用下列命令批量生成新的dashboard定义文件:</p>

<pre><code>$ cat dashboard.txt | sed 's/haizhuhu/aotizhongxin/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/aotizhongxin/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/baiyunshan/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/dafushan/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/gongyuanqian/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/haizhubaogang/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/haizhuchisha/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/haizhuhu/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/haizhushayuan/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/huangpudashadi/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/huangpuwenchong/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/huangshalubianzhan/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/liwanfangcun/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/liwanxicun/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/luhu/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/luogangxiqu/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/tianhelongdong/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/tiyuxi/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/yayuncheng/g'|myclip
$ cat dashboard.txt | sed 's/haizhuhu/yangjilubianzhan/g'|myclip
</code></pre>

<p><code>myclip</code>是一个自定义的命令，可以将管道输出直接到系统剪贴板，
而后将内容新添加到dashboard定义文件中，点击update后，另存为新的dashboard即可.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用Graphite呈现广州空气质量(2)]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-2/"/>
    <updated>2015-12-16T12:11:21+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/12/16/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang-2</id>
    <content type="html"><![CDATA[<h3>更改后的脚本</h3>

<p>以下脚本可以用于取回网页上的数据，并将其写入到Graphite远程服务器。</p>

<pre><code>#!/usr/bin/env python
#-*-coding:utf-8 -*-

##################################################################################
# For fetching back the Air Quality Data and write it into Graphite on local server
# Graphite Data Definition, this is the general definition among every city
# air.city.citypoint.so2
# air.city.citypoint.no2
# air.city.citypoint.pm10
# air.city.citypoint.co
# air.city.citypoint.o38h
# air.city.citypoint.pm25
# air.city.citypoint.aqi
# air.city.citypoint.firstp
# air.city.citypoint.overp
# When running this script in crontab, be sure to give it a display
# Example, execute this script every hour at xx:05
# 5 */1 * * * export DISPLAY=:0;/home/adminubuntu/GuangzhouPM25.py
##################################################################################

# BeautifulSoup
from bs4 import BeautifulSoup

# Selenium
from contextlib import closing
from selenium.webdriver import Firefox
from selenium.webdriver.support.ui import WebDriverWait

# For writing into Graphite
import platform
import socket
import time

# Regex
import re

# pinyin
import pinyin

# Parameters comes here 
CARBON_SERVER = '0.0.0.0'
CARBON_PORT = 2003
DELAY = 5  # secs
URL = 'http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html'
CITY = 'guangzhou'

# All Points In Guangzhou City
positionsets = ["天河龙洞", "白云山", "麓湖", "公园前", "荔湾西村", "黄沙路边站", "杨箕
路边站", "荔湾芳村", "海珠宝岗", "海珠沙园", "海珠湖", "大夫山", "奥体中心", "萝岗西区
", "黄埔文冲", "黄埔大沙地", "亚运城", "体育西", "海珠赤沙"]

# regex for matching the digits.
pattern = re.compile(r'\d*')
floatpattern=re.compile(r'[\d|\.]*')

# Sending message to graphite server. 
def send_msg(message):
  print 'sending message:\n%s' % message
  sock = socket.socket()
  sock.connect((CARBON_SERVER, CARBON_PORT))
  sock.sendall(message)
  sock.close()

# Fetching data, runs each hour. In one-time access should fetch all of the data. 
def get_air_data(positionsets):
  # Dictionary hourdata is for holding data, DataStructure like: 
  # {'baiyunshan': [44, 5], 'haizhubaogang': [55, 6]}
  hourdata = {}
  # Calling selenium, need linux X
  browser = Firefox()
  browser.get(URL)
  # Click button one-by-one
  for position in positionsets:
    # After clicking, should re-get the page_source.
    browser.find_element_by_id(position).click()
    page_source = browser.page_source
    # Cooking Soup
    soup = BeautifulSoup(page_source, 'html.parser')
    # pm2.5 value would be something like xx 微克/立方米, so we need an regex for
    # matching, example: print int(pattern.match(input).group())
    PM25 = int(pattern.match(soup.find('td',{'id': 'pmtow'}).contents[0]).group())
    PM25_iaqi = int(pattern.match(soup.find('td',{'id':
'pmtow_iaqi'}).contents[0]).group())
    PM10 = int(pattern.match(soup.find('td',{'id': 'pmten'}).contents[0]).group())
    PM10_iaqi = int(pattern.match(soup.find('td',{'id':
'pmten_iaqi'}).contents[0]).group())
    SO2 = int(pattern.match(soup.find('td',{'id': 'sotwo'}).contents[0]).group())
    SO2_iaqi = int(pattern.match(soup.find('td',{'id':
'sotwo_iaqi'}).contents[0]).group())
    NO2 = int(pattern.match(soup.find('td',{'id': 'notwo'}).contents[0]).group())
    NO2_iaqi = int(pattern.match(soup.find('td',{'id':
'notwo_iaqi'}).contents[0]).group())
    # Special notice the CO would be float value
    CO = float(floatpattern.match(soup.find('td',{'id': 'co'}).contents[0]).group())
    CO_iaqi = int(pattern.match(soup.find('td',{'id': 'co_iaqi'}).contents[0]).group())
    O3 = int(pattern.match(soup.find('td',{'id': 'othree'}).contents[0]).group())
    O3_iaqi = int(pattern.match(soup.find('td',{'id':
'othree_iaqi'}).contents[0]).group())
    hourdata_key = pinyin.get(position)
    hourdata[hourdata_key] = []
    hourdata[hourdata_key].append(PM25)
    hourdata[hourdata_key].append(PM25_iaqi)
    hourdata[hourdata_key].append(PM10)
    hourdata[hourdata_key].append(PM10_iaqi)
    hourdata[hourdata_key].append(SO2)
    hourdata[hourdata_key].append(SO2_iaqi)
    hourdata[hourdata_key].append(NO2)
    hourdata[hourdata_key].append(NO2_iaqi)
    hourdata[hourdata_key].append(CO)
    hourdata[hourdata_key].append(CO_iaqi)
    hourdata[hourdata_key].append(O3)
    hourdata[hourdata_key].append(O3_iaqi)
  # After clicking all of the button, quit the firefox and return the dictionary
  browser.close()
  return hourdata

if __name__ == '__main__':
  airdata = get_air_data(positionsets)
  timestamp = int(time.time())
  for i in airdata.keys():
    # each key should contains the corresponding hourdata
    lines = [
      'air.guangzhou.%s.pm25 %s %d' % (i, airdata[i][0], timestamp),
      'air.guangzhou.%s.pm25_iaqi %s %d' % (i, airdata[i][1], timestamp),
      'air.guangzhou.%s.pm10 %s %d' % (i, airdata[i][2], timestamp),
      'air.guangzhou.%s.pm10_iaqi %s %d' % (i, airdata[i][3], timestamp),
      'air.guangzhou.%s.so2 %s %d' % (i, airdata[i][4], timestamp),
      'air.guangzhou.%s.so2_iaqi %s %d' % (i, airdata[i][5], timestamp),
      'air.guangzhou.%s.no2 %s %d' % (i, airdata[i][6], timestamp),
      'air.guangzhou.%s.no2_iaqi %s %d' % (i, airdata[i][7], timestamp),
      'air.guangzhou.%s.co %s %d' % (i, airdata[i][8], timestamp),
      'air.guangzhou.%s.co_iaqi %s %d' % (i, airdata[i][9], timestamp),
      'air.guangzhou.%s.o3 %s %d' % (i, airdata[i][10], timestamp),
      'air.guangzhou.%s.o3_iaqi %s %d' % (i, airdata[i][11], timestamp)
    ]
    message = '\n'.join(lines) + '\n'
    send_msg(message)
    # delay for graphite server will use a DELAY time for inserting data
    time.sleep(DELAY)
</code></pre>

<h3>使用方法</h3>

<p>将上面的文件保存为可执行文件，然后使用crontab添加一个定时任务，譬如以下的crontab条目会
在每个小时的xx:05分时自动运行该脚本文件，将取回的数据写入到Graphite远端。</p>

<pre><code>$ crontab -l
# hourly execute pm25 updating task, xx:05 will be the execute time
5 */1 * * * export DISPLAY=:0;/home/adminubuntu/GuangzhouPM25.py
</code></pre>

<p>写入graphite后的效果如下:  <br/>
<img src="/images/2015_12_16_12_20_04_284x453.jpg" alt="/images/2015_12_16_12_20_04_284x453.jpg" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用Graphite呈现广州空气质量]]></title>
    <link href="http://purplepalmdash.github.io/blog/2015/12/15/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang/"/>
    <updated>2015-12-15T10:05:39+08:00</updated>
    <id>http://purplepalmdash.github.io/blog/2015/12/15/yong-graphitecheng-xian-yan-zhou-kong-qi-zhi-liang</id>
    <content type="html"><![CDATA[<h3>数据源准备</h3>

<p>数据源地址在:   <br/>
<a href="http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html">http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html</a></p>

<p>但是这个地址取回数据比较困难。而在<a href="http://www.gzepb.gov.cn/">http://www.gzepb.gov.cn/</a>
右侧的栏里可以通过点击，打开某个监测点当前的空气质量指数,例如海珠湖的数据位于:</p>

<p><a href="http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96">http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96</a></p>

<h3>Beautiful Soup</h3>

<p>Beautiful Soup可以被理解为网页爬虫，用于爬取某个页面并取回所需信息。在Ubuntu/Debian系统
中，安装命令如下。同时为了使用对XML解析速度更快的lxml解析器，我们安装python-lxml:</p>

<pre><code>$ sudo apt-get install -y python-bs4
$ sudo apt-get install -y python-lxml 
</code></pre>

<p>现在我们打开某个终端，开始用命令行交互的方式，取回海珠湖监测点的数据:</p>

<p>首先，引入所需的库：</p>

<pre><code># python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from bs4 import BeautifulSoup
&gt;&gt;&gt; import urllib2
&gt;&gt;&gt; response = urllib2.urlopen('http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96')
&gt;&gt;&gt; print response.info()
Content-Length: 10216
Content-Type: text/html
Last-Modified: Wed, 13 May 2015 08:12:28 GMT
Accept-Ranges: bytes
ETag: "b680828d548dd01:da2"
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
Date: Tue, 15 Dec 2015 02:25:17 GMT
Connection: close

&gt;&gt;&gt; html = response.read()
&gt;&gt;&gt; print "Get the length :", len(html)
Get the length : 10216
&gt;&gt;&gt; response.close()  # best practice to close the file
</code></pre>

<p>上述的操作里调用urllib2取回了页面， html变量里包含了该网页的内容。接下来我们使用
BeautifulSoup来美化并从中取回我们想要的元素。</p>

<pre><code>&gt;&gt;&gt; soup = BeautifulSoup(html, 'html.parser')    
&gt;&gt;&gt; print soup.prettify()
</code></pre>

<p>仔细检查后发现，用urllib2取回的网页中，html变量里未包含当前的数据值。通过阅读代码得知，
当前页面的值是浏览器在载入网页时执行javascript函数得到的。因而我们使用一个真实的浏览器
来实现页面的抓取。</p>

<p>Selenium是一套用于进行浏览器自动化测试的开源工具集，可进行Web应用的端到端测试
。Selenium主要包括两个工具：一是Selenium IDE，二是Selenium WebDriver（简称
WebDriver）. 安装命令如下:</p>

<pre><code>$ pip install selenium
</code></pre>

<p>使用selenium抓取该网页的代码如下:</p>

<pre><code>&gt;&gt;&gt; from contextlib import closing
&gt;&gt;&gt; from selenium.webdriver import Firefox
&gt;&gt;&gt; from selenium.webdriver.support.ui import WebDriverWait
&gt;&gt;&gt; url='http://210.72.1.216:8080/gzaqi_new/DataList2.html?EPNAME=%E6%B5%B7%E7%8F%A0%E6%B9%96'
&gt;&gt;&gt; with closing(Firefox()) as browser:
...   browser.get(url)
...   page_source = browser.page_source
... 
&gt;&gt;&gt; print page_source
&gt;&gt;&gt; soup = BeautifulSoup(page_source, 'html.parser')
&gt;&gt;&gt; print soup
</code></pre>

<p>现在我们可以看到，取回的<code>page_source</code>变量中已经包含有该时段的数据。接下来就是如何把数据
从其中提取出来的过程。</p>

<p>定位到含有数据的表格, 根据其层叠结构，获得tr的值:</p>

<pre><code>&gt;&gt;&gt; table = soup.find('table', {'class': 'headTable'})
&gt;&gt;&gt; for td in table.tbody.tr:
...     print td
... 
&lt;td class="SO2_24H"&gt;7&lt;/td&gt;
&lt;td class="NO2_24H"&gt;50&lt;/td&gt;
&lt;td class="PM10_24H"&gt;29&lt;/td&gt;
&lt;td class="CO_24H"&gt;31&lt;/td&gt;
&lt;td class="O3_8H_24H"&gt;18&lt;/td&gt;
&lt;td class="PM25_24H"&gt;25&lt;/td&gt;
&lt;td class="AQI"&gt;50&lt;/td&gt;
&lt;td class="Pollutants"&gt;—&lt;/td&gt;
&lt;td class="jibie2"&gt;--&lt;/td&gt;
&lt;td class="jibie2"&gt;一级&lt;/td&gt;
&lt;td class="leibie"&gt;优                  &lt;/td&gt;
&lt;td class="yanse"&gt;&lt;img alt="" src="Images/you.jpg"/&gt;&lt;/td&gt;
</code></pre>

<p>更进一步得到值:</p>

<pre><code>&gt;&gt;&gt; for td in table.tbody.tr:
...     print td.contents[0]
... 
7
50
29
31
18
25
50
—
--
一级
优                  
&lt;img alt="" src="Images/you.jpg"/&gt;
</code></pre>

<p>对应的图片如下:</p>

<p><img src="/images/2015_12_15_12_06_23_943x201.jpg" alt="/images/2015_12_15_12_06_23_943x201.jpg" /></p>

<p>提取出来了数据，就可以做后续处理了。</p>

<h3>Graphite</h3>

<p>Graphite的搭建过程不提及。基于我们前面提取出的数据，只需要将其写入Graphite，就可以看
到数据的显示了。</p>

<p>具体的写入代码参考(需翻墙):</p>

<p><a href="http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html">http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html</a></p>

<p>按照博客中提供的例子，写入到Graphite后的数据在Graphite看起来是这样的:</p>

<p><img src="/images/2015_12_15_14_48_48_318x115.jpg" alt="/images/2015_12_15_14_48_48_318x115.jpg" /></p>

<p>而对应的数据格式则如下:</p>

<pre><code> sending message:
     system.monitorserver.loadavg_1min 0.18 1450161396
     system.monitorserver.loadavg_5min 0.25 1450161396
     system.monitorserver.loadavg_15min 0.23 1450161396
</code></pre>

<p>我们可以仿照这样的数据来组织自己的空气质量数据。</p>

<h3>数据来源再加工</h3>

<p>前面取回地址失败， 因为它只是返回空气日报的地址，我们需要的是实时情况，所以还是回到  <br/>
<a href="http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html">http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html</a></p>

<p>这里需要在selenium里模拟出鼠标快速点击所有链接的效果。</p>

<p>下面是一次完整的点击白云山按钮并获得PM2.5页面的过程:</p>

<pre><code>root@monitorserver:~/Code# python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; from contextlib import closing
&gt;&gt;&gt; from selenium.webdriver import Firefox
&gt;&gt;&gt; from selenium.webdriver.support.ui import WebDriverWait
&gt;&gt;&gt; driver = Firefox()                                                 
&gt;&gt;&gt; driver.get('http://210.72.1.216:8080/gzaqi_new/RealTimeDate.html')
&gt;&gt;&gt; driver.refresh()
&gt;&gt;&gt; baiyunmountain=driver.find_element_by_id("白云山")
&gt;&gt;&gt; baiyunmountain.click()
&gt;&gt;&gt; PM25=driver.find_element_by_id("PM25")
&gt;&gt;&gt; type(PM25)
&lt;class 'selenium.webdriver.remote.webelement.WebElement'&gt;
&gt;&gt;&gt; PM25.click()
</code></pre>
]]></content>
  </entry>
  
</feed>
